%!TEX root = thesis.tex

\chapter{Conclusion}

In this thesis we have demonstrated the versatility of the spatio-temporal topic model as a tool for understanding complex data in terms of coherent spatio-temporal semantic maps. We have shown their utility for many datasets \todo{datasets -> application domains?} beyond the textual ones normally associated with the topic modelling literature. More specifically, we have found that a prior that combines sparseness and spatial smoothness leads to a model that can learn categories closely related to the natural human-generated ones with extremely limited supervision.

In the process \todo{comma} we explored how the choice of feature function impacts the resulting topic prior maps, and how to use this to our advantage to get topics that match human-centric labels. Further, we investigated projecting topic priors back into the space of features. Because our topic priors are low-dimensional and spatio-temporally smooth, we found that feature distributions are easier to predict through predicting topic priors than they are directly. Combining these two concepts, we were able to predict feature distributions from very limited training data. Finally, \todo{in chapter ... [provides a specific chapter/secion or page pointer for people who are only looking at the conclusions]} we demonstrated an autoencoding method to learn a feature distribution function suitable for topic modelling. This method allows inverting a feature distribution back into the raw data domain -- images. Adding this last technique, we are able to directly inspect and evaluate the spatial predictions and topics themselves in a natural, human-interpretable domain.

In addition to these general contributions, we have specifically demonstrated the utility of the spatio-temporal topic model for a variety of appliations. First, we showed that with careful design of a feature function, a spatial topic model can be used to identify deep-sea substrate types from repurporsed ROV videos. Then, we showed that in addition, a temporal topic model can be used as an audio similarity metric for a challenging ambient sound dataset, using only off-the-shelf audio features. Next, we developed new insight for phytoplankton ecologists approaching a dataset of over 3,000,000 phytoplankton classifications from 47 taxa. We used the spatial topic model to simplify understanding interactions between taxa and the environment, improving phytoplankton species distribution predictions from weather and oceanographic data. In addition, we used a spatio-temporal topic model to understand the interactions between taxa, and predict the presence of unobserved plankton species given a set of observed ones. Finally, we showed that a spatial topic model can be used to visually model a scene or environment, giving a robot camera the ability to predict what it will see if it looks in a given direction. In the process we explored how to make visual topic models more interpretable by developing an autoencoding feature function that can be learned, and a loss function that helps ensure that images are distinguishable in feature space. This allowed us to view topics, image predictions based on the topic model, and image predictions based on predicted topics directly as images instead of pure feature distributions.

In all these applications, our model's hand-coded geometric knowledge is limited to an assumption that observations close to oneanother are likely to be semantically similar. Despite the fact that each application, most of all our final one, could be improved by more specific information about how observations in one spatio-temporal location affect what we expect to observe in others by limiting the prior information, we find a single model with minimal modification is able to achieve an impressive variety of tasks.

\section{Future Work}
In highlighting the versatility of topic models beyond the textual domain, and beyond simple unsupervised classification, we hope to enable and stimulate further work in areas that could benefit from sparse, smooth priors that we have not yet fully investigated. Firstly, our approach to learning features good for topic modelling is very general. In principle it should be simple to implement for any autoencoder architecture, in any domain where spatio-temporal data is available. Even within the domain we explored, image modelling, the space of possible autoencoder architectures is large, and our choices were based on simplicity rather than the most performant current models. Autoregressive convolutional autoencoders are an extremely good match for our method, naturally leading to multiple words with their sequential approach to image modelling, as well as being some of the top performing methods in terms of preserving detail for high-resolution images. Although our current autoencoding approach loses some detail when compressed through the topic representation, it seems likely that a more powerful feature model such as the autoregressive approach could improve this aspect. Further, beyond simple autoencoding, our extra loss term could be applied to feature layers of a supervised learning problem, and by doing so it may be possible to learn to exert a form of weak supervision on a later unsupervised topic modelling problem, akin to our hand-designed feature functions in Ch.~\ref{ch:topic-models-examples}.

Along similar lines, although we have focused on online, limited data regimes, sparse, spatially smooth representations are equally interesting for extremely large `web-scale' problems where approximate inference through variational methods is preferrable to MCMC. These problems are particularly interesting for end-to-end learning of the feature distribution function, the topic model, and the feature distribution decoding function. Nevertheless, to our knowledge, approximate inference for deep LDA-like models has only been attempted for text -- arguably a domain that needs feature learning for LDA much less than others.

Finally, given recent notable successes in inverse reinforcement learning (IRL), we note that topic space is a potentially interesting feature space for such problems. In particular, many IRL approaches such as those descended from Maximum Entropy IRL \citep{ziebart2008maximum} are considerably simplified when the reward is linear in some feature function, essentially learning the degree to which an agent `likes' each feature dimension compared to the others. As a result, IRL applications often involve extensive supervised training of a pre-trained classifier or other feature function. As we have shown, with the right assumptions, topic-priors align well with intuitive semantic labels (that an `agent' who we are trying to imitate may really care about) and are easy to model from limited samples in downstream learning tasks. Therefore, we propose that topics could be used in the place of such a model, alleviating the need for supervision in the pre-training steps.

\section{Final Word}
In this thesis we presented an approach to learn spatially coherent maps of complex data without any supervision. Our key insight is that interpretable maps feature a combination of semantic sparseness and spatio-temporal smoothness for a wide variety of domains. We implemented these assumptions in a Bayesian probabalistic framework, allowing our approach to be robust to different conditions associated with different problems by considering them as soft prior information rather than as hard constraints. We have shown that in comparison to prevailing unsupervised learning techniques, our approach produces representations that are both simpler to understand for humans and easier to use for later learning tasks in the common scenario where training data is limited. Our technique represents a unique tool to learn meaningul representations of spatio-temporal datasets, enabling intuitive understanding and informing further exploration for scientists, end-users, and robots alike.

\todo{Bravo!}