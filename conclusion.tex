%!TEX root = thesis.tex

\chapter{Conclusion}

In this thesis we have demonstrated the versatility of the spatio-temporal topic model as a tool for understanding complex data in terms of coherent spatio-semantic maps. We have shown their utility for many datasets beyond the textual ones normally associated with the topic modelling literature. More specifically, we have found that a prior that combines sparseness and spatial smoothness leads to a model that can learn categories closely related to the natural human-generated ones with extremely limited supervision.

In the process we explored how the choice of feature function impacts the resulting topic prior maps, and how to use this to our advantage to get topics that match human-centric labels. Further, we investigated projecting topic priors back into the space of features. Because our topic priors are low-dimensional and spatio-temporally smooth, we found that they are easier to predict than feature distributions directly. Combining these two concepts, we were able to predict feature distributions from very limited training data. Finally, we demonstrated an autoencoding method to learn a feature distribution function suitable for topic modelling. This method allows inverting a feature distribution back into the raw data domain -- images. Adding this last technique, we are able to directly inspect and evaluate the spatial predictions and topics themselves in a natural, human-interpretable domain.

In addition to these general contributions, we have specifically demonstrated the utility of the spatio-temporal topic model for a variety of domains. First, we showed that with careful design of a feature function, the model can be used to identify deep-sea substrate types from repurporsed ROV videos. Then, we showed that the model's temporal smoothness assumption makes for an improved audio similarity measure based on topics. Next, we used the topic model to model phytoplankton communities, offering novel understanding of the relationships and inter-relationships between species and environmental factors.
\todo[inline]{Finally, we showed something specific in the last chapter.}

\section{Future Work}
In highlighting the versatility of topic models beyond the textual domain, and beyond simple unsupervised classification, we hope to enable and stimulate further work in promising areas that we have not yet fully investigated. Firstly, our approach to learning features good for topic modelling is very general. In principle it should be simple to implement for any autoencoder architecture, in any domain where spatio-temporal data is available. Even within the domain we explored, image modelling, the space of possible autoencoder architectures is large, and our choices were based on simplicity rather than the most performant current models. Autoregressive convolutional autoencoders are an extremely good match for our method, naturally leading to multiple words with their sequential approach to image modelling, as well as being some of the top performing methods in terms of preserving detail for high-resolution images. Further, beyond simple autoencoding, our extra loss term could be applied to feature layers of a supervised learning problem, and by doing so it may be possible to learn to exert a form of weak supervision on a later unsupervised topic modelling problem, akin to our hand-designed feature functions in Ch.~\ref{ch:topic-models-examples}.

Along similar lines, although we have focused on online, limited data regimes, sparse, spatially smooth representations are equally interesting for extremely large `web-scale' problems where approximate inference through variational methods is preferrable to MCMC. These problems are particularly interesting for end-to-end learning of the feature distribution function, the topic model, and the feature distribution decoding function. Nevertheless, to our knowledge, approximate inference for deep LDA-like models has only been attempted for text -- arguably a domain that needs feature learning for LDA much less than others.

Finally, given recent notable successes in inverse reinforcement learning (IRL), we note that topic space is a potentially interesting feature space for such problems. In particular, many IRL approaches are considerably simplified when the reward is linear in some feature function, essentially learning the degree to which an agent `likes' each feature dimension compared to the others. As a result, IRL applications often involve extensive supervised training of a pre-trained classifier or other feature function. As we have shown, with the right assumptions, topic-priors align well with intuitive semantic labels and are easy to model from limited samples in downstream learning tasks. Therefore, we propose that topics could be used in the place of such a model, alleviating the need for supervision in the pre-training steps.

\section{Final Word}
In this thesis we presented an approach to learn spatially coherent maps of complex data without any supervision. Our key insight is that interpretable maps across many domains feature a combination of semantic sparseness and spatio-temporal smoothness. We implemented these assumptions in a Bayesian probabalistic framework, which allowed us to learn easy to use representations for a wide variety of perception tasks. We showed both that human-centric interpretation is simplified and supervised learning is honed in limited-data scenarios by using our learned representations.