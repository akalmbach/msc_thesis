@article{peacock2014parasitic,
  title={Parasitic infection of the diatom Guinardia delicatula, a recurrent and ecologically important phenomenon on the New England Shelf},
  journal={ Marine Ecology Progress Series},
  volume = {503},
  pages={1-10},
  author={Peacock, Emily E and Olson, Robert J and Sosik, Heidi M},
  year={2014},
  publisher={Inter-Research}
}
@misc{Sosik2016,
author = {Sosik, Heidi M. and Futrelle, Joe and Brownlee, Emily Fay and Peacock, Emily and Crockford, Taylor and Olson, Robert J.},
doi = {10.5281/zenodo.153978},
title = {{IFCB-Analysis software system}},
url = {https://doi.org/10.5281/zenodo.153978}
}
@article{Sosik2007,
author = {Sosik, Heidi M. and Olson, Robert J.},
doi = {10.4319/lom.2007.5.204},
file = {:Users/yogesh/References/Mendeley/Sosik, Olson - 2007 - Automated taxonomic classification of phytoplankton sampled with imaging-in-flow cytometry.pdf:pdf},
issn = {15415856},
journal = {Limnology and Oceanography: Methods},
month = {jun},
number = {6},
pages = {204--216},
title = {{Automated taxonomic classification of phytoplankton sampled with imaging-in-flow cytometry}},
url = {http://doi.wiley.com/10.4319/lom.2007.5.204},
volume = {5},
year = {2007}
}
@article{Olson2007a,
author = {Olson, Robert J. and Sosik, Heidi M.},
doi = {10.4319/lom.2007.5.195},
file = {:Users/yogesh/References/Mendeley/Olson, Sosik - 2007 - A submersible imaging-in-flow instrument to analyze nano-and microplankton Imaging FlowCytobot.pdf:pdf},
issn = {15415856},
journal = {Limnology and Oceanography: Methods},
month = {jun},
number = {6},
pages = {195--203},
title = {{A submersible imaging-in-flow instrument to analyze nano-and microplankton: Imaging FlowCytobot}},
url = {http://doi.wiley.com/10.4319/lom.2007.5.195},
volume = {5},
year = {2007}
}
@article{Lorenzen1966,
    title = {{A method for the continuous measurement of in vivo chlorophyll concentration}},
    year = {1966},
    journal = {Deep-Sea Research},
    author = {Lorenzen, Carl J.},
    pages = {223--227},
    volume = {13}
}
@article{TehHDP,
author = {Yee Whye Teh and Michael I Jordan and Matthew J Beal and David M Blei},
title = {Hierarchical Dirichlet Processes},
journal = {Journal of the American Statistical Association},
volume = {101},
number = {476},
pages = {1566-1581},
year  = {2006},
publisher = {Taylor & Francis},
doi = {10.1198/016214506000000302},

URL = { 
        https://doi.org/10.1198/016214506000000302
    
},
eprint = { 
        https://doi.org/10.1198/016214506000000302
    
}

}
@article {Pritchard2000,
	author = {Pritchard, Jonathan K. and Stephens, Matthew and Donnelly, Peter},
	title = {Inference of Population Structure Using Multilocus Genotype Data},
	volume = {155},
	number = {2},
	pages = {945--959},
	year = {2000},
	publisher = {Genetics},
	URL = {http://www.genetics.org/content/155/2/945},
	eprint = {http://www.genetics.org/content/155/2/945.full.pdf},
	journal = {Genetics}
}
@inproceedings{Blei2003Captions,
 author = {Blei, David M. and Jordan, Michael I.},
 title = {Modeling Annotated Data},
 booktitle = {Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval},
 series = {SIGIR '03},
 year = {2003},
 isbn = {1-58113-646-3},
 location = {Toronto, Canada},
 pages = {127--134},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/860435.860460},
 doi = {10.1145/860435.860460},
 acmid = {860460},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automatic image annotation, empirical Bayes, image retrieval, probabilistic graphical models, variational methods},
} 
@ARTICLE{bart2011,
author={E. Bart and M. Welling and P. Perona},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Unsupervised Organization of Image Collections: Taxonomies and Beyond},
year={2011},
volume={33},
number={11},
pages={2302-2315},
keywords={image processing;trees (mathematics);unsupervised learning;visual databases;TAX;natural image organization;nested Chinese restaurant process;nonparametric Bayesian model;tree-shaped taxonomy;unsupervised image collection organization;Data models;Image color analysis;Navigation;Organizations;Organizing;Taxonomy;Visualization;Taxonomy;clustering.;hierarchy},
doi={10.1109/TPAMI.2011.79},
ISSN={0162-8828},
month={Nov},}
@article{griffiths2004,
  title={Finding scientific topics},
  author={Griffiths, Thomas L and Steyvers, Mark},
  journal={Proceedings of the National academy of Sciences},
  volume={101},
  number={suppl 1},
  pages={5228--5235},
  year={2004},
  publisher={National Acad Sciences}
}
@Article{Niebles2008,
author="Niebles, Juan Carlos
and Wang, Hongcheng
and Fei-Fei, Li",
title="Unsupervised Learning of Human Action Categories Using Spatial-Temporal Words",
journal="International Journal of Computer Vision",
year="2008",
month="Sep",
day="01",
volume="79",
number="3",
pages="299--318",
issn="1573-1405",
doi="10.1007/s11263-007-0122-4",
url="https://doi.org/10.1007/s11263-007-0122-4"
}
@article{hoffman2013stochastic,
  title={Stochastic variational inference},
  author={Hoffman, Matthew D and Blei, David M and Wang, Chong and Paisley, John},
  journal={The Journal of Machine Learning Research},
  volume={14},
  number={1},
  pages={1303--1347},
  year={2013},
  publisher={JMLR. org}
}
@INPROCEEDINGS{FeiFei2005,
author={L. Fei-Fei and P. Perona},
booktitle={2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)},
title={A Bayesian hierarchical model for learning natural scene categories},
year={2005},
volume={2},
number={},
pages={524-531 vol. 2},
keywords={belief networks;image classification;image representation;natural scenes;unsupervised learning;Bayesian hierarchical model;codeword distribution;learning natural scene category;training set;unsupervised learning;Animals;Bayesian methods;Cities and towns;Dictionaries;Frequency;Histograms;Humans;Layout;Unsupervised learning;Vehicles},
doi={10.1109/CVPR.2005.16},
ISSN={1063-6919},
month={June},}
@article{Wulfmeier2017,
abstract = {Appearance changes due to weather and seasonal conditions represent a strong impediment to the robust implementation of machine learning systems in outdoor robotics. While supervised learning optimises a model for the training domain, it will deliver degraded performance in application domains that underlie distributional shifts caused by these changes. Traditionally, this problem has been addressed via the collection of labelled data in multiple domains or by imposing priors on the type of shift between both domains. We frame the problem in the context of unsupervised domain adaptation and develop a framework for applying adversarial techniques to adapt popular, state-of-the-art network architectures with the additional objective to align features across domains. Moreover, as adversarial training is notoriously unstable, we first perform an extensive ablation study, adapting many techniques known to stabilise generative adversarial networks, and evaluate on a surrogate classification task with the same appearance change. The distilled insights are applied to the problem of free-space segmentation for motion planning in autonomous driving.},
archivePrefix = {arXiv},
arxivId = {1703.01461},
author = {Wulfmeier, Markus and Bewley, Alex and Posner, Ingmar},
eprint = {1703.01461},
isbn = {9781538626818},
title = {{Addressing Appearance Change in Outdoor Robotics with Adversarial Domain Adaptation}},
url = {http://arxiv.org/abs/1703.01461},
year = {2017}
}
@article{Bai2017,
archivePrefix = {arXiv},
arxivId = {1611.08303},
author = {Bai, Min and Urtasun, Raquel},
doi = {10.1109/CVPR.2017.305},
eprint = {1611.08303},
journal = {Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on},
pages = {2858----2866},
title = {{Deep Watershed Transform for Instance Segmentation}},
url = {http://arxiv.org/abs/1611.08303},
year = {2017}
}
@article{Xu2015,
abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.},
archivePrefix = {arXiv},
arxivId = {1502.03044},
author = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua},
doi = {10.1109/72.279181},
eprint = {1502.03044},
file = {:home/arnold/Documents/pdf-library/Library/Xu et al.{\_}2015{\_}Show, Attend and Tell Neural Image Caption Generation with Visual Attention.pdf:pdf},
isbn = {1045-9227 VO - 5},
issn = {19410093},
pmid = {18267787},
title = {{Show, Attend and Tell: Neural Image Caption Generation with Visual Attention}},
url = {http://arxiv.org/abs/1502.03044},
year = {2015}
}
@article{Zhu2017,
abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain {\$}X{\$} to a target domain {\$}Y{\$} in the absence of paired examples. Our goal is to learn a mapping {\$}G: X \backslashrightarrow Y{\$} such that the distribution of images from {\$}G(X){\$} is indistinguishable from the distribution {\$}Y{\$} using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping {\$}F: Y \backslashrightarrow X{\$} and introduce a cycle consistency loss to push {\$}F(G(X)) \backslashapprox X{\$} (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
archivePrefix = {arXiv},
arxivId = {1703.10593},
author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
doi = {10.1109/ICCV.2017.244},
eprint = {1703.10593},
file = {:home/arnold/Documents/pdf-library/Library/Zhu et al.{\_}2017{\_}Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks.pdf:pdf},
title = {{Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks}},
url = {http://arxiv.org/abs/1703.10593},
year = {2017}
}
@article{Ba2014,
abstract = {We present an attention-based model for recognizing multiple objects in images. The proposed model is a deep recurrent neural network trained with reinforcement learning to attend to the most relevant regions of the input image. We show that the model learns to both localize and recognize multiple objects despite being given only class labels during training. We evaluate the model on the challenging task of transcribing house number sequences from Google Street View images and show that it is both more accurate than the state-of-the-art convolutional networks and uses fewer parameters and less computation.},
archivePrefix = {arXiv},
arxivId = {1412.7755},
author = {Ba, Jimmy and Mnih, Volodymyr and Kavukcuoglu, Koray},
eprint = {1412.7755},
file = {:home/arnold/Documents/pdf-library/Library/Ba, Mnih, Kavukcuoglu{\_}2014{\_}Multiple Object Recognition with Visual Attention.pdf:pdf},
isbn = {9781424465163},
pages = {1--10},
title = {{Multiple Object Recognition with Visual Attention}},
url = {http://arxiv.org/abs/1412.7755},
year = {2014}
}
@article{Binney2013,
abstract = {We introduce a graph-based informative path planning algorithm for a mobile robot which explicitly handles time. The objective function must be submodular in the samples taken by the robot, and the samples obtained are allowed to depend on the time at which the robot visits each location. Using a submodular objective function allows our algorithm to handle problems with diminishing returns, e.g. the case when taking a sample provides less utility when other nearby points have already been sampled. We give a formal description of this framework wherein an objective function that maps the path of the robot to the set of samples taken is defined. We also show how this framework can handle the case in which the robot takes samples along the edges of the graph. A proof of the approximation guarantee for the algorithm is given. Finally, quantitative results are shown for three problems: one simple example with a known Gaussian process model, one simulated example for an underwater robot planning problem using data from a well-known ocean modeling system, and one field experiment using an autonomous surface vehicle (ASV) measuring wireless signal strength on a lake.},
author = {Binney, Jonathan and Krause, Andreas and Sukhatme, Gaurav S.},
doi = {10.1177/0278364913488427},
file = {:home/arnold/Documents/pdf-library/Library/Binney, Krause, Sukhatme{\_}2010{\_}Informative Path Planning for an Autonomous Underwater Vehicle.pdf:pdf},
isbn = {0278-3649},
issn = {02783649},
journal = {International Journal of Robotics Research},
keywords = {mapping,marine robotics,sensor networks},
number = {8},
pages = {873--888},
title = {{Optimizing waypoints for monitoring spatiotemporal phenomena}},
volume = {32},
year = {2013}
}
@article{Li2017,
author = {Li, Yingzhen},
file = {:home/arnold/Documents/pdf-library/Library/Li{\_}2017{\_}Approximate Inference New Visions.pdf:pdf},
number = {November},
title = {{Approximate Inference: New Visions}},
url = {http://yingzhenli.net/home/pdf/draft{\_}phd{\_}thesis.pdf},
year = {2017}
}
@inproceedings{Hawke2017,
author = {Hawke, Jeffrey and Bewley, Alex and Posner, Ingmar},
booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems},
file = {:home/arnold/Documents/pdf-library/Library/Hawke, Bewley, Posner{\_}2017{\_}What Makes a Place Building Bespoke Place Dependent Object Detectors for Robotics.pdf:pdf},
isbn = {9781538626818},
title = {{What Makes a Place? Building Bespoke Place Dependent Object Detectors for Robotics}},
url = {http://www.robots.ox.ac.uk/{~}mobile/Papers/2017IROS{\_}hawke.pdf},
year = {2017}
}
@article{Marchegiani2017,
abstract = {— Urban environments are characterised by the presence of distinctive audio signals which alert the drivers to events that require prompt action. The detection and inter-pretation of these signals would be highly beneficial for smart vehicle systems, as it would provide them with complementary information to navigate safely in the environment. In this paper, we present a framework that spots the presence of acoustic events, such as horns and sirens, using a two-stage approach. We first model the urban soundscape and use anomaly detection to identify the presence of an anomalous sound, and later determine the nature of this sound. As the audio samples are affected by copious non-stationary and unstructured noise, which can degrade classification performance, we propose a noise-removal technique to obtain a clean representation of the data we can use for classification and waveform reconstruction. The method is based on the idea of analysing the spectrograms of the incoming signals as images and applying spectrogram segmentation to isolate and extract the alerting signals from the background noise. We evaluate our framework on four hours of urban sounds collected driving around urban Oxford on different kinds of road and in different traffic conditions. When compared to traditional feature representations, such as Mel-frequency cepstrum coefficients, our framework shows an improvement of up to 31{\%} in the classification rate.},
author = {Marchegiani, Letizia and Posner, Ingmar},
doi = {10.1109/ICRA.2017.7989774},
file = {:home/arnold/Documents/pdf-library/Library/Marchegiani, Posner{\_}2017{\_}Leveraging the urban soundscape Auditory perception for smart vehicles.pdf:pdf},
isbn = {9781509046331},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {6547--6554},
title = {{Leveraging the urban soundscape: Auditory perception for smart vehicles}},
year = {2017}
}
@article{Engelcke2016,
abstract = {This paper proposes a computationally efficient approach to detecting objects natively in 3D point clouds using convolutional neural networks (CNNs). In particular, this is achieved by leveraging a feature-centric voting scheme to implement novel convolutional layers which explicitly exploit the sparsity encountered in the input. To this end, we examine the trade-off between accuracy and speed for different architectures and additionally propose to use an L1 penalty on the filter activations to further encourage sparsity in the intermediate representations. To the best of our knowledge, this is the first work to propose sparse convolutional layers and L1 regularisation for efficient large-scale processing of 3D data. We demonstrate the efficacy of our approach on the KITTI object detection benchmark and show that Vote3Deep models with as few as three layers outperform the previous state of the art in both laser and laser-vision based approaches by margins of up to 40{\%} while remaining highly competitive in terms of processing time.},
archivePrefix = {arXiv},
arxivId = {1609.06666},
author = {Engelcke, Martin and Rao, Dushyant and Wang, Dominic Zeng and Tong, Chi Hay and Posner, Ingmar},
eprint = {1609.06666},
file = {:home/arnold/Documents/pdf-library/Library/Engelcke et al.{\_}2016{\_}Vote3Deep Fast Object Detection in 3D Point Clouds Using Efficient Convolutional Neural Networks.pdf:pdf},
isbn = {9781509046324},
title = {{Vote3Deep: Fast Object Detection in 3D Point Clouds Using Efficient Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1609.06666},
year = {2016}
}
@article{Girdhar2016,
archivePrefix = {arXiv},
arxivId = {1509.07975},
author = {FIXME!!},
doi = {10.1007/s10514-015-9500-x},
eprint = {1509.07975},
issn = {15737527},
journal = {Autonomous Robots},
number = {7},
pages = {1267--1278},
title = {{Modeling curiosity in a mobile robot for long-term autonomous exploration and monitoring}},
volume = {40},
year = {2016}
}
@inproceedings{Kalmbach2017,
archivePrefix = {arXiv},
arxivId = {1711.09013},
author = {Kalmbach, Arnold and Sosik, Heidi M. and Dudek, Gregory and Girdhar, Yogesh},
booktitle = {Oceans Mts/Ieee},
eprint = {1711.09013},
title = {{Learning Seasonal Phytoplankton Communities with Topic Models}},
year = {2017}
}
@inproceedings{Kalmbach2017a,
abstract = {Many interesting natural phenomena are sparsely distributed and discrete. Locating the hotspots of such sparsely distributed phenomena is often difficult because their density gradient is likely to be very noisy. We present a novel approach to this search problem, where we model the co-occurrence relations between a robot's observations with a Bayesian nonparametric topic model. This approach makes it possible to produce a robust estimate of the spatial distribution of the target, even in the absence of direct target observations. We apply the proposed approach to the problem of finding the spatial locations of the hotspots of a specific phytoplankton taxon in the ocean. We use classified image data from Imaging FlowCytobot (IFCB), which automatically measures individual microscopic cells and colonies of cells. Given these individual taxon-specific observations, we learn a phytoplankton community model that characterizes the co-occurrence relations between taxa. We present experiments with simulated robot missions drawn from real observation data collected during a research cruise traversing the US Atlantic coast. Our results show that the proposed approach outperforms nearest neighbor and k-means based methods for predicting the spatial distribution of hotspots from in-situ observations.},
archivePrefix = {arXiv},
arxivId = {1703.07309},
author = {Kalmbach, Arnold and Girdhar, Yogesh and Sosik, Heidi M. and Dudek, Gregory},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2017.7989568},
eprint = {1703.07309},
isbn = {9781509046331},
issn = {10504729},
pages = {4906--4913},
title = {{Phytoplankton hotspot prediction with an unsupervised spatial community model}},
year = {2017}
}
@phdthesis{D.P.Kingma2017,
author = {{D.P. Kingma}},
title = {{Variational Inference and Deep Learning: A New Synthesis}},
year = {2017}
}
@article{Scott2014,
abstract = {Information systems researchers have shown an increasing interest in the notion of sociomateriality. In this paper, we continue this exploration by focusing specifically on entanglement: the inseparability of meaning and matter. Our particular approach is differentiated by its grounding in a relational and performative ontology, and its use of agential realism. We explore some of the key ideas of entanglement through a comparison of two phenomena in the travel sector: an institutionalized accreditation scheme offered by the AA and an online social media website hosted by TripAdvisor. Our analysis centers on the production of anonymity in these two practices of hotel evaluation. By examining how anonymity is constituted through an entanglement of matter and meaning, we challenge the predominantly social treatments of anonymity to date and draw attention to the uncertainties and outcomes generated by specific performances of anonymity in practice. In closing, we consider what the particular agential realist concept of entanglement entails for understanding anonymity, and discuss its implications for research practice. [ABSTRACT FROM AUTHOR]},
author = {Scott, Susan V. and Orlikowski, Wanda J.},
doi = {10.25300/MISQ/2014/38.3.11},
isbn = {0276-7783},
issn = {02767783},
journal = {MIS Quarterly},
number = {3},
pages = {873--893},
title = {{Entanglements in Practice: Performing Anonymity Through Social Media}},
url = {https://misq.org/entanglements-in-practice-performing-anonymity-through-social-media.html},
volume = {38},
year = {2014}
}
@article{Burda2015,
abstract = {The variational autoencoder (VAE; Kingma, Welling (2014)) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It typically makes strong assumptions about posterior inference, for instance that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simplified representations which fail to use the network's entire modeling capacity. We present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.},
archivePrefix = {arXiv},
arxivId = {1509.00519},
author = {Burda, Yuri and Grosse, Roger and Salakhutdinov, Ruslan},
eprint = {1509.00519},
file = {:home/arnold/Documents/pdf-library/Library/Burda, Grosse, Salakhutdinov{\_}2015{\_}Importance Weighted Autoencoders.pdf:pdf},
isbn = {1509.00519},
issn = {1312.6114v10},
pages = {1--14},
title = {{Importance Weighted Autoencoders}},
url = {http://arxiv.org/abs/1509.00519},
year = {2015}
}
@article{Grosse2015,
abstract = {Computing the marginal likelihood (ML) of a model requires marginalizing out all of the parameters and latent variables, a difficult high-dimensional summation or integration problem. To make matters worse, it is often hard to measure the accuracy of one's ML estimates. We present bidirectional Monte Carlo, a technique for obtaining accurate log-ML estimates on data simulated from a model. This method obtains stochastic lower bounds on the log-ML using annealed importance sampling or sequential Monte Carlo, and obtains stochastic upper bounds by running these same algorithms in reverse starting from an exact posterior sample. The true value can be sandwiched between these two stochastic bounds with high probability. Using the ground truth log-ML estimates obtained from our method, we quantitatively evaluate a wide variety of existing ML estimators on several latent variable models: clustering, a low rank approximation, and a binary attributes model. These experiments yield insights into how to accurately estimate marginal likelihoods.},
archivePrefix = {arXiv},
arxivId = {1511.02543},
author = {Grosse, Roger B. and Ghahramani, Zoubin and Adams, Ryan P.},
eprint = {1511.02543},
file = {:home/arnold/Documents/pdf-library/Library/Grosse, Ghahramani, Adams{\_}2015{\_}Sandwiching the marginal likelihood using bidirectional Monte Carlo.pdf:pdf},
pages = {1--32},
title = {{Sandwiching the marginal likelihood using bidirectional Monte Carlo}},
url = {http://arxiv.org/abs/1511.02543},
year = {2015}
}
@article{Zhu2017a,
abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain {\$}X{\$} to a target domain {\$}Y{\$} in the absence of paired examples. Our goal is to learn a mapping {\$}G: X \backslashrightarrow Y{\$} such that the distribution of images from {\$}G(X){\$} is indistinguishable from the distribution {\$}Y{\$} using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping {\$}F: Y \backslashrightarrow X{\$} and introduce a cycle consistency loss to push {\$}F(G(X)) \backslashapprox X{\$} (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
archivePrefix = {arXiv},
arxivId = {1703.10593},
author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
doi = {10.1109/ICCV.2017.244},
eprint = {1703.10593},
file = {:home/arnold/Documents/pdf-library/Library/Zhu et al.{\_}2017{\_}Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks.pdf:pdf},
keywords = {cycle gan},
mendeley-tags = {cycle gan},
title = {{Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks}},
url = {http://arxiv.org/abs/1703.10593},
year = {2017}
}
@article{Shwartz-Ziv2017,
abstract = {Despite their great success, there is still no comprehensive theoretical understanding of learning with Deep Neural Networks (DNNs) or their inner organization. Previous work proposed to analyze DNNs in the $\backslash$textit{\{}Information Plane{\}}; i.e., the plane of the Mutual Information values that each layer preserves on the input and output variables. They suggested that the goal of the network is to optimize the Information Bottleneck (IB) tradeoff between compression and prediction, successively, for each layer. In this work we follow up on this idea and demonstrate the effectiveness of the Information-Plane visualization of DNNs. Our main results are: (i) most of the training epochs in standard DL are spent on {\{}$\backslash$emph compression{\}} of the input to efficient representation and not on fitting the training labels. (ii) The representation compression phase begins when the training errors becomes small and the Stochastic Gradient Decent (SGD) epochs change from a fast drift to smaller training error into a stochastic relaxation, or random diffusion, constrained by the training error value. (iii) The converged layers lie on or very close to the Information Bottleneck (IB) theoretical bound, and the maps from the input to any hidden layer and from this hidden layer to the output satisfy the IB self-consistent equations. This generalization through noise mechanism is unique to Deep Neural Networks and absent in one layer networks. (iv) The training time is dramatically reduced when adding more hidden layers. Thus the main advantage of the hidden layers is computational. This can be explained by the reduced relaxation time, as this it scales super-linearly (exponentially for simple diffusion) with the information compression from the previous layer.},
archivePrefix = {arXiv},
arxivId = {1703.00810},
author = {Shwartz-Ziv, Ravid and Tishby, Naftali},
eprint = {1703.00810},
file = {:home/arnold/Documents/pdf-library/Library/Shwartz-Ziv, Tishby{\_}2017{\_}Opening the Black Box of Deep Neural Networks via Information.pdf:pdf},
pages = {1--19},
title = {{Opening the Black Box of Deep Neural Networks via Information}},
url = {http://arxiv.org/abs/1703.00810},
year = {2017}
}
@article{Tishby2015,
abstract = {Deep Neural Networks (DNNs) are analyzed via the theoretical framework of the information bottleneck (IB) principle. We first show that any DNN can be quantified by the mutual information between the layers and the input and output variables. Using this representation we can calculate the optimal information theoretic limits of the DNN and obtain finite sample generalization bounds. The advantage of getting closer to the theoretical limit is quantifiable both by the generalization bound and by the network's simplicity. We argue that both the optimal architecture, number of layers and features/connections at each layer, are related to the bifurcation points of the information bottleneck tradeoff, namely, relevant compression of the input layer with respect to the output layer. The hierarchical representations at the layered network naturally correspond to the structural phase transitions along the information curve. We believe that this new insight can lead to new optimality bounds and deep learning algorithms.},
archivePrefix = {arXiv},
arxivId = {1503.02406},
author = {Tishby, Naftali and Zaslavsky, Noga},
doi = {10.1109/ITW.2015.7133169},
eprint = {1503.02406},
file = {:home/arnold/Documents/pdf-library/Library/Tishby, Zaslavsky{\_}2015{\_}Deep Learning and the Information Bottleneck Principle.pdf:pdf},
isbn = {978-1-4799-5524-4},
title = {{Deep Learning and the Information Bottleneck Principle}},
url = {http://arxiv.org/abs/1503.02406},
year = {2015}
}
@phdthesis{Shkurti2011,
author = {Shkurti, Florian and Dudek, Gregory},
doi = {10.1109/IROS.2011.6094680},
file = {:home/arnold/Documents/pdf-library/Library/Shkurti, Dudek{\_}2011{\_}State estimation of an underwater robot using visual and inertial cues.pdf:pdf},
isbn = {978-1-61284-456-5},
issn = {2153-0858},
number = {October},
pages = {5054--5060},
title = {{State estimation of an underwater robot using visual and inertial cues}},
url = {http://ieeexplore.ieee.org/document/6094680/},
year = {2011}
}
@phdthesis{GamboaHiguera2012,
author = {{Gamboa Higuera}, Juan Camilo},
file = {:home/arnold/Documents/pdf-library/Library/Gamboa Higuera{\_}2012{\_}Distributing work among heterogeneous robots An approach based on fair division theory.pdf:pdf},
pages = {75},
title = {{Distributing work among heterogeneous robots An approach based on fair division theory}},
year = {2012}
}
@article{Erhan2009,
abstract = {Deep architectures have demonstrated state-of-the-art results in a variety of settings, especially with vision datasets. Beyond the model definitions and the quantitative analyses, there is a need for qualitative comparisons of the solutions learned by various deep architectures. The goal of this paper is to find good qualitative interpretations of high level features represented by such models. To this end, we contrast and compare several techniques applied on Stacked Denoising Autoencoders and Deep Belief Networks, trained on several vision datasets. We show that, perhaps counter-intuitively, such interpretation is possible at the unit level, that it is simple to accomplish and that the results are consistent across various techniques. We hope that such techniques will allow researchers in deep architectures to understand more of how and why deep architectures work},
author = {Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
file = {:home/arnold/Documents/pdf-library/Library/Erhan et al.{\_}2009{\_}Visualizing higher-layer features of a deep network.pdf:pdf},
number = {1341},
pages = {1--13},
title = {{Visualizing higher-layer features of a deep network}},
url = {http://igva2012.wikispaces.asu.edu/file/view/Erhan+2009+Visualizing+higher+layer+features+of+a+deep+network.pdf},
year = {2009}
}
@article{Tishby2018,
abstract = {The practical successes of deep neural networks have not been matched by theoret-ical progress that satisfyingly explains their behavior. In this work, we study the information bottleneck (IB) theory of deep learning, which makes three specific claims: first, that deep networks undergo two distinct phases consisting of an initial fitting phase and a subsequent compression phase; second, that the compres-sion phase is causally related to the excellent generalization performance of deep networks; and third, that the compression phase occurs due to the diffusion-like be-havior of stochastic gradient descent. Here we show that none of these claims hold true in the general case. Through a combination of analytical results and simulation, we demonstrate that the information plane trajectory is predominantly a function of the neural nonlinearity employed: double-sided saturating nonlinearities like tanh yield a compression phase as neural activations enter the saturation regime, but linear activation functions and single-sided saturating nonlinearities like the widely used ReLU in fact do not. Using recent results on generalization error dynamics in linear neural networks, we show that there is no evident causal connection between compression and generalization: networks that do not compress are still capable of generalization, and vice versa. We also show that the compression phase, when it exists, does not require any stochasticity in training by demonstrating that we can replicate the IB findings using full batch gradient descent rather than stochastic gradient descent. Finally, we show that when an input domain consists of a subset of task-relevant and task-irrelevant information, hidden representations do com-press the task-irrelevant information, though the overall information about the input increases, and this compression happens at the same time as the fitting process rather than during a subsequent compression period.},
author = {Tishby, Naftali},
file = {:home/arnold/Documents/pdf-library/Library/Tishby{\_}2018{\_}On the Information Bottleneck Theory of Deep Learning.pdf:pdf},
isbn = {0444565191},
number = {2000},
pages = {1--13},
title = {{On the Information Bottleneck Theory of Deep Learning}},
year = {2018}
}
@article{Achille2017,
abstract = {Using established principles from Information Theory and Statistics, we show that in a deep neural network invariance to nuisance factors is equivalent to information minimality of the learned representation, and that stacking layers and injecting noise during training naturally bias the network towards learning invariant representations. We then show that, in order to avoid memorization, we need to limit the quantity of information stored in the weights, which leads to a novel usage of the Information Bottleneck Lagrangian on the weights as a learning criterion. This also has an alternative interpretation as minimizing a PAC-Bayesian bound on the test error. Finally, we exploit a duality between weights and activations induced by the architecture, to show that the information in the weights bounds the minimality and Total Correlation of the layers, therefore showing that regularizing the weights explicitly or implicitly, using SGD, not only helps avoid overfitting, but also fosters invariance and disentangling of the learned representation. The theory also enables predicting sharp phase transitions between underfitting and overfitting random labels at precise information values, and sheds light on the relation between the geometry of the loss function, in particular so-called "flat minima," and generalization.},
archivePrefix = {arXiv},
arxivId = {1706.01350},
author = {Achille, Alessandro and Soatto, Stefano},
eprint = {1706.01350},
file = {:home/arnold/Documents/pdf-library/Library/Achille, Soatto{\_}2017{\_}Emergence of Invariance and Disentangling in Deep Representations.pdf:pdf},
keywords = {deep learning,flat minima,generalization,information bot-,information com-,minimality,neural network,overfitting,pac-bayes,plexity,regularization,representation,sensitivity,stochastic gradient descent,sufficiency,tleneck,total correlation},
title = {{Emergence of Invariance and Disentangling in Deep Representations}},
url = {http://arxiv.org/abs/1706.01350},
year = {2017}
}
@article{Gurau2017,
author = {Gurău, Corina and Rao, Dushyant and Tong, Chi Hay and Posner, Ingmar},
doi = {10.1177/0278364917730603},
file = {:home/arnold/Documents/pdf-library/Library/Gurău et al.{\_}2017{\_}Learn from experience probabilistic prediction of perception performance to avoid failure.pdf:pdf},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
keywords = {autonomous driving,introspection,object detection,performance estimation,robotics},
pages = {027836491773060},
title = {{Learn from experience: probabilistic prediction of perception performance to avoid failure}},
url = {http://journals.sagepub.com/doi/10.1177/0278364917730603},
year = {2017}
}
@article{Devroye2012,
abstract = {We present a black-box style rejection method that is valid for generating random variables with any log-concave density, provided that one knows a mode of the density. When the density is only known up to a constant factor, that method is no longer applicable. ?? 2012 Elsevier B.V..},
author = {Devroye, Luc},
doi = {10.1016/j.spl.2012.01.022},
file = {:home/arnold/Documents/pdf-library/Library/Devroye{\_}2012{\_}A note on generating random variables with log-concave densities.pdf:pdf},
issn = {01677152},
journal = {Statistics and Probability Letters},
keywords = {Expected time analysis,Log-concavity,Monte Carlo method,Random variate generation,Simulation},
number = {5},
pages = {1035--1039},
title = {{A note on generating random variables with log-concave densities}},
volume = {82},
year = {2012}
}
@book{Kingma2017,
author = {Kingma, Diederik Pieter},
file = {:home/arnold/Documents/pdf-library/Library/Kingma{\_}2017{\_}Variational Inference {\&} Deep Learning A New Synthesis.pdf:pdf},
institution = {University of Amsterdam},
isbn = {978-94-6299-745-5},
title = {{Variational Inference {\&} Deep Learning: A New Synthesis}},
year = {2017}
}
@article{Chen2014,
abstract = {Hamiltonian Monte Carlo (HMC) sampling methods provide a mechanism for defining distant proposals with high acceptance probabilities in a Metropolis-Hastings framework, enabling more efficient exploration of the state space than standard random-walk proposals. The popularity of such methods has grown significantly in recent years. However, a limitation of HMC methods is the required gradient computation for simulation of the Hamiltonian dynamical system-such computation is infeasible in problems involving a large sample size or streaming data. Instead, we must rely on a noisy gradient estimate computed from a subset of the data. In this paper, we explore the properties of such a stochastic gradient HMC approach. Surprisingly, the natural implementation of the stochastic approximation can be arbitrarily bad. To address this problem we introduce a variant that uses second-order Langevin dynamics with a friction term that counteracts the effects of the noisy gradient, maintaining the desired target distribution as the invariant distribution. Results on simulated data validate our theory. We also provide an application of our methods to a classification task using neural networks and to online Bayesian matrix factorization.},
archivePrefix = {arXiv},
arxivId = {1402.4102},
author = {Chen, Tianqi and Fox, Emily B. and Guestrin, Carlos},
eprint = {1402.4102},
file = {:home/arnold/Documents/pdf-library/Library/Chen, Fox, Guestrin{\_}2014{\_}Stochastic Gradient Hamiltonian Monte Carlo.pdf:pdf},
isbn = {9781634393973},
title = {{Stochastic Gradient Hamiltonian Monte Carlo}},
url = {http://arxiv.org/abs/1402.4102},
volume = {32},
year = {2014}
}
@article{Seita2016,
abstract = {We present a novel Metropolis-Hastings method for large datasets that uses small expected-size minibatches of data. Previous work on reducing the cost of Metropolis-Hastings tests yield variable data consumed per sample, with only constant factor reductions versus using the full dataset for each sample. Here we present a method that can be tuned to provide arbitrarily small batch sizes, by adjusting either proposal step size or temperature. Our test uses the noise-tolerant Barker acceptance test with a novel additive correction variable. The resulting test has similar cost to a normal SGD update. Our experiments demonstrate several order-of-magnitude speedups over previous work.},
archivePrefix = {arXiv},
arxivId = {1610.06848},
author = {Seita, Daniel and Pan, Xinlei and Chen, Haoyu and Canny, John},
eprint = {1610.06848},
file = {:home/arnold/Documents/pdf-library/Library/Seita et al.{\_}2016{\_}An Efficient Minibatch Acceptance Test for Metropolis-Hastings.pdf:pdf},
title = {{An Efficient Minibatch Acceptance Test for Metropolis-Hastings}},
url = {http://arxiv.org/abs/1610.06848},
year = {2016}
}
@article{Mittal2012,
abstract = {We propose a natural scene statistic-based distortion-generic blind/no-reference (NR) image quality assessment (IQA) model that operates in the spatial domain. The new model, dubbed blind/referenceless image spatial quality evaluator (BRISQUE) does not compute distortion-specific features, such as ringing, blur, or blocking, but instead uses scene statistics of locally normalized luminance coefficients to quantify possible losses of "naturalness" in the image due to the presence of distortions, thereby leading to a holistic measure of quality. The underlying features used derive from the empirical distribution of locally normalized luminances and products of locally normalized luminances under a spatial natural scene statistic model. No transformation to another coordinate frame (DCT, wavelet, etc.) is required, distinguishing it from prior NR IQA approaches. Despite its simplicity, we are able to show that BRISQUE is statistically better than the full-reference peak signal-to-noise ratio and the structural similarity index, and is highly competitive with respect to all present-day distortion-generic NR IQA algorithms. BRISQUE has very low computational complexity, making it well suited for real time applications. BRISQUE features may be used for distortion-identification as well. To illustrate a new practical application of BRISQUE, we describe how a nonblind image denoising algorithm can be augmented with BRISQUE in order to perform blind image denoising. Results show that BRISQUE augmentation leads to performance improvements over state-of-the-art methods. A software release of BRISQUE is available online: http://live.ece.utexas.edu/research/quality/BRISQUE{\_}release.zip for public use and evaluation.},
author = {Mittal, Anish and Moorthy, Anush Krishna and Bovik, Alan Conrad},
doi = {10.1109/TIP.2012.2214050},
file = {:home/arnold/Documents/pdf-library/Library/Mittal, Moorthy, Bovik{\_}2012{\_}No-reference image quality assessment in the spatial domain.pdf:pdf},
isbn = {1057-7149},
issn = {1941-0042},
journal = {IEEE transactions on image processing : a publication of the IEEE Signal Processing Society},
number = {12},
pages = {4695--708},
pmid = {22910118},
title = {{No-reference image quality assessment in the spatial domain.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22910118},
volume = {21},
year = {2012}
}
@inproceedings{Shkurti2012,
abstract = {In this paper we describe a heterogeneous multi-robot system for assisting scientists in environmental monitoring tasks, such as the inspection of marine ecosystems. This team of robots is comprised of a fixed-wing aerial vehicle, an autonomous airboat, and an agile legged underwater robot. These robots interact with off-site scientists and operate in a hierarchical structure to autonomously collect visual footage of interesting underwater regions, from multiple scales and mediums. We discuss organizational and scheduling complexities associated with multi-robot experiments in a field robotics setting. We also present results from our field trials, where we demonstrated the use of this heterogeneous robot team to achieve multi-domain monitoring of coral reefs, based on real-time interaction with a remotely-located marine biologist. {\textcopyright} 2012 IEEE.},
author = {Shkurti, F. and Xu, A. and Meghjani, M. and {Gamboa Higuera}, J.C. and Girdhar, Y. and Giguere, P. and Dey, B.B. and Li, J. and Kalmbach, A. and Prahacs, C. and Turgeon, K. and Rekleitis, I. and Dudek, G.},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2012.6385685},
isbn = {9781467317375},
issn = {21530858},
title = {{Multi-domain monitoring of marine environments using a heterogeneous robot team}},
year = {2012}
}
@inproceedings{Kalmbach2016,
abstract = {{\textcopyright} 2016 IEEE. We propose and evaluate a method for learning deep-sea substrate types using video recorded with a remotely operated vehicle (ROV). The goal of this work is to create a labelled spatial map of substrate types from ROV video in order to support biological and geological domain research. The output of our method describes the mixtures of geological features such as sediment and types of lava flow in images taken at a set of points chosen from an ROV dive. The main contribution of this work is the assembly of a pipeline combining several unique approaches which is able to robustly generate substrate type mixtures under the varying lighting and perspective conditions of deep-sea ROV dive videos. The pipeline comprises three main components: sampling, in which a trained classifier and spatial sampling is used to select relevant frames from the dataset; feature extraction, in which the improved local binary pattern descriptor (ILBP) is used to generate a Bag of Words (BoW) representation of the dataset; and topic modelling in which a variant of Latent Dirichlet Allocation (LDA), is used to infer the mixture of substrate types represented by each BoW. Our method significantly outperforms techniques relying on keypoint based features rather than texture based features, and k-means rather than LDA, demonstrating that our proposed pipeline accurately learns and identifies visible substrate types.},
author = {Kalmbach, A. and Hoeberechts, M. and Albu, A.B. and Glotin, H. and Paris, S. and Girdhar, Y.},
booktitle = {2016 IEEE Winter Conference on Applications of Computer Vision, WACV 2016},
doi = {10.1109/WACV.2016.7477600},
isbn = {9781509006410},
title = {{Learning deep-sea substrate types with visual topic models}},
year = {2016}
}
@inproceedings{Meghjani2014,
abstract = {In this paper we address the rendezvous problem between an autonomous underwater vehicle (AUV) and a passively floating drifter on the sea surface. The AUV's mission is to keep an estimate of the floating drifter's position while exploring the underwater environment and periodically attempting to rendezvous with it. We are interested in the case where the AUV loses track of the drifter, predicts its location and searches for it in the vicinity of the predicted location. We parameterize this search problem with respect to both the uncertainty in the drifter's position estimate and the ratio between the drifter and the AUV speeds. We examine two search strategies for the AUV, an inward spiral and an outward spiral. We derive conditions under which these patterns are guaranteed to find a drifter, and we empirically analyze them with respect to different parameters in simulation. In addition, we present results from field trials in which an AUV successfully found a drifter after periods of communication loss during which the robot was exploring. {\textcopyright} 2014 IEEE.},
author = {Meghjani, M. and Shkurti, F. and Higuera, J.C.G. and Kalmbach, A. and Whitney, D. and Dudek, G.},
booktitle = {Proceedings - Conference on Computer and Robot Vision, CRV 2014},
doi = {10.1109/CRV.2014.31},
file = {:home/arnold/Documents/pdf-library/Library/Meghjani et al.{\_}2014{\_}Asymmetric rendezvous search at sea.pdf:pdf},
isbn = {9781479943388},
keywords = {Marine Robotics,Motion and Path Planning,Search and Rescue Robots},
title = {{Asymmetric rendezvous search at sea}},
year = {2014}
}
@inproceedings{Kalmbach2017b,
abstract = {{\textcopyright} 2017 IEEE. Many interesting natural phenomena are sparsely distributed and discrete. Locating the hotspots of such sparsely distributed phenomena is often difficult because their density gradient is likely to be very noisy. We present a novel approach to this search problem, where we model the co-occurrence relations between a robot's observations with a Bayesian nonparametric topic model. This approach makes it possible to produce a robust estimate of the spatial distribution of the target, even in the absence of direct target observations. We apply the proposed approach to the problem of finding the spatial locations of the hotspots of a specific phytoplankton taxon in the ocean. We use classified image data from Imaging FlowCytobot (IFCB), which automatically measures individual microscopic cells and colonies of cells. Given these individual taxon-specific observations, we learn a phytoplankton community model that characterizes the co-occurrence relations between taxa. We present experiments with simulated robot missions drawn from real observation data collected during a research cruise traversing the US Atlantic coast. Our results show that the proposed approach outperforms nearest neighbor and k-means based methods for predicting the spatial distribution of hotspots from in-situ observations.},
author = {Kalmbach, A. and Girdhar, Y. and Sosik, H.M. and Dudek, G.},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2017.7989568},
file = {:home/arnold/Documents/pdf-library/Library/Kalmbach et al.{\_}2017{\_}Phytoplankton hotspot prediction with an unsupervised spatial community model.pdf:pdf},
isbn = {9781509046331},
issn = {10504729},
title = {{Phytoplankton hotspot prediction with an unsupervised spatial community model}},
year = {2017}
}
@inproceedings{Kalmbach2013a,
abstract = {We discuss the problem of automatically discovering different acoustic regions in the world, and then labeling the trajectory of a robot using these region labels. We use quantized Mel Frequency Cepstral Coefficients (MFCC) as low level features, and a temporally smoothed variant of Latent Dirichlet Allocation (LDA) to compute both the region models, and most likely region labels associated with each time step in the robot's trajectory. We validate our technique by showing results from two datasets containing sound recorded from 51 and 43 minute long trajectories through downtown Montreal and the McGill University campus. Our preliminary experiments indicate that the regions discovered by the proposed technique correlate well with ground truth, labeled by a human expert. {\textcopyright} 2013 IEEE.},
author = {Kalmbach, A. and Girdhar, Y. and Dudek, G.},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2013.6630948},
isbn = {9781467356411},
issn = {10504729},
title = {{Unsupervised environment recognition and modeling using sound sensing}},
year = {2013}
}
@inproceedings{Xu2014,
abstract = {{\textcopyright} 2014 IEEE. The problem of Adaptation from Participation (AfP) aims to improve the efficiency of a human-robot team by adapting a robot's autonomous systems and behaviors based on command-level input from a human supervisor. As a solution to AfP, the Adaptive Parameter EXploration (APEX) algorithm continuously explores the space of all possible parameter configurations for the robot's autonomous system in an online and anytime manner. Guided by information deduced from the human's latest intervening commands, APEX is capable of adapting an arbitrary robot system to dynamic changes in task objectives and conditions during a session. We explore this framework within visual navigation contexts where the humanrobot team is tasked with covering or patrolling over multiple terrain boundaries such as coastlines and roads. We present empirical evaluations of two separate APEX-enabled systems: the first, deployed on an aerial robot within a controlled environment, and the second, on a wheeled robot operating within a challenging university campus setting.},
author = {Xu, A. and Kalmbach, A. and Dudek, G.},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2014.6907336},
issn = {10504729},
title = {{Adaptive Parameter EXploration (APEX): Adaptation of robot autonomy from human participation}},
year = {2014}
}
@article{MacKay1998,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {MacKay, D.J.C. J C},
doi = {http://dx.doi.org/10.1023/A:1007558615313},
eprint = {arXiv:1011.1669v3},
file = {:home/arnold/Documents/pdf-library/Library/MacKay{\_}1998{\_}Choice of basis for Laplace approximation.pdf:pdf},
isbn = {9788578110796},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {bayes factor,bayesian inference,graphical models,hidden markov models,latent variable models,marginal likelihood},
number = {1},
pages = {77--86},
pmid = {25246403},
title = {{Choice of basis for Laplace approximation}},
url = {http://www.springerlink.com/index/t75546321v4330m0.pdf},
volume = {33},
year = {1998}
}
@article{Henning2012,
abstract = {Latent Dirichlet Allocation models discrete data as a mixture of discrete distributions, using Dirichlet beliefs over the mixture weights. We study a variation of this concept, in which the documents' mixture weight beliefs are replaced with squashed Gaussian distributions. This allows documents to be associated with elements of a Hilbert space, admitting kernel topic models (KTM), modelling temporal, spatial, hierarchical, social and other structure between documents. The main challenge is efficient approximate inference on the latent Gaussian. We present an approximate algorithm cast around a Laplace approximation in a transformed basis. The KTM can also be interpreted as a type of Gaussian process latent variable model, or as a topic model conditional on document features, uncovering links between earlier work in these areas.},
archivePrefix = {arXiv},
arxivId = {1110.4713},
author = {Henning, P and Stern, David and Herbrich, Ralf and Graepel, Thore and Hennig, Philipp},
eprint = {1110.4713},
file = {:home/arnold/Documents/pdf-library/Library/Henning et al.{\_}2012{\_}Kernel Topic Models.pdf:pdf},
issn = {15337928},
journal = {Proceedings of the 15th International Conference on Artificial Intelligence and Statistics (AISTATS)},
keywords = {Kernel methods,Laplace approximation,topic models},
pages = {511--519},
title = {{Kernel Topic Models}},
volume = {22},
year = {2012}
}
@article{Flaspohler2017,
author = {Flaspohler, Genevieve and Roy, Nicholas and Girdhar, Yogesh},
file = {:home/arnold/Documents/pdf-library/Library/Flaspohler, Roy, Girdhar{\_}2017{\_}Feature discovery and visualization of robot mission data using convolutional autoencoders and Bayesian no.pdf:pdf},
journal = {Intelligent Robots and Systems (IROS)},
title = {{Feature discovery and visualization of robot mission data using convolutional autoencoders and Bayesian nonparametric topic modeling}},
year = {2017}
}
@article{Molchanov2017,
abstract = {We explore a recently proposed Variational Dropout technique that provided an elegant Bayesian interpretation to Gaussian Dropout. We extend Variational Dropout to the case when dropout rates are unbounded, propose a way to reduce the variance of the gradient estimator and report first experimental results with individual dropout rates per weight. Interestingly, it leads to extremely sparse solutions both in fully-connected and convolutional layers. This effect is similar to automatic relevance determination effect in empirical Bayes but has a number of advantages. We reduce the number of parameters up to 280 times on LeNet architectures and up to 68 times on VGG-like networks with a negligible decrease of accuracy.},
archivePrefix = {arXiv},
arxivId = {1701.05369},
author = {Molchanov, Dmitry and Ashukha, Arsenii and Vetrov, Dmitry},
eprint = {1701.05369},
file = {:home/arnold/Documents/pdf-library/Library/Molchanov, Ashukha, Vetrov{\_}2017{\_}Variational Dropout Sparsifies Deep Neural Networks.pdf:pdf},
issn = {1938-7228},
title = {{Variational Dropout Sparsifies Deep Neural Networks}},
url = {http://arxiv.org/abs/1701.05369},
year = {2017}
}
@article{Gal2015,
abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1506.02142},
author = {Gal, Yarin and Ghahramani, Zoubin},
eprint = {1506.02142},
file = {:home/arnold/Documents/pdf-library/Library/Gal, Ghahramani{\_}2015{\_}Dropout as a Bayesian Approximation Representing Model Uncertainty in Deep Learning.pdf:pdf},
isbn = {1506.02142},
issn = {1938-7228},
pmid = {88045},
title = {{Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning}},
url = {http://arxiv.org/abs/1506.02142},
volume = {48},
year = {2015}
}
@article{Srivastava2017,
abstract = {Topic models are one of the most popular methods for learning representations of text, but a major challenge is that any change to the topic model requires mathematically deriving a new inference algorithm. A promising approach to address this problem is autoencoding variational Bayes (AEVB), but it has proven diffi- cult to apply to topic models in practice. We present what is to our knowledge the first effective AEVB based inference method for latent Dirichlet allocation (LDA), which we call Autoencoded Variational Inference For Topic Model (AVITM). This model tackles the problems caused for AEVB by the Dirichlet prior and by component collapsing. We find that AVITM matches traditional methods in accuracy with much better inference time. Indeed, because of the inference network, we find that it is unnecessary to pay the computational cost of running variational optimization on test data. Because AVITM is black box, it is readily applied to new topic models. As a dramatic illustration of this, we present a new topic model called ProdLDA, that replaces the mixture model in LDA with a product of experts. By changing only one line of code from LDA, we find that ProdLDA yields much more interpretable topics, even if LDA is trained via collapsed Gibbs sampling.},
archivePrefix = {arXiv},
arxivId = {1703.01488},
author = {Srivastava, Akash and Sutton, Charles},
eprint = {1703.01488},
file = {:home/arnold/Documents/pdf-library/Library/Srivastava, Sutton{\_}2017{\_}Autoencoding Variational Inference For Topic Models.pdf:pdf},
isbn = {9781611970685},
pages = {1--12},
title = {{Autoencoding Variational Inference For Topic Models}},
url = {http://arxiv.org/abs/1703.01488},
year = {2017}
}
@article{Blundell2015,
abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1505.05424},
author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
eprint = {1505.05424},
file = {:home/arnold/Documents/pdf-library/Library/Blundell et al.{\_}2015{\_}Weight Uncertainty in Neural Networks.pdf:pdf},
isbn = {9781510810587},
title = {{Weight Uncertainty in Neural Networks}},
url = {http://arxiv.org/abs/1505.05424},
volume = {37},
year = {2015}
}
@article{Louizos2017,
abstract = {We reinterpret multiplicative noise in neural networks as auxiliary random variables that augment the approximate posterior in a variational setting for Bayesian neural networks. We show that through this interpretation it is both efficient and straightforward to improve the approximation by employing normalizing flows while still allowing for local reparametrizations and a tractable lower bound. In experiments we show that with this new approximation we can significantly improve upon classical mean field for Bayesian neural networks on both predictive accuracy as well as predictive uncertainty.},
archivePrefix = {arXiv},
arxivId = {1703.01961},
author = {Louizos, Christos and Welling, Max},
eprint = {1703.01961},
file = {:home/arnold/Documents/pdf-library/Library/Louizos, Welling{\_}2017{\_}Multiplicative Normalizing Flows for Variational Bayesian Neural Networks.pdf:pdf},
issn = {1938-7228},
title = {{Multiplicative Normalizing Flows for Variational Bayesian Neural Networks}},
url = {http://arxiv.org/abs/1703.01961},
year = {2017}
}
@article{Alemi2016,
abstract = {We present a variational approximation to the information bottleneck of Tishby et al. (1999). This variational approach allows us to parameterize the information bottleneck model using a neural network and leverage the reparameterization trick for efficient training. We call this method "Deep Variational Information Bottleneck", or Deep VIB. We show that models trained with the VIB objective outperform those that are trained with other forms of regularization, in terms of generalization performance and robustness to adversarial attack.},
archivePrefix = {arXiv},
arxivId = {1612.00410},
author = {Alemi, Alexander A and Fischer, Ian and Dillon, Joshua V and Murphy, Kevin},
eprint = {1612.00410},
file = {:home/arnold/Documents/pdf-library/Library/Alemi et al.{\_}2016{\_}Deep Variational Information Bottleneck.pdf:pdf},
pages = {1--19},
title = {{Deep Variational Information Bottleneck}},
url = {http://arxiv.org/abs/1612.00410},
year = {2016}
}
@article{Kingma2015,
abstract = {We investigate a local reparameterizaton technique for greatly reducing the variance of stochastic gradients for variational Bayesian inference (SGVB) of a posterior over model parameters, while retaining parallelizability. This local reparameterization translates uncertainty about global parameters into local noise that is independent across datapoints in the minibatch. Such parameterizations can be trivially parallelized and have variance that is inversely proportional to the minibatch size, generally leading to much faster convergence. Additionally, we explore a connection with dropout: Gaussian dropout objectives correspond to SGVB with local reparameterization, a scale-invariant prior and proportionally fixed posterior variance. Our method allows inference of more flexibly parameterized posteriors; specifically, we propose variational dropout, a generalization of Gaussian dropout where the dropout rates are learned, often leading to better models. The method is demonstrated through several experiments.},
archivePrefix = {arXiv},
arxivId = {1506.02557},
author = {Kingma, Diederik P. and Salimans, Tim and Welling, Max},
eprint = {1506.02557},
file = {:home/arnold/Documents/pdf-library/Library/Kingma, Salimans, Welling{\_}2015{\_}Variational Dropout and the Local Reparameterization Trick.pdf:pdf},
isbn = {1506.02557},
issn = {10495258},
number = {Mcmc},
pages = {1--14},
title = {{Variational Dropout and the Local Reparameterization Trick}},
url = {http://arxiv.org/abs/1506.02557},
year = {2015}
}
@article{Gr,
author = {Gr, Peter and Rooij, Steven De and Erven, Tim Van},
file = {:home/arnold/Documents/pdf-library/Library/Gr, Rooij, Erven{\_}Unknown{\_}The Catch-Up Phenomenon.pdf:pdf},
isbn = {9781424422708},
pages = {259--260},
title = {{The Catch-Up Phenomenon}}
}
@article{Grunwald2008,
abstract = {We consider inference based on a countable set of models (sets of probability distributions), focusing on two tasks: model selection and model averaging. In model selection tasks, the goal is to select the model that best explains the given data. In model averaging, the goal is to find the weighted combination of models that leads to the best prediction of future data from the same source.},
author = {Gr̈unwald, Peter and {De Rooij}, Steven and {Van Erven}, Tim},
doi = {10.1109/ITW.2008.4578662},
file = {:home/arnold/Documents/pdf-library/Library/Gr̈unwald, De Rooij, Van Erven{\_}2008{\_}The catch-up phenomenon.pdf:pdf},
isbn = {9781424422708},
journal = {2008 IEEE Information Theory Workshop, ITW},
pages = {259--260},
title = {{The catch-up phenomenon}},
year = {2008}
}
@incollection{Lacoste-Julien2017,
author = {Lacoste-Julien, Simon},
booktitle = {Structured Prediction - IFT 6085 Notes},
file = {:home/arnold/Documents/pdf-library/Library/Lacoste-Julien{\_}2017{\_}Lecture 1 - Scribbles.pdf:pdf},
pages = {1--3},
title = {{Lecture 8 - Structured SVM Dual (1)}},
year = {2017}
}
@incollection{Lacoste-Julien2017a,
author = {Lacoste-Julien, Simon},
booktitle = {Structured Prediction - IFT 6085 Notes},
title = {{Lecture 17 - Surrogate Loss Functions}},
year = {2017}
}
@incollection{Lacoste-Julien2015,
author = {Lacoste-Julien, Simon},
booktitle = {Introduction to Graphical Models},
file = {:home/arnold/Documents/pdf-library/Library/Lacoste-Julien{\_}2015{\_}Lecture 7 - Sum Product Algorithm and HMMs.pdf:pdf},
number = {November 2014},
pages = {1--17},
title = {{Lecture 7 - Sum Product Algorithm and HMMs}},
volume = {1},
year = {2015}
}
@incollection{Lacoste-Julien2017b,
author = {Lacoste-Julien, Simon},
booktitle = {Structured Prediction - IFT 6085 Notes},
file = {:home/arnold/Documents/pdf-library/Library/Lacoste-Julien{\_}2017{\_}Lecture 4 - Convex Analysis Recap.pdf:pdf},
pages = {1--6},
title = {{Lecture 4 - Convex Analysis Recap}},
year = {2017}
}
@article{Taskar2006,
abstract = {We present a simple and scalable algorithm for maximum-margin estimation of structured output models, including an important class of Markov networks and combinatorial models. We formulate the estimation problem as a convex-concave saddle-point problem that allows us to use simple projection methods based on the dual extragradient algorithm (Nesterov, 2003). The projection step can be solved using dynamic programming or combinatorial algorithms for min-cost convex flow, depending on the structure of the problem. We show that this approach provides a memory-efficient alternative to formulations based on reductions to a quadratic program (QP). We analyze the convergence of the method and present experiments on two very different structured prediction tasks: 3D image segmentation and word alignment, illustrating the favorable scaling properties of our algorithm.},
author = {Taskar, Ben and Lacoste-Julien, Simon and Jordan, Michael I.},
doi = {10.1.1.62.4214},
file = {:home/arnold/Documents/pdf-library/Library/Taskar, Lacoste-Julien, Jordan{\_}2006{\_}Structured Prediction, Dual Extragradient and Bregman Projections.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {The Journal of Machine Learning Research},
keywords = {Computational,Information-Theoretic Learning with,Machine Vision,Natural Language Processing,Theory {\&} Algorithms},
pages = {1627--1653},
title = {{Structured Prediction, Dual Extragradient and Bregman Projections}},
volume = {7},
year = {2006}
}
@incollection{Lacoste-Julien2017c,
author = {Lacoste-Julien, Simon},
booktitle = {Structured Prediction - IFT 6085 Notes},
file = {:home/arnold/Documents/pdf-library/Library/Lacoste-Julien{\_}2017{\_}Lecture 3 - Scribbles.pdf:pdf},
pages = {1--7},
title = {{Lecture 3 - Overview of Surrogate Loss Functions}},
year = {2017}
}
@article{McAllester2007,
annote = {NULL},
author = {McAllester, David},
file = {:home/arnold/Documents/pdf-library/Library/McAllester{\_}2007{\_}Proof of the Occam Bound.pdf:pdf},
number = {ln 2},
pages = {1--9},
title = {{Proof of the Occam Bound}},
volume = {01},
year = {2007}
}
@article{LeCun2006,
abstract = {Energy-Based Models (EBMs) capture dependencies between variables by associating a scalar energy to each configuration of the variables. Inference consists in clamping the value of observed variables and finding configurations of the remaining variables that minimize the energy. Learning consists in finding an energy function in which observed configurations of the variables are given lower energies than unobserved ones. The EBM approach provides a common theoretical framework for many learning models, including traditional discriminative and generative approaches, as well as graph-transformer networks, conditional random fields, maximum margin Markov networks, and several manifold learning methods. Probabilistic models must be properly normalized, which sometimes requires evaluating intractable integrals over the space of all possible variable configurations. Since EBMs have no requirement for proper normalization, this problem is naturally circumvented. EBMs can be viewed as a form of non-probabilistic factor graphs, and they provide considerably more flexibility in the design of architectures and training criteria than probabilistic approaches.},
author = {LeCun, Yann and Chopra, Sumit and Hadsell, Raia and Ranzato, Marc'Aurelio and Huang, Fu Jie},
doi = {10.1198/tech.2008.s913},
file = {:home/arnold/Documents/pdf-library/Library/LeCun et al.{\_}2006{\_}A Tutorial on Energy-Based Learning.pdf:pdf},
isbn = {9780262026178},
issn = {{\textless}null{\textgreater}},
journal = {Predicting Structured Data},
pages = {191--246},
title = {{A Tutorial on Energy-Based Learning}},
year = {2006}
}
@incollection{Lacoste-Julien2015a,
author = {Lacoste-Julien, Simon},
booktitle = {Introduction to Graphical Models},
file = {:home/arnold/Documents/pdf-library/Library/Lacoste-Julien{\_}2015{\_}Lecture 1 - Probabilistic Graphical Models.pdf:pdf},
pages = {1--15},
title = {{Lecture 1 - Probabilistic Graphical Models}},
volume = {100},
year = {2015}
}
@incollection{Lacoste-Julien2015b,
author = {Lacoste-Julien, Simon},
booktitle = {Introduction to Graphical Models},
file = {:home/arnold/Documents/pdf-library/Library/Lacoste-Julien{\_}2015{\_}Lecture 4 - Directed and Undirected Graphical Models.pdf:pdf},
pages = {1--14},
title = {{Lecture 4 - Directed and Undirected Graphical Models}},
volume = {0},
year = {2015}
}
@article{Osokin2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1703.02403v1},
author = {Osokin, Anton and Bach, Francis and Lacoste-julien, Simon},
eprint = {arXiv:1703.02403v1},
file = {:home/arnold/Documents/pdf-library/Library/Osokin, Bach, Lacoste-julien{\_}2017{\_}On Structured Prediction Theory with Calibrated Convex Surrogate Losses.pdf:pdf},
title = {{On Structured Prediction Theory with Calibrated Convex Surrogate Losses}},
year = {2017}
}
@incollection{Lacoste-Julien2017d,
author = {Lacoste-Julien, Simon},
booktitle = {Structured Prediction - IFT 6085 Notes},
file = {:home/arnold/Documents/pdf-library/Library/Lacoste-Julien{\_}2017{\_}Lecture 9 - Structured SVM Dual (2).pdf:pdf},
pages = {1--4},
title = {{Lecture 9 - Structured SVM Dual (2)}},
year = {2017}
}
@incollection{Lacoste-Julien2015c,
author = {Lacoste-Julien, Simon},
booktitle = {Introduction to Graphical Models},
file = {:home/arnold/Documents/pdf-library/Library/Lacoste-Julien{\_}2015{\_}Lecture 8 - Learning in Graphical Models {\&} MC Methods.pdf:pdf},
pages = {1--13},
title = {{Lecture 8 - Learning in Graphical Models {\&} MC Methods}},
year = {2015}
}
@incollection{Lacoste-Julien2015d,
author = {Lacoste-Julien, Simon},
booktitle = {Introduction to Graphical Models},
file = {:home/arnold/Documents/pdf-library/Library/Lacoste-Julien{\_}2015{\_}Lecture 2 - Single Node Models.pdf:pdf},
pages = {1--8},
title = {{Lecture 2 - Single Node Models}},
year = {2015}
}
@incollection{Lacoste-Julien2017e,
author = {Lacoste-Julien, Simon},
booktitle = {Structured Prediction - IFT 6085 Notes},
file = {:home/arnold/Documents/pdf-library/Library/Lacoste-Julien{\_}2017{\_}Lecture 7 - Scribbles.pdf:pdf},
pages = {1--5},
title = {{Lecture 7 - Max Margin Markov Nets}},
year = {2017}
}
@incollection{Lacoste-Julien2017f,
author = {Lacoste-Julien, Simon},
booktitle = {Structured Prediction - IFT 6085 Notes},
file = {:home/arnold/Documents/pdf-library/Library/Lacoste-Julien{\_}2017{\_}Lecture 5 - scribbles.pdf:pdf},
pages = {1--8},
title = {{Lecture 5 - Fundamentals of Convex Optimization}},
year = {2017}
}
@incollection{Lacoste-Julien2017g,
author = {Lacoste-Julien, Simon},
booktitle = {Structured Prediction - IFT 6085 Notes},
file = {:home/arnold/Documents/pdf-library/Library/Lacoste-Julien{\_}2017{\_}Lecture 12 - Frank-Wolfe for SVMstruct.pdf:pdf},
pages = {1--4},
title = {{Lecture 12 - Frank-Wolfe for SVMstruct}},
year = {2017}
}
@article{Lacoste-Julien2013,
abstract = {We propose a randomized block-coordinate variant of the classic Frank-Wolfe algorithm for convex optimization with block-separable constraints. Despite its lower iteration cost, we show that it achieves a similar convergence rate in duality gap as the full Frank-Wolfe algorithm. We also show that, when applied to the dual structural support vector machine (SVM) objective, this yields an online algorithm that has the same low iteration complexity as primal stochastic subgradient methods. However, unlike stochastic subgradient methods, the block-coordinate Frank-Wolfe algorithm allows us to compute the optimal step-size and yields a computable duality gap guarantee. Our experiments indicate that this simple algorithm outperforms competing structural SVM solvers.},
archivePrefix = {arXiv},
arxivId = {1207.4747},
author = {Lacoste-Julien, Simon and Jaggi, Martin and Schmidt, Mark and Pletscher, Patrick},
eprint = {1207.4747},
file = {:home/arnold/Documents/pdf-library/Library/Lacoste-Julien et al.{\_}2013{\_}Block-Coordinate Frank-Wolfe Optimization for Structural SVMs.pdf:pdf},
journal = {International Conference on Machine Learning},
pages = {9},
title = {{Block-Coordinate Frank-Wolfe Optimization for Structural SVMs}},
url = {http://arxiv.org/abs/1207.4747},
volume = {28},
year = {2013}
}
@article{Lacoste-Julien2012,
abstract = {In this note, we present a new averaging technique for the projected stochastic subgradient method. By using a weighted average with a weight of t+1 for each iterate w{\_}t at iteration t, we obtain the convergence rate of O(1/t) with both an easy proof and an easy implementation. The new scheme is compared empirically to existing techniques, with similar performance behavior.},
archivePrefix = {arXiv},
arxivId = {arXiv:1212.2002v2},
author = {Lacoste-Julien, S and Schmidt, Mark and Bach, Francis},
eprint = {arXiv:1212.2002v2},
file = {:home/arnold/Documents/pdf-library/Library/Lacoste-Julien, Schmidt, Bach{\_}2012{\_}A simpler approach to obtaining an O(1t) convergence rate for the projected stochastic subgradient me.pdf:pdf},
journal = {arXiv preprint arXiv:1212.2002},
number = {c},
pages = {1--8},
title = {{A simpler approach to obtaining an O(1/t) convergence rate for the projected stochastic subgradient method}},
url = {http://arxiv.org/abs/1212.2002},
volume = {2},
year = {2012}
}
@incollection{Lacoste-Julien2015e,
author = {Lacoste-Julien, Simon},
booktitle = {Introduction to Graphical Models},
file = {:home/arnold/Documents/pdf-library/Library/Lacoste-Julien{\_}2015{\_}Lecture 5 - Information Theory, Exponential Families.pdf:pdf},
pages = {1--12},
title = {{Lecture 5 - Information Theory, Exponential Families}},
year = {2015}
}
@incollection{Lacoste-Julien2015f,
author = {Lacoste-Julien, Simon},
booktitle = {Introduction to Graphical Models},
file = {:home/arnold/Documents/pdf-library/Library/Lacoste-Julien{\_}2015{\_}Lecture 6 - Directed and Undirected Graphical Models.pdf:pdf},
pages = {1--9},
title = {{Lecture 6 - Directed and Undirected Graphical Models}},
year = {2015}
}
@misc{Vedaldi2013,
abstract = {This tutorial introduces Structured Support Vector Machines (SSVMs) as a tool to effectively learn functions over arbitrary spaces. For example, one can use a SSVM to rank a set of items by decreasing relevance, to localise an object such as a cat in an image, or to estimate the pose of a human in a video. The tutorial reviews the standard notion of SVM and shows how this can be extended to arbitrary output spaces, introducing the corresponding learning formulations. It then gives a complete example on how to design and learn a SSVM with off-the-shelf solvers in MATLAB. The last part discusses how such solvers can be implemented, focusing in particular on the cutting plane and BMRM algorithms. 2 / 85},
author = {Vedaldi, Andrea},
file = {:home/arnold/Documents/pdf-library/Library/Vedaldi{\_}2013{\_}Flexible discriminative learning with structured output support vector machines.pdf:pdf},
keywords = {SSVM},
mendeley-tags = {SSVM},
title = {{Flexible discriminative learning with structured output support vector machines}},
year = {2013}
}
@incollection{Lacoste-Julien2015g,
author = {Lacoste-Julien, Simon},
booktitle = {Introduction to Graphical Models},
file = {:home/arnold/Documents/pdf-library/Library/Lacoste-Julien{\_}2015{\_}Lecture 10 - Bayesian Methods.pdf:pdf},
number = {July},
pages = {1--69},
title = {{Lecture 10 - Bayesian Methods}},
year = {2015}
}
@incollection{Lacoste-Julien2017h,
author = {Lacoste-Julien, Simon},
booktitle = {Structured Prediction - IFT 6085 Notes},
file = {:home/arnold/Documents/pdf-library/Library/Lacoste-Julien{\_}2017{\_}Lecture 2 - Scribbles.pdf:pdf},
pages = {1--6},
title = {{Lecture 2 - Structured Prediction Problem Formulation}},
year = {2017}
}
@incollection{Lacoste-Julien2015h,
author = {Lacoste-Julien, Simon},
booktitle = {Introduction to Graphical Models},
file = {:home/arnold/Documents/pdf-library/Library/Lacoste-Julien{\_}2015{\_}Lecture 3 - k-means, EM, Gaussian Mixture, Graph Theory.pdf:pdf},
pages = {1--15},
title = {{Lecture 3 - k-means, EM, Gaussian Mixture, Graph Theory}},
year = {2015}
}
@incollection{Lacoste-Julien2017i,
author = {Lacoste-Julien, Simon},
booktitle = {Structured Prediction - IFT 6085 Notes},
file = {:home/arnold/Documents/pdf-library/Library/Lacoste-Julien{\_}2017{\_}Lecture 11 - Frank-Wolfe Convergence.pdf:pdf},
title = {{Lecture 11 - Frank-Wolfe Convergence}},
year = {2017}
}
@article{Joachims2009,
abstract = {Discriminative training approaches like structural SVMs have shown much promise for building highly complex and accurate models in areas like natural language processing, protein structure prediction, and information retrieval. However, current training algorithms are computationally expensive or intractable on large datasets. To overcome this bottleneck, this paper explores how cutting-plane methods can provide fast training not only for classification SVMs, but also for structural SVMs. We show that for an equivalent “1-slack” reformulation of the linear SVM training problem, our cutting-plane method has time complexity linear in the number of training examples. In particular, the number of iterations does not depend on the number of training examples, and it is linear in the desired precision and the regularization parameter. Furthermore, we present an extensive empirical evaluation of the method applied to binary classification, multi-class classification, HMM sequence tagging, and CFG parsing. The experiments show that the cutting-plane algorithm is broadly applicable and fast in practice. On large datasets, it is typically several orders of magnitude faster than conventional training methods derived from decomposition methods like SVM-light, or conventional cutting-plane methods. Implementations of our methods are available at                   www.joachims.org                                  .},
author = {Joachims, Thorsten and Finley, Thomas and Yu, Chun Nam John},
doi = {10.1007/s10994-009-5108-8},
file = {:home/arnold/Documents/pdf-library/Library/Joachims, Finley, Yu{\_}2009{\_}Cutting-plane training of structural SVMs.pdf:pdf},
isbn = {0885-6125},
issn = {08856125},
journal = {Machine Learning},
keywords = {Structural SVMs,Structured output prediction,Support vector machines,Training algorithms},
number = {1},
pages = {27--59},
title = {{Cutting-plane training of structural SVMs}},
volume = {77},
year = {2009}
}
@article{Taskar2003,
abstract = {In typical classification tasks, we seek a function which assigns a label to a single object. Kernel-based approaches, such as support vector machines (SVMs), which maximize the margin of confidence of the classifier, are the method of choice for many such tasks. Their popularity stems both from the ability to use high-dimensional feature spaces, and from their strong theoretical guarantees. However, many real-world tasks involve sequential, spatial, or structured data, where multiple labels must be assigned. Existing kernel-based methods ignore structure in the problem, assigning labels independently to each object, losing much useful information. Conversely, probabilistic graphical models, such as Markov networks, can represent correlations between labels, by exploiting problem structure, but cannot handle high-dimensional feature spaces, and lack strong theoretical generalization guarantees. In this paper, we present a new framework that combines the advantages of both approaches: Maximum margin Markov (M3 ) networks incorporate both kernels, which efficiently deal with high-dimensional features, and the ability to capture correlations in structured data. We present an efficient algorithm for learning M3 networks based on a compact quadratic program formulation. We provide a new theoretical bound for generalization in structured domains. Experiments on the task of handwritten character recognition and collective hypertext classification demonstrate very significant gains over previous approaches.},
author = {Taskar, Ben and Guestrin, Carlos and Koller, Daphne},
doi = {10.1.1.129.8439},
file = {:home/arnold/Documents/pdf-library/Library/Taskar, Guestrin, Koller{\_}2003{\_}Max-margin Markov networks.pdf:pdf},
isbn = {1049-5258},
journal = {Advances in Neural Information Processing Systems 16 - NIPS'03},
pages = {25--32},
title = {{Max-margin Markov networks}},
url = {https://www.google.com/books?hl=en{\&}lr={\&}id=0F-9C7K8fQ8C{\&}oi=fnd{\&}pg=PA25{\&}dq=max+margin+markov+networks{\&}ots=THFtlWO741{\&}sig=Kuvbe{\_}fKGhmmB1kTCXm1cemhlmM{\%}5Cnhttp://papers.nips.cc/paper/2397-max-margin-markov-networks.pdf},
year = {2003}
}
@incollection{Lacoste-Julien2017j,
author = {Lacoste-Julien, Simon},
booktitle = {Structured Prediction - IFT 6085 Notes},
file = {:home/arnold/Documents/pdf-library/Library/Lacoste-Julien{\_}2017{\_}Lecture 6 - Scribbles.pdf:pdf},
pages = {1--5},
title = {{Lecture 6 - Approaches to Optimizing Loss-Augmented Decoding}},
year = {2017}
}
@article{Sutton2010,
abstract = {Often we wish to predict a large number of variables that depend on each other as well as on other observed variables. Structured prediction methods are essentially a combination of classification and graphical modeling, combining the ability of graphical models to compactly model multivariate data with the ability of classification methods to perform prediction using large sets of input features. This tutorial describes conditional random fields, a popular probabilistic method for structured prediction. CRFs have seen wide application in natural language processing, computer vision, and bioinformatics. We describe methods for inference and parameter estimation for CRFs, including practical issues for implementing large scale CRFs. We do not assume previous knowledge of graphical modeling, so this tutorial is intended to be useful to practitioners in a wide variety of fields.},
archivePrefix = {arXiv},
arxivId = {1011.4088},
author = {Sutton, Charles and McCallum, Andrew},
doi = {10.1561/2200000013},
eprint = {1011.4088},
file = {:home/arnold/Documents/pdf-library/Library/Sutton, McCallum{\_}2010{\_}An Introduction to Conditional Random Fields.pdf:pdf},
isbn = {9781601985729},
issn = {1935-8237},
journal = {Machine Learning},
number = {4},
pages = {267--373},
pmid = {19008334},
title = {{An Introduction to Conditional Random Fields}},
url = {http://arxiv.org/abs/1011.4088{\%}5Cnhttp://www.arxiv.org/pdf/1011.4088.pdf},
volume = {4},
year = {2010}
}
@incollection{Lacoste-Julien2017k,
author = {Lacoste-Julien, Simon},
booktitle = {Structured Prediction - IFT 6085 Notes},
file = {:home/arnold/Documents/pdf-library/Library/Lacoste-Julien{\_}2017{\_}Lecture 14 - BCFW and Intro to Decision Theory.pdf:pdf},
pages = {1--6},
title = {{Lecture 14 - BCFW and Intro to Decision Theory}},
year = {2017}
}
@article{Germain2016,
abstract = {We exhibit a strong link between frequentist PAC-Bayesian bounds and the Bayesian marginal likelihood. That is, for the negative log-likelihood loss function, we show that the minimization of PAC-Bayesian generalization bounds maximizes the Bayesian marginal likelihood. This provides an alternative explanation to the Bayesian Occam's razor criteria, under the assumption that the data is generated by a i.i.d. distribution. Moreover, as the negative log-likelihood is an unbounded loss function, we motivate and propose a PAC-Bayesian theorem tailored for the sub-Gamma loss family, and we show that our approach is sound on classical Bayesian linear regression tasks.},
archivePrefix = {arXiv},
arxivId = {1605.08636},
author = {Germain, Pascal and Bach, Francis and Lacoste, Alexandre and Lacoste-Julien, Simon},
eprint = {1605.08636},
file = {:home/arnold/Documents/pdf-library/Library/Germain et al.{\_}2016{\_}PAC-Bayesian Theory Meets Bayesian Inference.pdf:pdf},
journal = {arXiv stat.ML},
number = {Nips},
pages = {1--12},
title = {{PAC-Bayesian Theory Meets Bayesian Inference}},
url = {http://arxiv.org/abs/1605.08636},
year = {2016}
}
@book{Boyd2010,
abstract = {We are developing a dual panel breast-dedicated PET system using LSO scintillators coupled to position sensitive avalanche photodiodes (PSAPD). The charge output is amplified and read using NOVA RENA-3 ASICs. This paper shows that the coincidence timing resolution of the RENA-3 ASIC can be improved using certain list-mode calibrations. We treat the calibration problem as a convex optimization problem and use the RENA-3s analog-based timing system to correct the measured data for time dispersion effects from correlated noise, PSAPD signal delays and varying signal amplitudes. The direct solution to the optimization problem involves a matrix inversion that grows order (n3) with the number of parameters. An iterative method using single-coordinate descent to approximate the inversion grows order (n). The inversion does not need to run to convergence, since any gains at high iteration number will be low compared to noise amplification. The system calibration method is demonstrated with measured pulser data as well as with two LSO-PSAPD detectors in electronic coincidence. After applying the algorithm, the 511keV photopeak paired coincidence time resolution from the LSO-PSAPD detectors under study improved by 57{\%}, from the raw value of 16.30.07 ns FWHM to 6.920.02 ns FWHM (11.520.05 ns to 4.890.02 ns for unpaired photons).},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Boyd, Stephen and Vandenberghe, Lieven},
booktitle = {Optimization Methods and Software},
doi = {10.1080/10556781003625177},
eprint = {1111.6189v1},
file = {:home/arnold/Documents/pdf-library/Library/Boyd, Vandenberghe{\_}2010{\_}Convex Optimization.pdf:pdf},
isbn = {9780521833783},
issn = {10556788},
number = {3},
pages = {487--487},
pmid = {20876008},
title = {{Convex Optimization}},
url = {https://web.stanford.edu/{~}boyd/cvxbook/bv{\_}cvxbook.pdf},
volume = {25},
year = {2010}
}
@incollection{Lacoste-Julien2017l,
annote = {NULL},
author = {Lacoste-Julien, Simon},
booktitle = {Structured Prediction - IFT 6085 Notes},
pages = {1--4},
title = {{Lecture 16 - PAC-Bayes}},
year = {2017}
}
@article{Krause2012,
abstract = {Submodularity1 is a property of set functions with deep theoretical consequences and far– reaching applications. At first glance it appears very similar to concavity, in other ways it resembles convexity. It appears in a wide variety of applications: in Computer Science it ... $\backslash$n},
author = {Krause, Andreas and Golovin, Daniel},
doi = {10.1017/CBO9781139177801.004},
file = {:home/arnold/Documents/pdf-library/Library/Krause, Golovin{\_}2012{\_}Submodular function maximization.pdf:pdf},
isbn = {9781139177801},
issn = {{\textless}null{\textgreater}},
journal = {Tractability: Practical Approaches to Hard Problems},
pages = {71--104},
title = {{Submodular function maximization}},
volume = {3},
year = {2012}
}
@incollection{Lacoste-Julien2017m,
author = {Lacoste-Julien, Simon},
booktitle = {Structured Prediction - IFT 6085 Notes},
file = {:home/arnold/Documents/pdf-library/Library/Lacoste-Julien{\_}2017{\_}Lecture 13 - Polytopes.pdf:pdf},
pages = {1--7},
title = {{Lecture 13 - Polytopes}},
year = {2017}
}
@incollection{Lacoste-Julien2017n,
author = {Lacoste-Julien, Simon},
booktitle = {Structured Prediction - IFT 6085 Notes},
file = {:home/arnold/Documents/pdf-library/Library/Lacoste-Julien{\_}2017{\_}Lecture 1 - Introduction to Structured Prediction.pdf:pdf},
pages = {1--5},
title = {{Lecture 1 - Introduction to Structured Prediction}},
year = {2017}
}
@incollection{Lacoste-Julien2017o,
author = {Lacoste-Julien, Simon},
booktitle = {Structured Prediction - IFT 6085 Notes},
file = {:home/arnold/Documents/pdf-library/Library/Lacoste-Julien{\_}2017{\_}Lecture 10 - The Frank-Wolfe Optimization Algorithm.pdf:pdf},
title = {{Lecture 10 - The Frank-Wolfe Optimization Algorithm}},
year = {2017}
}
@article{McAllester2007a,
annote = {NULL},
author = {McAllester, David},
file = {:home/arnold/Documents/pdf-library/Library/McAllester{\_}2007{\_}Proof of the Occam Bound.pdf:pdf},
number = {ln 2},
pages = {1--9},
title = {{Proof of the Occam Bound}},
volume = {01},
year = {2007}
}
@incollection{Lacoste-Julien2017p,
annote = {NULL},
author = {Lacoste-Julien, Simon},
booktitle = {Structured Prediction - IFT 6085 Notes},
pages = {1--4},
title = {{Lecture 15 - Decision Theory}},
year = {2017}
}
@phdthesis{Brefeld2008,
author = {Brefeld, Ulf},
file = {:home/arnold/Documents/pdf-library/Library/Ulf{\_}2008{\_}Semi-supervised Structured Prediction Models.pdf:pdf},
pages = {182},
school = {Humboldt-Universit{\"{a}}t zu Berlin},
title = {{Semi-supervised Structured Prediction Models}},
year = {2008}
}
@article{Mccallum2006,
author = {Mccallum, Andrew and Pal, Chris and Druck, Greg and Wang, Xuerui},
file = {:home/arnold/Documents/pdf-library/Library/Mccallum et al.{\_}2006{\_}Multi-Conditional Learning Generative Discriminative Training for Clustering and Classification.pdf:pdf},
journal = {American Association for Artificial Intelligence},
title = {{Multi-Conditional Learning : Generative / Discriminative Training for Clustering and Classification}},
year = {2006}
}
@article{Taskar2005,
abstract = {We consider large margin estimation in a broad range of prediction models where inference involves solving combinatorial optimization problems, for example, weighted graph-cuts or matchings. Our goal is to learn parameters such that inference using the model reproduces correct answers on the training data. Our method relies on the expressive power of convex optimization problems to compactly capture inference or solution optimality in structured prediction models. Directly embedding this structure within the learning formulation produces concise convex problems for efficient estimation of very complex and diverse models. We describe experimental results on a matching task, disulfide connectivity prediction, showing significant improvements over state-of-the-art methods.},
author = {Taskar, Ben and Chatalbashev, Vassil and Koller, Daphne and Guestrin, Carlos},
doi = {10.1145/1102351.1102464},
file = {:home/arnold/Documents/pdf-library/Library/Taskar et al.{\_}2005{\_}Learning structured prediction models A large margin approach.pdf:pdf},
isbn = {1595931805},
issn = {1595931805},
journal = {Proceedings of the 22nd international conference on Machine learning},
number = {December},
pages = {896--903},
title = {{Learning structured prediction models: A large margin approach}},
url = {http://dx.doi.org/10.1145/1102351.1102464},
year = {2005}
}
@article{Golovin2011,
abstract = {Solving stochastic optimization problems under partial observability, where one needs to adaptively make decisions with uncertain outcomes, is a fundamental but notoriously difficult challenge. In this paper, we introduce the concept of adaptive submodularity, generalizing submodular set functions to adaptive policies. We prove that if a problem satisfies this property, a simple adaptive greedy algorithm is guaranteed to be competitive with the optimal policy. In addition to providing performance guarantees for both stochastic maximization and coverage, adaptive submodularity can be exploited to drastically speed up the greedy algorithm by using lazy evaluations. We illustrate the usefulness of the concept by giving several examples of adaptive submodular objectives arising in diverse applications including sensor placement, viral marketing and active learning. Proving adaptive submodularity for these problems allows us to recover existing results in these applications as special cases, improve approximation guarantees and handle natural generalizations.},
archivePrefix = {arXiv},
arxivId = {1003.3967},
author = {Golovin, Daniel and Krause, Andreas},
doi = {10.1613/jair.3278},
eprint = {1003.3967},
file = {:home/arnold/Documents/pdf-library/Library/Golovin, Krause{\_}2011{\_}Adaptive submodularity Theory and applications in active learning and stochastic optimization.pdf:pdf},
isbn = {1076-9757},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
keywords = {active,adaptive optimization,learning,optimal decision trees,partial observability,stochastic optimization,submodularity},
number = {1},
pages = {427--486},
title = {{Adaptive submodularity: Theory and applications in active learning and stochastic optimization}},
url = {http://www.jair.org/papers/paper3278.html},
volume = {42},
year = {2011}
}
@article{Smith2014,
author = {Smith, Brian Cantwell},
file = {:home/arnold/Documents/pdf-library/Library/Smith{\_}2014{\_}A Rough and Ready Guide to the tortuous landscape of metaphysics, ontology, and epistemology.pdf:pdf},
title = {{A Rough and Ready Guide to the tortuous landscape of metaphysics, ontology, and epistemology}},
year = {2014}
}
@article{Woolley2010,
abstract = {applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Woolley, Anita Williams and Chabris, Christopher F. and Pentland, Alex and Hashmi, Nada and Malone, Thomas W.},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/arnold/Documents/pdf-library/Library/Woolley et al.{\_}2010{\_}Evidence for a Collective Intelligence Factor in the Performance of Human Groups.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Journal of Chemical Information and Modeling},
keywords = {icle},
pages = {686--688},
pmid = {25246403},
title = {{Evidence for a Collective Intelligence Factor in the Performance of Human Groups}},
volume = {330},
year = {2010}
}
@article{Salminen2012,
abstract = {This literature review focuses on collective intelligence in humans. A keyword search was performed on the Web of Knowledge and selected papers were reviewed in order to reveal themes relevant to collective intelligence. Three levels of abstraction were identified in discussion about the phenomenon: the micro-level, the macro-level and the level of emergence. Recurring themes in the literature were categorized under the above-mentioned framework and directions for future research were identified.},
archivePrefix = {arXiv},
arxivId = {1204.3401},
author = {Salminen, Juho},
eprint = {1204.3401},
file = {:home/arnold/Documents/pdf-library/Library/Salminen{\_}2012{\_}Collective Intelligence in Humans a Literature Review.pdf:pdf},
journal = {Arxiv preprint arXiv12043401},
pages = {1 -- 8},
title = {{Collective Intelligence in Humans : a Literature Review}},
url = {http://arxiv.org/abs/1204.3401},
volume = {abs/1204.3},
year = {2012}
}
@article{Smith1996,
abstract = {On the Origin of Objects is the culmination of Brian Cantwell Smith's decade-long investigation into the philosophical and metaphysical foundations of computation, artificial intelligence, and cognitive science. Based on a sustained critique of the formal tradition that underlies the reigning views, he presents an argument for an embedded, participatory, "irreductionist," metaphysical alternative. Smith seeks nothing less than to revise our understanding not only of the machines we build but also of the world with which they interact. Smith's ambitious project begins as a search for a comprehensive theory of computation, able to do empirical justice to practice and conceptual justice to the computational theory of mind. A rigorous commitment to these two criteria ultimately leads him to recommend a radical overhaul of our traditional conception of metaphysics. Everything that exists - objects, properties, life, practice - lies Smith claims in the "middle distance," an intermediate realm of partial engagement with and partial separation from, the enveloping world. Patterns of separation and engagement are taken to underlie a single notion unifying representation and ontology: that of subjects' "registration" of the world around them. Along the way, Smith offers many fascinating ideas: the distinction between particularity and individuality, the methodological notion of an "inscription error," an argument that there are no individualswithin physics, various deconstructions of the type-instance distinction, an analysis of formality as overly disconnected ("discreteness run amok"), a conception of the boundaries of objects as properties of unruly interactions between objects and subjects, an argument for the theoretical centrality of reference preservation, and a theatrical, acrobatic metaphor for the contortions involved in the preservation of reference and resultant stabilization of objects. Sidebars and diagrams throughout the book help clarify and guide Smith's highly original and compelling argument.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Smith, Brian Cantwell},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/arnold/Documents/pdf-library/Library/Smith{\_}1996{\_}On the Origin of Objects.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {MIT Press, Cambridge, MA, USA. Steedman, M},
pages = {834--839},
pmid = {25246403},
title = {{On the Origin of Objects}},
volume = {pages},
year = {1996}
}
@article{Pedersen2012,
author = {Pedersen, M and Hardeberg, J Y},
file = {:home/arnold/Documents/pdf-library/Library/Pedersen, Hardeberg{\_}2012{\_}Full-Reference Image Quality Metrics Classification and Evaluation.pdf:pdf},
journal = {Foundations and Trends in Computer Graphics and Vision},
number = {1},
pages = {1--80},
title = {{Full-Reference Image Quality Metrics: Classification and Evaluation}},
volume = {7},
year = {2012}
}
@article{Luan2017,
abstract = {This paper introduces a deep-learning approach to photographic style transfer that handles a large variety of image content while faithfully transferring the reference style. Our approach builds upon the recent work on painterly transfer that separates style from the content of an image by considering different layers of a neural network. However, as is, this approach is not suitable for photorealistic style transfer. Even when both the input and reference images are photographs, the output still exhibits distortions reminiscent of a painting. Our contribution is to constrain the transformation from the input to the output to be locally affine in colorspace, and to express this constraint as a custom fully differentiable energy term. We show that this approach successfully suppresses distortion and yields satisfying photorealistic style transfers in a broad variety of scenarios, including transfer of the time of day, weather, season, and artistic edits.},
archivePrefix = {arXiv},
arxivId = {1703.07511},
author = {Luan, Fujun and Paris, Sylvain and Shechtman, Eli and Bala, Kavita},
doi = {10.1109/CVPR.2017.740},
eprint = {1703.07511},
file = {:home/arnold/Documents/pdf-library/Library/Luan et al.{\_}2017{\_}Deep Photo Style Transfer.pdf:pdf},
pages = {1--21},
title = {{Deep Photo Style Transfer}},
url = {http://arxiv.org/abs/1703.07511},
year = {2017}
}
@article{Smith,
author = {Smith, Schuyler},
file = {:home/arnold/Documents/pdf-library/Library/Smith{\_}Unknown{\_}Subjective Image Quality Evaluation.pdf:pdf},
title = {{Subjective Image Quality Evaluation}}
}
@article{Christiano2017,
abstract = {For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.},
archivePrefix = {arXiv},
arxivId = {1706.03741},
author = {Christiano, Paul and Leike, Jan and Brown, Tom B. and Martic, Miljan and Legg, Shane and Amodei, Dario},
eprint = {1706.03741},
file = {:home/arnold/Documents/pdf-library/Library/Christiano et al.{\_}2017{\_}Deep reinforcement learning from human preferences.pdf:pdf},
journal = {arXi},
title = {{Deep reinforcement learning from human preferences}},
url = {http://arxiv.org/abs/1706.03741},
year = {2017}
}
@article{Lovasz2006,
abstract = {We prove that the hit-and-run random walk is rapidly mixing for an arbitrary logconcave distribution starting from any point in the support. This extends the work of Lovasz and Vempala (2004), where this was shown for an important special case, and settles the main conjecture formulated there. From this result, we derive asymptotically faster algorithms in the general oracle model for sampling, rounding, integration and maximization of logconcave functions, improving or generalizing the main results of Lovasz and Vempala (2003), Applegate and Kannan (1990) and Kalai and Vempala respectively. The algorithms for integration and optimization both use sampling and are surprisingly similar},
author = {Lov{\'{a}}sz, L and Vempala, Santosh},
doi = {10.1109/FOCS.2006.28},
file = {:home/arnold/Documents/pdf-library/Library/Lov{\'{a}}sz, Vempala{\_}2006{\_}Fast algorithms for logconcave functions Sampling, rounding, integration and optimization.pdf:pdf},
isbn = {0769527205},
issn = {02725428},
journal = {Proceedings - Annual IEEE Symposium on Foundations of Computer Science, FOCS},
pages = {57--66},
title = {{Fast algorithms for logconcave functions: Sampling, rounding, integration and optimization}},
volume = {c},
year = {2006}
}
@article{Lovasz2003,
abstract = {In recent years, the problem of sampling a convex body has received much attention, and many efficient solutions have been proposed [4, 7, 12, 10, 9], all based on random walks. Of these, the hit-and-run random walk, first proposed by Smith [19], has the same worst-case ... $\backslash$n},
author = {Lov{\'{a}}sz, L and Vempala, Santosh},
file = {:home/arnold/Documents/pdf-library/Library/Lov{\'{a}}sz, Vempala{\_}2003{\_}Hit-and-Run is Fast and Fun.pdf:pdf},
journal = {preprint, Microsoft Research},
pages = {1--28},
title = {{Hit-and-Run is Fast and Fun}},
url = {http://ftp.cs.elte.hu/{~}lovasz/logcon-hitrun.pdf},
year = {2003}
}
@article{Sadigh2017,
abstract = {—Our goal is to efficiently learn reward functions encoding a human's preferences for how a dynamical system should act. There are two challenges with this. First, in many problems it is difficult for people to provide demonstrations of the desired system trajectory (like a high-DOF robot arm motion or an aggressive driving maneuver), or to even assign how much numerical reward an action or trajectory should get. We build on work in label ranking and propose to learn from preferences (or comparisons) instead: the person provides the system a relative preference between two trajec-tories. Second, the learned reward function strongly depends on what environments and trajectories were experienced during the training phase. We thus take an active learning approach, in which the system decides on what preference queries to make. A novel aspect of our work is the complexity and continuous nature of the queries: continuous trajectories of a dynamical system in environments with other moving agents (humans or robots). We contribute a method for actively synthesizing queries that satisfy the dynamics of the system. Further, we learn the reward function from a continuous hypothesis space by maximizing the volume removed from the hypothesis space by each query. We assign weights to the hypothesis space in the form of a log-concave distribution and provide a bound on the number of iterations required to converge. We show that our algorithm converges faster to the desired reward compared to approaches that are not active or that do not synthesize queries in an autonomous driving domain. We then run a user study to put our method to the test with real people.},
author = {Sadigh, Dorsa and Dragan, Anca D and Sastry, Shankar and Seshia, Sanjit A},
doi = {10.15607/RSS.2017.XIII.053},
file = {:home/arnold/Documents/pdf-library/Library/Sadigh et al.{\_}2017{\_}Active Preference-Based Learning of Reward Functions.pdf:pdf},
journal = {Robotics: Science and Systems (RSS)},
title = {{Active Preference-Based Learning of Reward Functions}},
year = {2017}
}
@article{Zhang,
archivePrefix = {arXiv},
arxivId = {arXiv:1605.06450v1},
author = {Zhang, Jiakai},
eprint = {arXiv:1605.06450v1},
file = {:home/arnold/Documents/pdf-library/Library/Zhang{\_}Unknown{\_}Query-Efficient Imitation Learning for End-to-End Autonomous Driving.pdf:pdf},
title = {{Query-Efficient Imitation Learning for End-to-End Autonomous Driving}}
}
@article{Fischer,
archivePrefix = {arXiv},
arxivId = {arXiv:1504.06852v2},
author = {Fischer, Philipp and Ilg, Eddy and Philip, H and Hazırbas, Caner and Smagt, Patrick Van Der and Cremers, Daniel and Brox, Thomas},
eprint = {arXiv:1504.06852v2},
file = {:home/arnold/Documents/pdf-library/Library/Fischer et al.{\_}Unknown{\_}FlowNet Learning Optical Flow with Convolutional Networks.pdf:pdf},
title = {{FlowNet: Learning Optical Flow with Convolutional Networks}}
}
@article{Cao2016,
abstract = {We present an approach to efficiently detect the 2D pose of multiple people in an image. The approach uses a nonparametric representation, which we refer to as Part Affinity Fields (PAFs), to learn to associate body parts with individuals in the image. The architecture encodes global context, allowing a greedy bottom-up parsing step that maintains high accuracy while achieving realtime performance, irrespective of the number of people in the image. The architecture is designed to jointly learn part locations and their association via two branches of the same sequential prediction process. Our method placed first in the inaugural COCO 2016 keypoints challenge, and significantly exceeds the previous state-of-the-art result on the MPII Multi-Person benchmark, both in performance and efficiency.},
archivePrefix = {arXiv},
arxivId = {1611.08050},
author = {Cao, Zhe and Simon, Tomas and Wei, Shih-en and Sheikh, Yaser},
doi = {10.1109/CVPR.2017.143},
eprint = {1611.08050},
file = {:home/arnold/Documents/pdf-library/Library/Cao et al.{\_}Unknown{\_}Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields ∗.pdf:pdf},
isbn = {978-1-5386-0457-1},
journal = {Arxiv preprint 1611.08050},
title = {{Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields}},
url = {http://arxiv.org/abs/1611.08050},
year = {2016}
}
@article{Hepp2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1705.09314v1},
author = {Hepp, Benjamin and Niessner, Matthias and Hilliges, Otmar},
eprint = {arXiv:1705.09314v1},
file = {:home/arnold/Documents/pdf-library/Library/Hepp, Niessner, Hilliges{\_}2017{\_}Plan3D Viewpoint and Trajectory Optimization for Aerial Multi-View Stereo Reconstruction.pdf:pdf},
number = {May},
title = {{Plan3D : Viewpoint and Trajectory Optimization for Aerial Multi-View Stereo Reconstruction}},
year = {2017}
}
@article{Wang,
author = {Wang, Sen and Clark, Ronald and Wen, Hongkai and Trigoni, Niki},
file = {:home/arnold/Documents/pdf-library/Library/Wang et al.{\_}Unknown{\_}DeepVO Towards End-to-End Visual Odometry with Deep Recurrent Convolutional Neural Networks.pdf:pdf},
title = {{DeepVO : Towards End-to-End Visual Odometry with Deep Recurrent Convolutional Neural Networks}}
}
@article{Freer2013,
author = {Freer, C E},
doi = {10.1103/PhysRevLett.110.168702},
file = {:home/arnold/Documents/pdf-library/Library/Freer{\_}2013{\_}Causal Entropic Forces.pdf:pdf},
number = {April},
pages = {1--5},
title = {{Causal Entropic Forces}},
volume = {168702},
year = {2013}
}
@article{Bojarski,
archivePrefix = {arXiv},
arxivId = {arXiv:1604.07316v1},
author = {Bojarski, Mariusz and Testa, Davide Del and Goyal, Prasoon and Zhang, Jiakai and Dworakowski, Daniel and Jackel, Lawrence D and Firner, Bernhard and Monfort, Mathew and Zhao, Jake and Zieba, Karol},
eprint = {arXiv:1604.07316v1},
file = {:home/arnold/Documents/pdf-library/Library/Bojarski et al.{\_}Unknown{\_}End to End Learning for Self-Driving Cars.pdf:pdf},
pages = {1--9},
title = {{End to End Learning for Self-Driving Cars}}
}
@article{Ross2010,
abstract = {Sequential prediction problems such as imitation learning, where future observations depend on previous predictions (actions), violate the common i.i.d. assumptions made in statistical learning. This leads to poor performance in theory and often in practice. Some recent approaches provide stronger guarantees in this setting, but remain somewhat unsatisfactory as they train either non-stationary or stochastic policies and require a large number of iterations. In this paper, we propose a new iterative algorithm, which trains a stationary deterministic policy, that can be seen as a no regret algorithm in an online learning setting. We show that any such no regret algorithm, combined with additional reduction assumptions, must find a policy with good performance under the distribution of observations it induces in such sequential settings. We demonstrate that this new approach outperforms previous approaches on two challenging imitation learning problems and a benchmark sequence labeling problem.},
archivePrefix = {arXiv},
arxivId = {1011.0686},
author = {Ross, Stephane and Gordon, Geoffrey J and Bagnell, J Andrew},
eprint = {1011.0686},
file = {:home/arnold/Documents/pdf-library/Library/Ross, Gordon, Bagnell{\_}2010{\_}A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning.pdf:pdf},
issn = {{\textless}null{\textgreater}},
journal = {Aistats},
pages = {627--635},
title = {{A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning}},
url = {http://arxiv.org/abs/1011.0686},
volume = {15},
year = {2010}
}
@article{Daftry,
archivePrefix = {arXiv},
arxivId = {arXiv:1608.00627v1},
author = {Daftry, Shreyansh and Bagnell, J Andrew and Hebert, Martial},
eprint = {arXiv:1608.00627v1},
file = {:home/arnold/Documents/pdf-library/Library/Daftry, Bagnell, Hebert{\_}Unknown{\_}Reactive MAV Control.pdf:pdf},
keywords = {autonomous monocular navigation,domain adaptation,micro aerial vehicles,reactive control,transfer learning},
title = {{Reactive MAV Control}}
}
@article{Ziebart2010,
author = {Ziebart, Brian D and Bagnell, J Andrew},
file = {:home/arnold/Documents/pdf-library/Library/Ziebart, Bagnell{\_}2010{\_}Modeling Interaction via the Principle of Maximum Causal Entropy.pdf:pdf},
title = {{Modeling Interaction via the Principle of Maximum Causal Entropy}},
year = {2010}
}
@article{Rasmus2015,
abstract = {We combine supervised learning with unsupervised learning in deep neural networks. The proposed model is trained to simultaneously minimize the sum of supervised and unsupervised cost functions by backpropagation, avoiding the need for layer-wise pretraining. Our work builds on top of the Ladder network proposed by Valpola (2015) which we extend by combining the model with supervision. We show that the resulting model reaches state-of-the-art performance in various tasks: MNIST and CIFAR-10 classification in a semi-supervised setting and permutation invariant MNIST in both semi-supervised and full-labels setting.},
archivePrefix = {arXiv},
arxivId = {arXiv:1507.02672v1},
author = {Rasmus, Antti and Valpola, Harri and Berglund, Mathias},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1507.02672v1},
file = {:home/arnold/Documents/pdf-library/Library/Rasmus, Valpola, Berglund{\_}2015{\_}Semi-Supervised Learning with Ladder Network.pdf:pdf},
isbn = {9788578110796},
issn = {10495258},
journal = {arXiv},
pages = {1--17},
pmid = {25246403},
title = {{Semi-Supervised Learning with Ladder Network}},
year = {2015}
}
@article{Valpola2014,
abstract = {A network supporting deep unsupervised learning is presented. The network is an autoencoder with lateral shortcut connections from the encoder to decoder at each level of the hierarchy. The lateral shortcut connections allow the higher levels of the hierarchy to focus on abstract invariant features. While standard autoencoders are analogous to latent variable models with a single layer of stochastic variables, the proposed network is analogous to hierarchical latent variables models. Learning combines denoising autoencoder and denoising sources separation frameworks. Each layer of the network contributes to the cost function a term which measures the distance of the representations produced by the encoder and the decoder. Since training signals originate from all levels of the network, all layers can learn efficiently even in deep networks. The speedup offered by cost terms from higher levels of the hierarchy and the ability to learn invariant features are demonstrated in experiments.},
archivePrefix = {arXiv},
arxivId = {1411.7783},
author = {Valpola, Harri},
doi = {10.1016/B978-0-12-802806-3.00008-7},
eprint = {1411.7783},
file = {:home/arnold/Documents/pdf-library/Library/Valpola{\_}2014{\_}From neural PCA to deep unsupervised learning.pdf:pdf},
isbn = {9780128028063},
pages = {1--23},
title = {{From neural PCA to deep unsupervised learning}},
url = {http://arxiv.org/abs/1411.7783},
year = {2014}
}
@article{Dasgupta2011,
abstract = {An active learner has a collection of data points, each with a label that is initially hidden but can be obtained at some cost. Without spending too much, it wishes to find a classifier that will accurately map points to labels. There are two common intuitions about how this learning process should be organized: (i) by choosing query points that shrink the space of candidate classifiers as rapidly as possible; and (ii) by exploiting natural clusters in the (unlabeled) data set. Recent research has yielded learning algorithms for both paradigms that are efficient, work with generic hypothesis classes, and have rigorously characterized labeling requirements. Here we survey these advances by focusing on two representative algorithms and discussing their mathematical properties and empirical performance. ?? 2010 Elsevier B.V. All rights reserved.},
author = {Dasgupta, Sanjoy},
doi = {10.1016/j.tcs.2010.12.054},
file = {:home/arnold/Documents/pdf-library/Library/Dasgupta{\_}2011{\_}Two faces of active learning.pdf:pdf},
isbn = {3642047467},
issn = {03043975},
journal = {Theoretical Computer Science},
keywords = {Active learning,Learning theory},
number = {19},
pages = {1767--1781},
title = {{Two faces of active learning}},
volume = {412},
year = {2011}
}
@article{Pezeshki2015,
abstract = {The Manual labeling of data is and will remain a costly endeavor. For this reason, semi-supervised learning remains a topic of practical importance. The recently proposed Ladder Network is one such approach that has proven to be very successful. In addition to the supervised objective, the Ladder Network also adds an unsupervised objective corresponding to the reconstruction costs of a stack of denoising autoencoders. Although the empirical results are impressive, the Ladder Network has many components intertwined, whose contributions are not obvious in such a complex architecture. In order to help elucidate and disentangle the different ingredients in the Ladder Network recipe, this paper presents an extensive experimental investigation of variants of the Ladder Network in which we replace or remove individual components to gain more insight into their relative importance. We find that all of the components are necessary for achieving optimal performance, but they do not contribute equally. For semi-supervised tasks, we conclude that the most important contribution is made by the lateral connection, followed by the application of noise, and finally the choice of what we refer to as the `combinator function' in the decoder path. We also find that as the number of labeled training examples increases, the lateral connections and reconstruction criterion become less important, with most of the improvement in generalization being due to the injection of noise in each layer. Furthermore, we present a new type of combinator function that outperforms the original design in both fully- and semi-supervised tasks, reducing record test error rates on Permutation-Invariant MNIST to 0.57{\%} for the supervised setting, and to 0.97{\%} and 1.0{\%} for semi-supervised settings with 1000 and 100 labeled examples respectively.},
archivePrefix = {arXiv},
arxivId = {1511.06430},
author = {Pezeshki, Mohammad and Fan, Linxi and Brakel, Philemon and Courville, Aaron and Bengio, Yoshua},
eprint = {1511.06430},
file = {:home/arnold/Documents/pdf-library/Library/Pezeshki et al.{\_}2015{\_}Deconstructing the Ladder Network Architecture.pdf:pdf},
isbn = {9290795158},
issn = {9290795158},
title = {{Deconstructing the Ladder Network Architecture}},
url = {http://arxiv.org/abs/1511.06430},
volume = {48},
year = {2015}
}
@article{Ioffe2015,
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9{\%} top-5 validation error (and 4.8{\%} test error), exceeding the accuracy of human raters.},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Ioffe, Sergey and Szegedy, Christian},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1502.03167},
file = {:home/arnold/Documents/pdf-library/Library/Ioffe, Szegedy{\_}2015{\_}Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.pdf:pdf},
isbn = {9780874216561},
issn = {0717-6163},
pmid = {15003161},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
url = {http://arxiv.org/abs/1502.03167},
year = {2015}
}
@article{Settles2010,
abstract = {The key idea behind active learning is that a machine learning algorithm can achieve greater accuracy with fewer training labels if it is allowed to choose the data from which it learns. An active learner may pose queries, usually in the form of unlabeled data instances to be labeled by an oracle (e.g., a human annotator). Active learning is well-motivated in many modern machine learning problems, where unlabeled data may be abundant or easily obtained, but labels are difficult, time-consuming, or expensive to obtain. This report provides a general introduction to active learning and a survey of the literature. This includes a discussion of the scenarios in which queries can be formulated, and an overview of the query strategy frameworks proposed in the literature to date. An analysis of the empirical and theoretical evidence for successful active learning, a summary of problem setting variants and practical issues, and a discussion of related topics in machine learning research are also presented.},
archivePrefix = {arXiv},
arxivId = {1206.5533},
author = {Settles, Burr},
doi = {10.1.1.167.4245},
eprint = {1206.5533},
file = {:home/arnold/Documents/pdf-library/Library/Settles{\_}2010{\_}Active Learning Literature Survey.pdf:pdf},
isbn = {978-1-4673-8391-2},
issn = {00483931},
journal = {Machine Learning},
number = {2},
pages = {201--221},
pmid = {15003161},
title = {{Active Learning Literature Survey}},
volume = {15},
year = {2010}
}
@article{Girshick2015,
abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
archivePrefix = {arXiv},
arxivId = {1504.08083},
author = {Girshick, Ross},
doi = {10.1109/ICCV.2015.169},
eprint = {1504.08083},
file = {:home/arnold/Documents/pdf-library/Library/Girshick{\_}2015{\_}Fast R-CNN.pdf:pdf},
isbn = {978-1-4673-8391-2},
issn = {978-1-4673-8391-2},
pmid = {23739795},
title = {{Fast R-CNN}},
url = {http://arxiv.org/abs/1504.08083},
year = {2015}
}
@book{Stroustrup2013,
abstract = {The new C++11 standard allows programmers to express ideas more clearly, simply, and directly, and to write faster, more efficient code. Bjarne Stroustrup, the designer and original implementer of C++, has reorganized, extended, and completely rewritten his definitive reference and tutorial for programmers who want to use C++ most effectively.      The C++ Programming Language, Fourth Edition,   delivers meticulous, richly explained, and integrated coverage of the entire language—its facilities, abstraction mechanisms, standard libraries, and key design techniques. Throughout, Stroustrup presents concise, “pure C++11” examples, which have been carefully crafted to clarify both usage and program design. To promote deeper understanding, the author provides extensive cross-references, both within the book and to the ISO standard.   New C++11 coverage includes  Support for concurrency   Regular expressions, resource management pointers, random numbers, and improved containers   General and uniform initialization, simplified for-statements, move semantics, and Unicode support   Lambdas, general constant expressions, control over class defaults, variadic templates, template aliases, and user-defined literals   Compatibility issues   Topics addressed in this comprehensive book include  Basic facilities: type, object, scope, storage, computation fundamentals, and more   Modularity, as supported by namespaces, source files, and exception handling   C++ abstraction, including classes, class hierarchies, and templates in support of a synthesis of traditional programming, object-oriented programming, and generic programming   Standard Library: containers, algorithms, iterators, utilities, strings, stream I/O, locales, numerics, and more   The C++ basic memory model, in depth   This fourth edition makes C++11 thoroughly accessible to programmers moving from C++98 or other languages, while introducing insights and techniques that even cutting-edge C++11 programmers will find indispensable.    This book features an enhanced, layflat binding, which allows the book to stay open more easily when placed on a flat surface. This special binding method—noticeable by a small space inside the spine—also increases durability.},
author = {Stroustrup, Bjarne},
doi = {DOI-Test},
file = {:home/arnold/Documents/pdf-library/Library/Stroustrup{\_}2013{\_}The C Programming Language.pdf:pdf},
isbn = {9780321563842},
keywords = {623923},
pages = {1366},
title = {{The C ++ Programming Language}},
year = {2013}
}
@article{Nagy2011,
abstract = {The objective of this book is to expose the use of the Waf build system though the use of Waf in practice, the description of the Waf extension system, and an overview of the Waf internals. We hope that this book will serve as a reference for both new and advanced users. Although this book does not deal with build systems in general, a secondary objective is to illustrate quite a few new techniques and patterns through numerous examples. The chapters are ordered by difficulty, starting from the basic use of Waf and Python, and diving gradually into the most difficult topics. It is therefore recommended to read the chapters in order. It is also possible to start by looking at the examples from the Waf distribution before starting the reading.},
author = {Nagy, Thomas},
file = {:home/arnold/Documents/pdf-library/Library/Nagy{\_}2011{\_}The Waf Book.pdf:pdf},
keywords = {Thomas Nagy},
pages = {73},
title = {{The Waf Book}},
url = {http://docs.waf.googlecode.com/git/book{\_}17/single.html},
year = {2011}
}
@article{Simonyan2015,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
doi = {10.1016/j.infsof.2008.09.005},
eprint = {1409.1556},
file = {:home/arnold/Documents/pdf-library/Library/Simonyan, Zisserman{\_}2015{\_}Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf:pdf},
isbn = {9781450341448},
issn = {09505849},
journal = {International Conference on Learning Representations (ICRL)},
pages = {1--14},
pmid = {16873662},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {http://arxiv.org/abs/1409.1556},
year = {2015}
}
@article{Ren2015,
abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [7] and Fast R-CNN [5] have reduced the running time of these detection networks, exposing region pro-posal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolu-tional features. For the very deep VGG-16 model [18], our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2{\%} mAP) and 2012 (70.4{\%} mAP) using 300 proposals per image. The code will be released.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.01497v1},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
doi = {10.1016/j.nima.2015.05.028},
eprint = {arXiv:1506.01497v1},
file = {:home/arnold/Documents/pdf-library/Library/Ren et al.{\_}2015{\_}Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks.pdf:pdf},
isbn = {0162-8828 VO - PP},
issn = {01689002},
journal = {Nips},
pages = {1--10},
pmid = {27295650},
title = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}},
year = {2015}
}
@article{Gal2017,
archivePrefix = {arXiv},
arxivId = {1703.02910},
author = {Gal, Yarin},
eprint = {1703.02910},
file = {:home/arnold/Documents/pdf-library/Library/Gal{\_}2017{\_}Deep Bayesian Active Learning with Image Data.pdf:pdf},
journal = {arXiv preprint arXiv:1703.02910},
title = {{Deep Bayesian Active Learning with Image Data}},
year = {2017}
}
@book{Shalizi2013,
author = {Shalizi, Cosma Rohilla},
file = {:home/arnold/Documents/pdf-library/Library/Shalizi{\_}2013{\_}Advanced Data Analysis from an Elementary Point of View.pdf:pdf},
pages = {1--583},
title = {{Advanced Data Analysis from an Elementary Point of View}},
year = {2013}
}
@article{Wanga,
archivePrefix = {arXiv},
arxivId = {arXiv:1701.03551v1},
author = {Wang, Keze and Zhang, Dongyu and Li, Ya and Zhang, Ruimao and Lin, Liang and Member, Senior},
eprint = {arXiv:1701.03551v1},
file = {:home/arnold/Documents/pdf-library/Library/Wang et al.{\_}Unknown{\_}Cost-Effective Active Learning for Deep Image Classification.pdf:pdf},
pages = {1--10},
title = {{Cost-Effective Active Learning for Deep Image Classification}}
}
@book{Ardour2012,
author = {Ardour},
file = {:home/arnold/Documents/pdf-library/Library/Ardour{\_}2012{\_}Ardour Development Manual.pdf:pdf},
title = {{Ardour Development Manual}},
year = {2012}
}
@inproceedings{Ziebart2008,
author = {Ziebart, B.D. and Maas, Andrew and Bagnell, J.A. and Dey, A.K.},
booktitle = {Proc. 23rd AAAI Conf. Artificial Intelligence},
file = {:home/arnold/Documents/pdf-library/Library/Ziebart et al.{\_}2008{\_}Maximum entropy inverse reinforcement learning.pdf:pdf},
keywords = {imitation learning,inverse reinforcement learning},
pages = {1438},
title = {{Maximum entropy inverse reinforcement learning}},
url = {http://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf},
volume = {1433},
year = {2008}
}
@article{Freund2004,
abstract = {Slides about Semidefinite programming taken from a MIT course.},
author = {Freund, Rm},
file = {:home/arnold/Documents/pdf-library/Library/Freund{\_}2004{\_}Introduction to Semidefinite Programming (SDP).pdf:pdf},
journal = {Massachusetts Institute of Technology},
pages = {1--54},
title = {{Introduction to Semidefinite Programming (SDP)}},
url = {http://60-199-198-66.static.tfn.net.tw/cocw/mit/NR/rdonlyres/Sloan-School-of-Management/15-084JSpring2004/30872F3D-A64E-4230-BA0A-E391EC2A3CDB/0/lec23{\_}semidef{\_}opt.pdf},
year = {2004}
}
@article{Henrion2013,
abstract = {Many problems of systems control theory boil down to solving polynomial equations, polynomial inequalities or polyomial differential equations. Recent advances in convex optimization and real algebraic geometry can be combined to generate approximate solutions in floating point arithmetic. In the first part of the course we describe semidefinite programming (SDP) as an extension of linear programming (LP) to the cone of positive semidefinite matrices. We investigate the geometry of spectrahedra, convex sets defined by linear matrix inequalities (LMIs) or affine sections of the SDP cone. We also introduce spectrahedral shadows, or lifted LMIs, obtained by projecting affine sections of the SDP cones. Then we review existing numerical algorithms for solving SDP problems. In the second part of the course we describe several recent applications of SDP. First, we explain how to solve polynomial optimization problems, where a real multivariate polynomial must be optimized over a (possibly nonconvex) basic semialgebraic set. Second, we extend these techniques to ordinary differential equations (ODEs) with polynomial dynamics, and the problem of trajectory optimization (analysis of stability or performance of solutions of ODEs). Third, we conclude this part with applications to optimal control (design of a trajectory optimal w.r.t. a given functional). For some of these decision and optimization problems, it is hoped that the numerical solutions computed by SDP can be refined a posteriori and certified rigorously with appropriate techniques.},
archivePrefix = {arXiv},
arxivId = {1309.3112},
author = {Henrion, Didier},
eprint = {1309.3112},
file = {:home/arnold/Documents/pdf-library/Library/Henrion{\_}2013{\_}Optimization on linear matrix inequalities for polynomial systems control.pdf:pdf},
journal = {arXiv.org},
keywords = {Convex relaxation,Polynomial optimization},
title = {{Optimization on linear matrix inequalities for polynomial systems control}},
volume = {math.OC},
year = {2013}
}
@article{Ratliff2006,
abstract = {Imitation learning of sequential, goaldirected behavior by standard supervised techniques is often difficult. We frame learning such behaviors as a maximum margin structured prediction problem over a space of policies. In this approach, we learn mappings from features to cost so an optimal policy in an MDP with these cost mimics the expert's behavior. Further, we demonstrate a simple, provably efficient approach to structured maximum margin learning, based on the subgradient method, that leverages existing fast algorithms for inference. Although the technique is general, it is particularly relevant in problems where A * and dynamic programming approaches make learning policies tractable in problems beyond the limitations of a QP formulation. We demonstrate our approach applied to route planning for outdoor mobile robots, where the behavior a designer wishes a planner to execute is often clear, while specifying cost functions that engender this behavior is a much more difficult task. 1.},
author = {Ratliff, Nathan D. and Bagnell, J. Andrew and Zinkevich, Martin a.},
doi = {10.1145/1143844.1143936},
file = {:home/arnold/Documents/pdf-library/Library/Ratliff, Bagnell, Zinkevich{\_}2006{\_}Maximum margin planning.pdf:pdf},
isbn = {1595933832},
issn = {17458358},
journal = {International conference on Machine learning - ICML '06},
number = {23},
pages = {729--736},
pmid = {17914344},
title = {{Maximum margin planning}},
url = {http://portal.acm.org/citation.cfm?doid=1143844.1143936},
year = {2006}
}
@article{Abbeel2005,
abstract = {Kalman filters are a workhorse of robotics and are routinely used in state-estimation problems. However, their performance critically depends on a large number of modeling parameters which can be very difficult to obtain, and are often set via significant manual tweaking and at a great cost of engineering time. In this paper, we propose a method for automatically learning the noise parameters of a Kalman filter. We also demonstrate on a commercial wheeled rover that our Kalman filter's learned noise covariance parameters—obtained quickly and fully automatically—significantly outperform an earlier, carefully and laboriously hand-designed one.},
author = {Abbeel, Pieter and Coates, Adam and Montemerlo, Michael and Ng, Andrew Y and Thrun, Sebastian},
doi = {10.1.1.76.759},
file = {:home/arnold/Documents/pdf-library/Library/Abbeel et al.{\_}2005{\_}Discriminative Training of Kalman Filters.pdf:pdf},
isbn = {9780262701143},
journal = {Proceedings of Robotics: Science and Systems I},
pages = {289--296},
title = {{Discriminative Training of Kalman Filters}},
year = {2005}
}
@article{Hazan2008,
abstract = {We propose an algorithm for approximately maximizing a concave function over the bounded semi-definite cone, which produces sparse solutions. Sparsity for SDP corresponds to low rank matrices, and is a important property for both computational as well as learning theoretic reasons. As an application, building on Aaronson's recent work, we derive a linear time algorithm for Quantum State Tomography.},
author = {Hazan, Elad},
doi = {10.1007/978-3-540-78773-0_27},
file = {:home/arnold/Documents/pdf-library/Library/Hazan{\_}2008{\_}Sparse approximate solutions to semidefinite programs.pdf:pdf},
isbn = {3540787720},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {306--316},
title = {{Sparse approximate solutions to semidefinite programs}},
volume = {4957 LNCS},
year = {2008}
}
@article{Osokin2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1605.09346v1},
author = {Osokin, Anton and Alayrac, Jean-baptiste and Lastname, First and Fr, Inria and Lukasewitz, Isabella and Dokania, Puneet K},
eprint = {arXiv:1605.09346v1},
file = {:home/arnold/Documents/pdf-library/Library/Osokin et al.{\_}2016{\_}Minding the Gaps for Block Frank-Wolfe Optimization of Structured SVMs.pdf:pdf},
title = {{Minding the Gaps for Block Frank-Wolfe Optimization of Structured SVMs}},
volume = {48},
year = {2016}
}
@article{Priess2015,
author = {Priess, M Cody and Conway, Richard and Choi, Jongeun and Popovich, John M and Radcliffe, Clark},
file = {:home/arnold/Documents/pdf-library/Library/Priess et al.{\_}2015{\_}Solutions to the Inverse LQR Problem With Application to Biological Systems Analysis.pdf:pdf},
journal = {IEEE Transactions on Control Systems Technology},
number = {2},
pages = {770--777},
title = {{Solutions to the Inverse LQR Problem With Application to Biological Systems Analysis}},
volume = {23},
year = {2015}
}
@article{Chen2015,
author = {Chen, Xiangli and Ziebart, Brian D},
file = {:home/arnold/Documents/pdf-library/Library/Chen, Ziebart{\_}2015{\_}Predictive Inverse Optimal Control for Linear-Quadratic-Gaussian Systems.pdf:pdf},
journal = {International Conference on Artificial Intelligence and Statistics},
title = {{Predictive Inverse Optimal Control for Linear-Quadratic-Gaussian Systems}},
year = {2015}
}
@article{Chen2016,
author = {Chen, Xiangli and Monofort, Mathew and Liu, Anqi and Ziebart, Brian D},
file = {:home/arnold/Documents/pdf-library/Library/Chen et al.{\_}2016{\_}Robust Covariate Shift Regression.pdf:pdf},
journal = {International Conference on Artificial Intelligence and Statistics},
pages = {1270--1279},
title = {{Robust Covariate Shift Regression}},
year = {2016}
}
@article{Defazio2014,
author = {Defazio, Aaron and Bach, Francis and Lacoste-julien, Simon},
file = {:home/arnold/Documents/pdf-library/Library/Defazio, Bach, Lacoste-julien{\_}2014{\_}SAGA A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives.pdf:pdf},
title = {{SAGA : A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives}},
year = {2014}
}
@article{Kitani2012,
author = {Kitani, Kris M and Ziebart, Brian D and Bagnell, J Andrew and Hebert, Martial},
file = {:home/arnold/Documents/pdf-library/Library/Kitani et al.{\_}2012{\_}Activity Forecasting.pdf:pdf},
journal = {European Conference on Computer Vision},
keywords = {activity forecasting,inverse optimal control},
pages = {1--14},
title = {{Activity Forecasting}},
year = {2012}
}
@article{Liu2014,
author = {Liu, Anqi and Ziebart, Brian D},
file = {:home/arnold/Documents/pdf-library/Library/Liu, Ziebart{\_}2014{\_}Robust Classification Under Sample Selection Bias.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
pages = {1--14},
title = {{Robust Classification Under Sample Selection Bias}},
year = {2014}
}
@article{Monfort2015,
author = {Monfort, Mathew and Liu, Anqi and Ziebart, Brian D},
file = {:home/arnold/Documents/pdf-library/Library/Monfort, Liu, Ziebart{\_}2015{\_}Intent Prediction and Trajectory Forecasting via Predictive Inverse Linear-Quadratic Regulation.pdf:pdf},
journal = {AAAI},
title = {{Intent Prediction and Trajectory Forecasting via Predictive Inverse Linear-Quadratic Regulation}},
year = {2015}
}
@incollection{Lacoste-Julien2017q,
author = {Lacoste-Julien, Simon},
booktitle = {Structured Prediction - IFT 6085 Notes},
file = {:home/arnold/Documents/pdf-library/Library/Lacoste-Julien{\_}2017{\_}Lecture 19 - CRF {\&} SAG.pdf:pdf},
pages = {1--5},
title = {{Lecture 19 - CRF {\&} SAG}},
year = {2017}
}
@article{Mishra2017,
abstract = {We introduce a method for learning the dynamics of complex nonlinear systems based on deep generative models over temporal segments of states and actions. Unlike dynamics models that operate over individual discrete timesteps, we learn the distribution over future state trajectories conditioned on past state, past action, and planned future action trajectories, as well as a latent prior over action trajectories. Our approach is based on convolutional autoregressive models and variational autoencoders. It makes stable and accurate predictions over long horizons for complex, stochastic systems, effectively expressing uncertainty and modeling the effects of collisions, sensory noise, and action delays. The learned dynamics model and action prior can be used for end-to-end, fully differentiable trajectory optimization and model-based policy optimization, which we use to evaluate the performance and sample-efficiency of our method.},
archivePrefix = {arXiv},
arxivId = {1703.04070},
author = {Mishra, Nikhil and Abbeel, Pieter and Mordatch, Igor},
eprint = {1703.04070},
file = {:home/arnold/Documents/pdf-library/Library/Mishra, Abbeel, Mordatch{\_}2017{\_}Prediction and Control with Temporal Segment Models.pdf:pdf},
title = {{Prediction and Control with Temporal Segment Models}},
url = {http://arxiv.org/abs/1703.04070},
year = {2017}
}
@article{Johnson2016,
abstract = {We propose a general modeling and inference framework that composes probabilistic graphical models with deep learning methods and combines their respective strengths. Our model family augments graphical structure in latent variables with neural network observation models. For inference, we extend variational autoencoders to use graphical model approximating distributions with recognition networks that output conjugate potentials. All components of these models are learned simultaneously with a single objective, giving a scalable algorithm that leverages stochastic variational inference, natural gradients, graphical model message passing, and the reparameterization trick. We illustrate this framework with several example models and an application to mouse behavioral phenotyping.},
archivePrefix = {arXiv},
arxivId = {1603.06277},
author = {Johnson, Matthew J. and Duvenaud, David and Wiltschko, Alexander B. and Datta, Sandeep R. and Adams, Ryan P.},
doi = {10.1126/science.1250298},
eprint = {1603.06277},
file = {:home/arnold/Documents/pdf-library/Library/Johnson et al.{\_}2016{\_}Composing Graphical Models with Neural Networks for Structured Representations and Fast Inference.pdf:pdf},
issn = {0036-8075},
journal = {Nips},
number = {6182},
pages = {386--392},
pmid = {24674869},
title = {{Composing Graphical Models with Neural Networks for Structured Representations and Fast Inference}},
url = {http://www.sciencemag.org/cgi/doi/10.1126/science.1250298{\%}5Cnhttp://arxiv.org/abs/1603.06277},
volume = {344},
year = {2016}
}
@article{Goel2014,
abstract = {Since the advent of deep learning, it has been used to solve various problems using many different architectures. The application of such deep architectures to auditory data is also not uncommon. However, these architectures do not always adequately consider the temporal dependencies in data. We thus propose a new generic architecture called the Deep Belief Network - Bidirectional Long Short-Term Memory (DBN-BLSTM) network that models sequences by keeping track of the temporal information while enabling deep representations in the data. We demonstrate this new architecture by applying it to the task of music generation and obtain state-of-the-art results.},
archivePrefix = {arXiv},
arxivId = {1412.6093},
author = {Goel, Kratarth and Vohra, Raunaq},
eprint = {1412.6093},
file = {:home/arnold/Documents/pdf-library/Library/Goel, Vohra{\_}2014{\_}Learning Temporal Dependencies in Data Using a DBN-BLSTM.pdf:pdf},
isbn = {9781467382731},
pages = {6},
title = {{Learning Temporal Dependencies in Data Using a DBN-BLSTM}},
url = {http://arxiv.org/abs/1412.6093},
year = {2014}
}
@article{Bayer2014,
abstract = {Leveraging advances in variational inference, we propose to enhance recurrent neural networks with latent variables, resulting in Stochastic Recurrent Networks (STORNs). The model i) can be trained with stochastic gradient methods, ii) allows structured and multi-modal conditionals at each time step, iii) features a reliable estimator of the marginal likelihood and iv) is a generalisation of deterministic recurrent neural networks. We evaluate the method on four polyphonic musical data sets and motion capture data.},
archivePrefix = {arXiv},
arxivId = {1411.7610},
author = {Bayer, Justin and Osendorfer, Christian},
eprint = {1411.7610},
file = {:home/arnold/Documents/pdf-library/Library/Bayer, Osendorfer{\_}2014{\_}Learning Stochastic Recurrent Networks.pdf:pdf},
pages = {1--9},
title = {{Learning Stochastic Recurrent Networks}},
url = {http://arxiv.org/abs/1411.7610},
year = {2014}
}
@article{Chung2015,
abstract = {In this paper, we explore the inclusion of random variables into the dynamic latent state of a recurrent neural network (RNN) by combining elements of the variational autoencoder. We argue that through the use of high-level latent random variables, our variational RNN (VRNN) is able to learn to model the kind of variability observed in highly-structured sequential data (such as speech). We empirically evaluate the proposed model against related sequential models on five sequence datasets, four of speech and one of handwriting. Our results show the importance of the role random variables can play in the RNN dynamic latent state.},
archivePrefix = {arXiv},
arxivId = {1506.02216},
author = {Chung, Junyoung and Kastner, Kyle and Dinh, Laurent and Goel, Kratarth and Courville, Aaron and Bengio, Yoshua},
eprint = {1506.02216},
file = {:home/arnold/Documents/pdf-library/Library/Chung et al.{\_}2015{\_}A Recurrent Latent Variable Model for Sequential Data.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems 28 (NIPS 2015)},
pages = {8},
title = {{A Recurrent Latent Variable Model for Sequential Data}},
url = {http://arxiv.org/abs/1506.02216},
year = {2015}
}
@article{Shelhamer2015,
abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20{\%} relative improvement to 62.2{\%} mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.},
archivePrefix = {arXiv},
arxivId = {1411.4038},
author = {Shelhamer, Evan and Long, Jonathan and Darrell, Trevor},
doi = {10.1109/CVPR.2015.7298965},
eprint = {1411.4038},
file = {:home/arnold/Documents/pdf-library/Library/Shelhamer, Long, Darrell{\_}2015{\_}Fully Convolutional Networks for Semantic Segmentation.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
pages = {3431--3440},
pmid = {16190471},
title = {{Fully Convolutional Networks for Semantic Segmentation}},
year = {2015}
}
@article{Holub,
author = {Holub, Alex and Perona, Pietro and Burl, Michael C},
file = {:home/arnold/Documents/pdf-library/Library/Holub, Perona, Burl{\_}Unknown{\_}Entropy-Based Active Learning for Object Recognition.pdf:pdf},
title = {{Entropy-Based Active Learning for Object Recognition}}
}
@article{Germain2016a,
abstract = {We exhibit a strong link between frequentist PAC-Bayesian bounds and the Bayesian marginal likelihood. That is, for the negative log-likelihood loss function, we show that the minimization of PAC-Bayesian generalization bounds maximizes the Bayesian marginal likelihood. This provides an alternative explanation to the Bayesian Occam's razor criteria, under the assumption that the data is generated by a i.i.d. distribution. Moreover, as the negative log-likelihood is an unbounded loss function, we motivate and propose a PAC-Bayesian theorem tailored for the sub-Gamma loss family, and we show that our approach is sound on classical Bayesian linear regression tasks.},
archivePrefix = {arXiv},
arxivId = {1605.08636},
author = {Germain, Pascal and Bach, Francis and Lacoste, Alexandre and Lacoste-Julien, Simon},
eprint = {1605.08636},
file = {:home/arnold/Documents/pdf-library/Library/Germain et al.{\_}2016{\_}PAC-Bayesian Theory Meets Bayesian Inference.pdf:pdf},
journal = {arXiv stat.ML},
number = {Nips},
pages = {1--12},
title = {{PAC-Bayesian Theory Meets Bayesian Inference}},
url = {http://arxiv.org/abs/1605.08636},
year = {2016}
}
@incollection{Lacoste-Julien2017r,
author = {Lacoste-Julien, Simon},
booktitle = {Structured Prediction - IFT 6085 Notes},
file = {:home/arnold/Documents/pdf-library/Library/Lacoste-Julien{\_}2017{\_}Lecture 18 - Consistency.pdf:pdf},
title = {{Lecture 18 - Consistency}},
year = {2017}
}
@article{Osokin2017a,
archivePrefix = {arXiv},
arxivId = {arXiv:1703.02403v1},
author = {Osokin, Anton and Bach, Francis and Lacoste-julien, Simon},
eprint = {arXiv:1703.02403v1},
file = {:home/arnold/Documents/pdf-library/Library/Osokin, Bach, Lacoste-julien{\_}2017{\_}On Structured Prediction Theory with Calibrated Convex Surrogate Losses.pdf:pdf},
title = {{On Structured Prediction Theory with Calibrated Convex Surrogate Losses}},
year = {2017}
}
@article{He2017,
archivePrefix = {arXiv},
arxivId = {1703.06870},
author = {He, Kaiming and Girshick, Ross},
eprint = {1703.06870},
file = {:home/arnold/Documents/pdf-library/Library/He, Girshick{\_}2017{\_}Mask R-CNN.pdf:pdf},
title = {{Mask R-CNN}},
year = {2017}
}
@article{Thompson1933,
author = {Thompson, William R.},
file = {:home/arnold/Documents/pdf-library/Library/Thompson{\_}1933{\_}On the Likeligood that One Unkown Probability Exceeds Another in View of the Evidence of Two Samples.pdf:pdf},
journal = {Biometrika},
title = {{On the Likeligood that One Unkown Probability Exceeds Another in View of the Evidence of Two Samples}},
year = {1933}
}
@article{Houlsby2011,
abstract = {Information theoretic active learning has been widely studied for prob- abilistic models. For simple regression an optimal myopic policy is easily tractable. However, for other tasks and with more complex models, such as classification with nonparametric models, the optimal solution is harder to compute. Current approaches make approximations to achieve tractabil- ity. We propose an approach that expresses information gain in terms of predictive entropies, and apply this method to the Gaussian Process Classifier (GPC). Our approach makes minimal approximations to the full information theoretic objective. Our experimental performance compares favourably to many popular active learning algorithms, and has equal or lower computational complexity. We compare well to decision theoretic approaches also, which are privy to more information and require much more computational time. Secondly, by developing further a reformulation of binary preference learning to a classification problem, we extend our algorithm to Gaussian Process preference learning. 1},
archivePrefix = {arXiv},
arxivId = {1112.5745v1},
author = {Houlsby, Neil and Huszar, Ferenc and Ghahramani, Zoubin and Lengyel, Mate},
eprint = {1112.5745v1},
file = {:home/arnold/Documents/pdf-library/Library/Houlsby et al.{\_}2011{\_}Bayesian Active Learning for Classification and Preference Learning.pdf:pdf},
pages = {45--64},
title = {{Bayesian Active Learning for Classification and Preference Learning}},
year = {2011}
}
@article{Huber2013,
abstract = {For large data sets, performing Gaussian process regression is computationally demanding or even intractable. If data can be processed sequentially, the recursive regression method proposed in this paper allows incorporating new data with constant computation time. For this purpose two operations are performed alternating on a fixed set of so-called basis vectors used for estimating the latent function: First, inference of the latent function at the new inputs. Second, utilization of the new data for updating the estimate. Numerical simulations show that the proposed approach significantly reduces the computation time and at the same time provides more accurate estimates compared to existing on-line and/or sparse Gaussian process regression approaches.},
author = {Huber, Marco F.},
doi = {10.1109/ICASSP.2013.6638281},
file = {:home/arnold/Documents/pdf-library/Library/Huber{\_}2013{\_}Recursive Gaussian process regression.pdf:pdf},
isbn = {978-1-4799-0356-6},
issn = {1520-6149},
journal = {2013 IEEE International Conference on Acoustics, Speech and Signal Processing},
keywords = {Bayes methods,Bayesian filtering,Gaussian processes,Joints,Kalman filters,Kernel,Runtime,Training,Vectors,basis vectors,constant computation time,data utilization,filtering theory,inference,large data sets,latent function estimation,numerical simulations,on-line regression,recursive Gaussian process regression,recursive processing,regression analysis,smoothing},
pages = {3362--3366},
title = {{Recursive Gaussian process regression}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6638281},
year = {2013}
}
@article{Alain2015,
abstract = {Humans are able to accelerate their learning by selecting training materials that are the most informative and at the appropriate level of difficulty. We propose a framework for distributing deep learning in which one set of workers search for the most informative examples in parallel while a single worker updates the model on examples selected by importance sampling. This leads the model to update using an unbiased estimate of the gradient which also has minimum variance when the sampling proposal is proportional to the L2-norm of the gradient. We show experimentally that this method reduces gradient variance even in a context where the cost of synchronization across machines cannot be ignored, and where the factors for importance sampling are not updated instantly across the training set.},
archivePrefix = {arXiv},
arxivId = {1511.06481},
author = {Alain, Guillaume and Lamb, Alex and Sankar, Chinnadhurai and Courville, Aaron and Bengio, Yoshua},
eprint = {1511.06481},
file = {:home/arnold/Documents/pdf-library/Library/Alain et al.{\_}2015{\_}Variance Reduction in SGD by Distributed Importance Sampling.pdf:pdf},
pages = {1--17},
title = {{Variance Reduction in SGD by Distributed Importance Sampling}},
url = {http://arxiv.org/abs/1511.06481},
year = {2015}
}
@article{Osband2013,
abstract = {Most provably-efficient learning algorithms introduce optimism about poorly-understood states and actions to encourage exploration. We study an alternative approach for efficient exploration, $\backslash$emph{\{}posterior sampling for reinforcement learning{\}} (PSRL). This algorithm proceeds in repeated episodes of known duration. At the start of each episode, PSRL updates a prior distribution over Markov decision processes and takes one sample from this posterior. PSRL then follows the policy that is optimal for this sample during the episode. The algorithm is conceptually simple, computationally efficient and allows an agent to encode prior knowledge in a natural way. We establish an {\$}\backslashtilde{\{}O{\}}(\backslashtau S \backslashsqrt{\{}AT{\}}){\$} bound on the expected regret, where {\$}T{\$} is time, {\$}\backslashtau{\$} is the episode length and {\$}S{\$} and {\$}A{\$} are the cardinalities of the state and action spaces. This bound is one of the first for an algorithm not based on optimism, and close to the state of the art for any reinforcement learning algorithm. We show through simulation that PSRL significantly outperforms existing algorithms with similar regret bounds.},
archivePrefix = {arXiv},
arxivId = {1306.0940},
author = {Osband, Ian and Russo, Daniel and {Van Roy}, Benjamin},
eprint = {1306.0940},
file = {:home/arnold/Documents/pdf-library/Library/Osband, Russo, Van Roy{\_}2013{\_}(More) Efficient Reinforcement Learning via Posterior Sampling.pdf:pdf},
journal = {arXiv},
pages = {10},
title = {{(More) Efficient Reinforcement Learning via Posterior Sampling}},
url = {http://arxiv.org/abs/1306.0940},
volume = {1306:0940},
year = {2013}
}
@incollection{Lacoste-Julien2017s,
author = {Lacoste-Julien, Simon},
booktitle = {Structured Prediction - IFT 6085 Notes},
file = {:home/arnold/Documents/pdf-library/Library/Lacoste-Julien{\_}2017{\_}Lecture 17 - Surrogate Loss Functions.pdf:pdf},
title = {{Lecture 17 - Surrogate Loss Functions}},
year = {2017}
}
@phdthesis{Gal2016,
abstract = {Deep learning has attracted tremendous attention from researchers in various fields of information engineering such as AI, computer vision, and language processing [Kalchbrenner and Blunsom, 2013; Krizhevsky et al., 2012; Mnih et al., 2013], but also from more traditional sciences such as physics, biology, and manufacturing [Anjos et al., 2015; Baldi et al., 2014; Bergmann et al., 2014]. Neural networks, image processing tools such as convolutional neural networks, sequence processing models such as recurrent neural networks, and regularisation tools such as dropout, are used extensively. However, fields such as physics, biology, and manufacturing are ones in which representing model uncertainty is of crucial importance [Ghahramani, 2015; Krzywinski and Altman, 2013]. With the recent shift in many of these fields towards the use of Bayesian uncertainty [Herzog and Ostwald, 2013; Nuzzo, 2014; Trafimow and Marks, 2015], new needs arise from deep learning. In this work we develop tools to obtain practical uncertainty estimates in deep learning, casting recent deep learning tools as Bayesian models without changing either the models or the optimisation. In the first part of this thesis we develop the theory for such tools, providing applications and illustrative examples. We tie approximate inference in Bayesian models to dropout and other stochastic regularisation techniques, and assess the approximations empirically. We give example applications arising from this connection between modern deep learning and Bayesian modelling such as active learning of image data and data efficient deep reinforcement learning. We further demonstrate the method's practicality through a survey of recent applications making use of the suggested tools in language applications, medical diagnostics, bioinformatics, image processing, and autonomous driving. In the second part of the thesis we explore its theoretical implications, and the insights stemming from the link between Bayesian modelling and deep learning. We discuss what determines model uncertainty properties, analyse the approximate inference analytically in the linear case, and theoretically examine various priors such as spike and slab priors.},
author = {Gal, Yarin},
file = {:home/arnold/Documents/pdf-library/Library/Gal{\_}2016{\_}Uncertainty in Deep Learning.pdf:pdf},
number = {October},
title = {{Uncertainty in Deep Learning}},
year = {2016}
}
@incollection{Lacoste-Julien2017t,
author = {Lacoste-Julien, Simon},
booktitle = {Structured Prediction - IFT 6085 Notes},
file = {:home/arnold/Documents/pdf-library/Library/Lacoste-Julien{\_}2017{\_}Lecture 16 - PAC-Bayes.pdf:pdf},
pages = {1--4},
title = {{Lecture 16 - PAC-Bayes}},
year = {2017}
}
@article{Hollinger2009,
abstract = {We propose three sampling-based motion planning algorithms for generating informative mobile robot trajectories. The goal is to find a trajectory that maximizes an information quality metric (e.g. variance reduction, information gain, or mutual information) and also falls within a pre-specified budget constraint (e.g. fuel, energy, or time). Prior algorithms have employed combinatorial optimization techniques to solve these problems, but existing techniques are typically restricted to discrete domains and often scale poorly in the size of the problem. Our proposed rapidly exploring information gathering (RIG) algorithms combine ideas from sampling-based motion planning with branch and bound techniques to achieve efficient information gathering in continuous space with motion constraints. We provide analysis of the asymptotic optimality of our algorithms, and we present several conservative pruning strategies for modular, submodular, and time-varying information objectives. We demonstrate that our proposed techniques find optimal solutions more quickly than existing combinatorial solvers, and we provide a proof-of-concept field implementation on an autonomous surface vehicle performing a wireless signal strength monitoring task in a lake.},
author = {Hollinger, Geoffrey A and Sukhatme, Gaurav S.},
doi = {10.1177/0278364914533443},
file = {:home/arnold/Documents/pdf-library/Library/Hollinger, Sukhatme{\_}2009{\_}Sampling-based Motion Planning for Robotic Information Gathering.pdf:pdf},
issn = {0278-3649},
journal = {Robotics: Science and Systems},
number = {9},
pages = {1271--1287},
title = {{Sampling-based Motion Planning for Robotic Information Gathering}},
volume = {33},
year = {2009}
}
@incollection{Lacoste-Julien2017u,
author = {Lacoste-Julien, Simon},
booktitle = {Structured Prediction - IFT 6085 Notes},
file = {:home/arnold/Documents/pdf-library/Library/Lacoste-Julien{\_}2017{\_}Lecture 15 - Decision Theory.pdf:pdf},
pages = {1--4},
title = {{Lecture 15 - Decision Theory}},
year = {2017}
}
@article{McAllester2007b,
author = {McAllester, David},
number = {ln 2},
pages = {1--9},
title = {{Proof of the Occam Bound}},
volume = {01},
year = {2007}
}
@article{Lehmann2017,
author = {Lehmann, Chris},
file = {:home/arnold/Documents/pdf-library/Library/Lehmann{\_}2017{\_}The Snare of Preparation:},
journal = {The Baffler},
number = {34},
title = {{The Snare of Preparation}},
year = {2017}
}
@article{Yu2014,
abstract = {We propose a novel non-linear extension to the Orienteering Problem (OP), called the Correlated Orienteering Problem (COP). We use COP to plan informative tours (cyclic paths) for persistent monitoring of an environment with spatial correlations, where the tours are constrained to a fixed length or time budget. The main feature of COP is a quadratic utility function that captures spatial correlations among points of interest that are close to each other. COP may be solved using mixed integer quadratic programming (MIQP) that can plan multiple disjoint tours that maximize the quadratic utility function. We perform extensive characterization of our method to verify its correctness, as well as its applicability to the estimation of a realistic, time-varying, and spatially correlated scalar field.},
archivePrefix = {arXiv},
arxivId = {1402.1896},
author = {Yu, Jingjin and Schwager, Mac and Rus, Daniela},
doi = {10.1109/IROS.2014.6942582},
eprint = {1402.1896},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Yu, Schwager, Rus{\_}2014{\_}Correlated Orienteering Problem and its application to informative path planning for persistent monitoring tasks.pdf:pdf},
isbn = {9781479969340},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
number = {Iros},
pages = {342--349},
title = {{Correlated Orienteering Problem and its application to informative path planning for persistent monitoring tasks}},
year = {2014}
}
@article{Yu2014a,
abstract = {We propose a novel non-linear extension to the Orienteering Problem (OP), called the Correlated Orienteering Problem (COP). We use COP to plan informative tours (cyclic paths) for persistent monitoring of an environment with spatial correlations, where the tours are constrained to a fixed length or time budget. The main feature of COP is a quadratic utility function that captures spatial correlations among points of interest that are close to each other. COP may be solved using mixed integer quadratic programming (MIQP) that can plan multiple disjoint tours that maximize the quadratic utility function. We perform extensive characterization of our method to verify its correctness, as well as its applicability to the estimation of a realistic, time-varying, and spatially correlated scalar field.},
archivePrefix = {arXiv},
arxivId = {1402.1896},
author = {Yu, Jingjin and Schwager, Mac and Rus, Daniela},
doi = {10.1109/IROS.2014.6942582},
eprint = {1402.1896},
file = {:home/arnold/Documents/pdf-library/Library/Yu, Schwager, Rus{\_}2014{\_}Correlated Orienteering Problem and its application to informative path planning for persistent monitoring tasks.pdf:pdf},
isbn = {9781479969340},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
number = {Iros},
pages = {342--349},
title = {{Correlated Orienteering Problem and its application to informative path planning for persistent monitoring tasks}},
year = {2014}
}
@article{Opper2009,
abstract = {The variational approximation of posterior distributions by multivariate gaussians has been much less popular in the machine learning community compared to the corresponding approximation by factorizing distributions. This is for a good reason: the gaussian approximation is in general plagued by an Omicron(N)(2) number of variational parameters to be optimized, N being the number of random variables. In this letter, we discuss the relationship between the Laplace and the variational approximation, and we show that for models with gaussian priors and factorizing likelihoods, the number of variational parameters is actually Omicron(N). The approach is applied to gaussian process regression with nongaussian likelihoods.},
author = {Opper, Manfred and Archambeau, C{\'{e}}dric},
doi = {10.1162/neco.2008.08-07-592},
file = {:home/arnold/Documents/pdf-library/Library/Opper, Archambeau{\_}2009{\_}The variational gaussian approximation revisited.pdf:pdf},
issn = {0899-7667},
journal = {Neural computation},
number = {3},
pages = {786--792},
pmid = {18785854},
title = {{The variational gaussian approximation revisited.}},
volume = {21},
year = {2009}
}
@article{Zhu2015,
abstract = {In this paper, we propose an approach that exploits ob-ject segmentation in order to improve the accuracy of object detection. We frame the problem as inference in a Markov Random Field, in which each detection hypothesis scores object appearance as well as contextual information using Convolutional Neural Networks, and allows the hypothesis to choose and score a segment out of a large pool of ac-curate object segmentation proposals. This enables the de-tector to incorporate additional evidence when it is avail-able and thus results in more accurate detections. Our ex-periments show an improvement of 4.1{\%} in mAP over the R-CNN baseline on PASCAL VOC 2010, and 3.4{\%} over the current state-of-the-art, demonstrating the power of our ap-proach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1502.04275v1},
author = {Zhu, Yukun and Urtasun, Raquel and Salakhutdinov, Ruslan and Fidler, Sanja},
doi = {10.1109/CVPR.2015.7299102},
eprint = {arXiv:1502.04275v1},
file = {:home/arnold/Documents/pdf-library/Library/Zhu et al.{\_}2015{\_}SegDeepM Exploiting segmentation and context in deep neural networks for object detection.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {4703--4711},
title = {{SegDeepM: Exploiting segmentation and context in deep neural networks for object detection}},
volume = {07-12-June},
year = {2015}
}
@article{Brooks2016,
abstract = {A novel topological segmentation of retinal images represents blood vessels as connected regions in the continuous image plane, having shape-related analytic and geometric properties. This paper presents topological segmentation results from the DRIVE retinal image database.},
archivePrefix = {arXiv},
arxivId = {1608.01339},
author = {Brooks, Martin},
eprint = {1608.01339},
file = {:home/arnold/Documents/pdf-library/Library/Brooks{\_}2016{\_}Retinal Vessel Segmentation Using A New Topological Method.pdf:pdf},
title = {{Retinal Vessel Segmentation Using A New Topological Method}},
url = {http://arxiv.org/abs/1608.01339},
year = {2016}
}
@article{Betancourt2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1701.02434v1},
author = {Betancourt, Michael},
eprint = {arXiv:1701.02434v1},
file = {:home/arnold/Documents/pdf-library/Library/Betancourt{\_}2017{\_}A Conceptual Introduction to Hamiltonian Monte Carlo.pdf:pdf},
title = {{A Conceptual Introduction to Hamiltonian Monte Carlo}},
year = {2017}
}
@article{Wangb,
archivePrefix = {arXiv},
arxivId = {arXiv:1604.01662v2},
author = {Wang, Hao and Yeung, Dit-yan},
eprint = {arXiv:1604.01662v2},
file = {:home/arnold/Documents/pdf-library/Library/Wang, Yeung{\_}Unknown{\_}Towards Bayesian Deep Learning A Survey.pdf:pdf},
pages = {1--17},
title = {{Towards Bayesian Deep Learning : A Survey}}
}
@article{Bengio2009,
abstract = {Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illus- trates gradually more concepts, and gradu- ally more complex ones. Here, we formal- ize such training strategies in the context of machine learning, and call them “curricu- lum learning”. In the context of recent re- search studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neu- ral networks), we explore curriculum learn- ing in various set-ups. The experiments show that significant improvements in generaliza- tion can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bengio, Yoshua and Louradour, J{\'{e}}r{\^{o}}me and Collobert, Ronan and Weston, Jason},
doi = {10.1145/1553374.1553380},
eprint = {arXiv:1011.1669v3},
file = {:home/arnold/Documents/pdf-library/Library/Bengio et al.{\_}2009{\_}Curriculum learning.pdf:pdf},
isbn = {9781605585161},
issn = {0022-5193},
journal = {Proceedings of the 26th annual international conference on machine learning},
pages = {41--48},
pmid = {5414602},
title = {{Curriculum learning}},
url = {http://portal.acm.org/citation.cfm?doid=1553374.1553380},
year = {2009}
}
@article{Mnih2013,
abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
archivePrefix = {arXiv},
arxivId = {1312.5602},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
doi = {10.1038/nature14236},
eprint = {1312.5602},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Mnih et al.{\_}2013{\_}Playing Atari with Deep Reinforcement Learning.pdf:pdf},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {0028-0836},
journal = {arXiv preprint arXiv: {\ldots}},
pages = {1--9},
pmid = {25719670},
title = {{Playing Atari with Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1312.5602},
year = {2013}
}
@article{Silver2016,
abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks' to evaluate board positions and ‘policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8{\%} winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
doi = {10.1038/nature16961},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Silver et al.{\_}2016{\_}Mastering the game of Go with deep neural networks and tree search.pdf:pdf},
isbn = {1476-4687 (Electronic)$\backslash$r0028-0836 (Linking)},
issn = {0028-0836},
journal = {Nature},
number = {7587},
pages = {484--489},
pmid = {26819042},
publisher = {Nature Publishing Group},
title = {{Mastering the game of Go with deep neural networks and tree search}},
url = {http://dx.doi.org/10.1038/nature16961},
volume = {529},
year = {2016}
}
@incollection{Lacoste-Julien2017v,
author = {Lacoste-Julien, Simon},
booktitle = {Structured Prediction - IFT 6085 Notes},
pages = {1--6},
title = {{Lecture 14 - BCFW and Intro to Decision Theory}},
year = {2017}
}
@book{Devroye2014,
abstract = {Mycotoxins are small (MW approximately 700), toxic chemical products formed as secondary metabolites by a few fungal species that readily colonise crops and contaminate them with toxins in the field or after harvest. Ochratoxins and Aflatoxins are mycotoxins of major significance and hence there has been significant research on broad range of analytical and detection techniques that could be useful and practical. Due to the variety of structures of these toxins, it is impossible to use one standard technique for analysis and/or detection. Practical requirements for high-sensitivity analysis and the need for a specialist laboratory setting create challenges for routine analysis. Several existing analytical techniques, which offer flexible and broad-based methods of analysis and in some cases detection, have been discussed in this manuscript. There are a number of methods used, of which many are lab-based, but to our knowledge there seems to be no single technique that stands out above the rest, although analytical liquid chromatography, commonly linked with mass spectroscopy is likely to be popular. This review manuscript discusses (a) sample pre-treatment methods such as liquid-liquid extraction (LLE), supercritical fluid extraction (SFE), solid phase extraction (SPE), (b) separation methods such as (TLC), high performance liquid chromatography (HPLC), gas chromatography (GC), and capillary electrophoresis (CE) and (c) others such as ELISA. Further currents trends, advantages and disadvantages and future prospects of these methods have been discussed.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Devroye, Luc and Gyorfi, Laszlo and Lugosi, Gabor},
booktitle = {Igarss 2014},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {arXiv:1011.1669v3},
file = {:home/arnold/Documents/pdf-library/Library/Devroye, Gyorfi, Lugosi{\_}2014{\_}A Probabilistic Theory of Pattern Recognition.pdf:pdf},
isbn = {9780874216561},
issn = {0717-6163},
keywords = {Bott},
number = {1},
pages = {1--5},
pmid = {15003161},
title = {{A Probabilistic Theory of Pattern Recognition}},
url = {http://ebooks.cambridge.org/ref/id/CBO9781107415324A009{\%}5Cnhttp://link.springer.com/10.1007/978-1-4612-0711-5},
volume = {31},
year = {2014}
}
@article{DeStefani2016,
abstract = {We present TRI$\backslash$`EST, a suite of one-pass streaming algorithms to compute unbiased, low-variance, high-quality approximations of the global and local (i.e., incident to each vertex) number of triangles in a fully-dynamic graph represented as an adversarial stream of edge insertions and deletions. Our algorithms use reservoir sampling and its variants to exploit the user-specified memory space at all times. This is in contrast with previous approaches which use hard-to-choose parameters (e.g., a fixed sampling probability) and offer no guarantees on the amount of memory they will use. We show a full analysis of the variance of the estimations and novel concentration bounds for these quantities. Our experimental results on very large graphs show that TRI$\backslash$`EST outperforms state-of-the-art approaches in accuracy and exhibits a small update time.},
archivePrefix = {arXiv},
arxivId = {1602.07424},
author = {{De Stefani}, Lorenzo and Epasto, Alessandro and Riondato, Matteo and Upfal, Eli},
doi = {10.1145/2939672.2939771},
eprint = {1602.07424},
isbn = {9781450342322},
journal = {Acm Sigcdd},
number = {X},
pages = {1--40},
title = {{TRIEST: Counting Local and Global Triangles in Fully-dynamic Streams with Fixed Memory Size}},
url = {http://arxiv.org/abs/1602.07424},
volume = {XX},
year = {2016}
}
@incollection{Lacoste-Julien2017w,
author = {Lacoste-Julien, Simon},
booktitle = {Structured Prediction - IFT 6085 Notes},
pages = {1--7},
title = {{Lecture 13 - Polytopes}},
year = {2017}
}
@article{Team2015,
author = {Team, Stan Development},
title = {{Stan Modeling Language User ' s Guide and Reference Manual (v. 2.6.2)}},
year = {2015}
}
@article{Salimans2014,
author = {Salimans, Tim and Kingma, Diederik P and Welling, Max},
journal = {arXiv.org},
title = {{Markov Chain Monte Carlo and Variational Inference: Bridging the Gap}},
year = {2014}
}
@incollection{Lacoste-Julien2017x,
author = {Lacoste-Julien, Simon},
booktitle = {Structured Prediction - IFT 6085 Notes},
title = {{Lecture 11 - Frank-Wolfe Convergence}},
year = {2017}
}
@incollection{Lacoste-Julien2017y,
author = {Lacoste-Julien, Simon},
booktitle = {Structured Prediction - IFT 6085 Notes},
pages = {1--4},
title = {{Lecture 12 - Frank-Wolfe for SVMstruct}},
year = {2017}
}
@book{Sutton2017,
archivePrefix = {arXiv},
arxivId = {1603.02199},
author = {Sutton, R S and Barto, A G},
booktitle = {Neural Networks IEEE Transactions on},
doi = {10.1109/TNN.1998.712192},
eprint = {1603.02199},
file = {:home/arnold/Documents/pdf-library/Library/Sutton, Barto{\_}2017{\_}Reinforcement learning an introduction (DRAFT).pdf:pdf},
isbn = {9780262193986},
issn = {1045-9227},
keywords = {Artificial intelligence,Cybernetics,Signal process},
pages = {1054},
pmid = {18255791},
title = {{Reinforcement learning: an introduction (Draft)}},
year = {2017}
}
@incollection{Lacoste-Julien2017z,
author = {Lacoste-Julien, Simon},
booktitle = {Structured Prediction - IFT 6085 Notes},
title = {{Frank-Wolfe}},
year = {2017}
}
@book{Sutton2017a,
archivePrefix = {arXiv},
arxivId = {1603.02199},
author = {Sutton, R S and Barto, A G},
booktitle = {Neural Networks IEEE Transactions on},
doi = {10.1109/TNN.1998.712192},
eprint = {1603.02199},
file = {:home/arnold/Documents/pdf-library/Library/Sutton, Barto{\_}2017{\_}Reinforcement learning an introduction (DRAFT).pdf:pdf},
isbn = {9780262193986},
issn = {1045-9227},
keywords = {Artificial intelligence,Cybernetics,Signal process},
pmid = {18255791},
title = {{Reinforcement learning: an introduction (DRAFT)}},
year = {2017}
}
@article{Azizzadenesheli2016,
abstract = {We propose a new reinforcement learning algorithm for partially observable Markov decision processes (POMDP) based on spectral decomposition methods. While spectral methods have been previously employed for consistent learning of (passive) latent variable models such as hidden Markov models, POMDPs are more challenging since the learner interacts with the environment and possibly changes the future observations in the process. We devise a learning algorithm running through episodes, in each episode we employ spectral techniques to learn the POMDP parameters from a trajectory generated by a fixed policy. At the end of the episode, an optimization oracle returns the optimal memoryless planning policy which maximizes the expected reward based on the estimated POMDP model. We prove an order-optimal regret bound w.r.t. the optimal memoryless policy and efficient scaling with respect to the dimensionality of observation and action spaces.},
archivePrefix = {arXiv},
arxivId = {1602.07764},
author = {Azizzadenesheli, Kamyar and Lazaric, Alessandro and Anandkumar, Animashree},
eprint = {1602.07764},
keywords = {cess,latent variable model,method of moments,partially observable markov decision,pro-,spectral methods,upper confidence reinforcement learning},
pages = {1--62},
title = {{Reinforcement Learning of POMDP's using Spectral Methods}},
url = {http://arxiv.org/abs/1602.07764},
volume = {49},
year = {2016}
}
@article{Janzamin2016,
abstract = {Training neural networks is a challenging non-convex optimization problem, and backpropagation or gradient descent can get stuck in spurious local optima. We propose a novel algorithm based on tensor decomposition for training a two-layer neural network. We prove efficient risk bounds for our proposed method, with a polynomial sample complexity in the relevant parameters, such as input dimension and number of neurons. While learning arbitrary target functions is NP-hard, we provide transparent conditions on the function and the input for generalizability. Our training method is based on tensor decomposition, which provably converges to the global optimum, under a set of mild non-degeneracy conditions. It consists of simple embarrassingly parallel linear and multi-linear operations, and is competitive with standard stochastic gradient descent (SGD), in terms of computational complexity. Thus, we have a computationally efficient method with guaranteed risk bounds for training neural networks with general non-linear activations.},
archivePrefix = {arXiv},
arxivId = {1506.08473},
author = {Janzamin, M. and Sedghi, H. and Anandkumar, A.},
eprint = {1506.08473},
journal = {arXiv},
keywords = {method-of-moments,neural networks,risk bound,tensor decomposition},
pages = {31},
title = {{Beating the Perils of Non-Convexity: Guaranteed Training of Neural Networks using Tensor Methods}},
url = {http://arxiv.org/abs/1506.08473},
year = {2016}
}
@article{Zhang2017,
abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.},
archivePrefix = {arXiv},
arxivId = {1611.03530},
author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Mortiz and Recht, Benjamin and Vinyals, Oriol},
eprint = {1611.03530},
journal = {ICLR},
pages = {1--14},
title = {{Understanding Deep Learning Requires Rethinking Generalization}},
url = {http://arxiv.org/abs/1611.03530},
year = {2017}
}
@incollection{Lacoste-Julien2017ba,
author = {Lacoste-Julien, Simon},
booktitle = {Structured Prediction - IFT 6085 Notes},
pages = {1--4},
title = {{Lecture 9 - Structured SVM Dual (2)}},
year = {2017}
}
@book{Boyd2010a,
abstract = {We are developing a dual panel breast-dedicated PET system using LSO scintillators coupled to position sensitive avalanche photodiodes (PSAPD). The charge output is amplified and read using NOVA RENA-3 ASICs. This paper shows that the coincidence timing resolution of the RENA-3 ASIC can be improved using certain list-mode calibrations. We treat the calibration problem as a convex optimization problem and use the RENA-3s analog-based timing system to correct the measured data for time dispersion effects from correlated noise, PSAPD signal delays and varying signal amplitudes. The direct solution to the optimization problem involves a matrix inversion that grows order (n3) with the number of parameters. An iterative method using single-coordinate descent to approximate the inversion grows order (n). The inversion does not need to run to convergence, since any gains at high iteration number will be low compared to noise amplification. The system calibration method is demonstrated with measured pulser data as well as with two LSO-PSAPD detectors in electronic coincidence. After applying the algorithm, the 511keV photopeak paired coincidence time resolution from the LSO-PSAPD detectors under study improved by 57{\%}, from the raw value of 16.30.07 ns FWHM to 6.920.02 ns FWHM (11.520.05 ns to 4.890.02 ns for unpaired photons).},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Boyd, Stephen and Vandenberghe, Lieven},
booktitle = {Optimization Methods and Software},
doi = {10.1080/10556781003625177},
eprint = {1111.6189v1},
file = {:home/arnold/Documents/pdf-library/Library/Boyd, Vandenberghe{\_}2010{\_}Convex Optimization.pdf:pdf},
isbn = {9780521833783},
issn = {10556788},
number = {3},
pages = {487--487},
pmid = {20876008},
title = {{Convex Optimization}},
url = {https://web.stanford.edu/{~}boyd/cvxbook/bv{\_}cvxbook.pdf},
volume = {25},
year = {2010}
}
@phdthesis{Ulf2008,
author = {Ulf, Brefeld},
booktitle = {Learning},
file = {:home/arnold/Documents/pdf-library/Library/Ulf{\_}2008{\_}Semi-supervised Structured Prediction Models.pdf:pdf},
keywords = {Halb{\"{u}}berwachtes L,Lernen mit strukturierten Daten},
pages = {182},
school = {Humboldt-Universit{\"{a}}t zu Berlin},
title = {{Semi-supervised Structured Prediction Models}},
year = {2008}
}
@incollection{Lacoste-Julien2017bb,
author = {Lacoste-Julien, Simon},
booktitle = {Structured Prediction - IFT 6085 Notes},
file = {:home/arnold/Documents/pdf-library/Library/Lacoste-Julien{\_}2017{\_}Lecture 7 - Scribbles.pdf:pdf},
pages = {1--5},
title = {{Lecture 7 - Scribbles}},
year = {2017}
}
@misc{Coff,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Coff, Christian},
booktitle = {History},
doi = {10.1016/j.aqpro.2013.07.003},
eprint = {arXiv:1011.1669v3},
file = {:home/arnold/Documents/pdf-library/Library/Coff{\_}Unknown{\_}Incorporation, Digestion and Incarnation.pdf:pdf},
isbn = {0030-8870},
issn = {00308870},
pages = {1--9},
pmid = {24439530},
title = {{Incorporation, Digestion and Incarnation}}
}
@book{Boyd2010b,
abstract = {We are developing a dual panel breast-dedicated PET system using LSO scintillators coupled to position sensitive avalanche photodiodes (PSAPD). The charge output is amplified and read using NOVA RENA-3 ASICs. This paper shows that the coincidence timing resolution of the RENA-3 ASIC can be improved using certain list-mode calibrations. We treat the calibration problem as a convex optimization problem and use the RENA-3s analog-based timing system to correct the measured data for time dispersion effects from correlated noise, PSAPD signal delays and varying signal amplitudes. The direct solution to the optimization problem involves a matrix inversion that grows order (n3) with the number of parameters. An iterative method using single-coordinate descent to approximate the inversion grows order (n). The inversion does not need to run to convergence, since any gains at high iteration number will be low compared to noise amplification. The system calibration method is demonstrated with measured pulser data as well as with two LSO-PSAPD detectors in electronic coincidence. After applying the algorithm, the 511keV photopeak paired coincidence time resolution from the LSO-PSAPD detectors under study improved by 57{\%}, from the raw value of 16.30.07 ns FWHM to 6.920.02 ns FWHM (11.520.05 ns to 4.890.02 ns for unpaired photons).},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Boyd, Stephen and Vandenberghe, Lieven},
booktitle = {Optimization Methods and Software},
doi = {10.1080/10556781003625177},
eprint = {1111.6189v1},
file = {:home/arnold/Documents/pdf-library/Library/Boyd, Vandenberghe{\_}2010{\_}Convex Optimization.pdf:pdf},
isbn = {9780521833783},
issn = {10556788},
number = {3},
pages = {487--487},
pmid = {20876008},
title = {{Convex Optimization}},
url = {https://web.stanford.edu/{~}boyd/cvxbook/bv{\_}cvxbook.pdf},
volume = {25},
year = {2010}
}
@incollection{Lacoste-Julien2017bc,
author = {Lacoste-Julien, Simon},
booktitle = {Structured Prediction - IFT 6085 Notes},
file = {:home/arnold/Documents/pdf-library/Library/Lacoste-Julien{\_}2017{\_}Lecture 1 - Scribbles.pdf:pdf},
pages = {1--3},
title = {{Lecture 8 - Scribbles}},
year = {2017}
}
@article{Kuck2006,
author = {K{\"{u}}ck, Hendrik and {De Freitas}, Nando and Doucet, Arnaud},
doi = {10.1109/NSSPW.2006.4378829},
isbn = {1424405815},
issn = {1424405815},
journal = {NSSPW - Nonlinear Statistical Signal Processing Workshop 2006},
number = {2},
title = {{SMC samplers for Bayesian optimal nonlinear design}},
year = {2006}
}
@article{Martinez-Cantin2009,
abstract = {We address the problem of online path planning for optimal sensing with a mobile robot. The objective of the robot is to learn the most about its pose and the environment given time constraints. We use a POMDP with a utility function that depends on the belief state to model the finite horizon planning problem. We replan as the robot progresses throughout the environment. The POMDP is high-dimensional, continuous, non-differentiable, nonlinear, non-Gaussian and must be solved in real-time. Most existing techniques for stochastic planning and reinforcement learning are therefore inapplicable. To solve this extremely complex problem, we propose a Bayesian optimization method that dynamically trades off exploration (minimizing uncertainty in unknown parts of the policy space) and exploitation (capitalizing on the current best solution). We demonstrate our approach with a visually-guide mobile robot. The solution proposed here is also applicable to other closely-related domains, including active vision, sequential experimental design, dynamic sensing and calibration with mobile sensors.},
archivePrefix = {arXiv},
arxivId = {0712.3744},
author = {Martinez-Cantin, Ruben and {De Freitas}, Nando and Brochu, Eric and Castellanos, Jos{\'{e}} and Doucet, Arnaud},
doi = {10.1007/s10514-009-9130-2},
eprint = {0712.3744},
file = {:home/arnold/Documents/pdf-library/Library/Martinez-Cantin et al.{\_}2009{\_}A Bayesian exploration-exploitation approach for optimal online sensing and planning with a visually guided.pdf:pdf},
isbn = {0929-5593},
issn = {09295593},
journal = {Autonomous Robots},
keywords = {Active SLAM,Active learning,Active vision,Attention and gaze planning,Bayesian optimization,Dynamic sensor networks,Model predictive control,Online path planning,Policy search,Reinforcement learning,Sequential experimental design},
number = {2},
pages = {93--103},
title = {{A Bayesian exploration-exploitation approach for optimal online sensing and planning with a visually guided mobile robot}},
volume = {27},
year = {2009}
}
@incollection{Lacoste-Julien2017bd,
author = {Lacoste-Julien, Simon},
booktitle = {Structured Prediction - IFT 6085 Notes},
file = {:home/arnold/Documents/pdf-library/Library/Lacoste-Julien{\_}2017{\_}Lecture 6 - Scribbles.pdf:pdf},
pages = {1--5},
title = {{Lecture 6 - Scribbles}},
year = {2017}
}
@article{Taskar2005a,
abstract = {We consider large margin estimation in a broad range of prediction models where inference involves solving combinatorial optimization problems, for example, weighted graph-cuts or matchings. Our goal is to learn parameters such that inference using the model reproduces correct answers on the training data. Our method relies on the expressive power of convex optimization problems to compactly capture inference or solution optimality in structured prediction models. Directly embedding this structure within the learning formulation produces concise convex problems for efficient estimation of very complex and diverse models. We describe experimental results on a matching task, disulfide connectivity prediction, showing significant improvements over state-of-the-art methods.},
author = {Taskar, Ben and Chatalbashev, Vassil and Koller, Daphne and Guestrin, Carlos},
doi = {10.1145/1102351.1102464},
file = {:home/arnold/Documents/pdf-library/Library/Taskar et al.{\_}2005{\_}Learning structured prediction models A large margin approach.pdf:pdf},
isbn = {1595931805},
issn = {1595931805},
journal = {Proceedings of the 22nd international conference on Machine learning},
number = {December},
pages = {896--903},
title = {{Learning structured prediction models: A large margin approach}},
url = {http://dx.doi.org/10.1145/1102351.1102464},
year = {2005}
}
@article{Taskar2006a,
abstract = {We present a simple and scalable algorithm for maximum-margin estimation of structured output models, including an important class of Markov networks and combinatorial models. We formulate the estimation problem as a convex-concave saddle-point problem that allows us to use simple projection methods based on the dual extragradient algorithm (Nesterov, 2003). The projection step can be solved using dynamic programming or combinatorial algorithms for min-cost convex flow, depending on the structure of the problem. We show that this approach provides a memory-efficient alternative to formulations based on reductions to a quadratic program (QP). We analyze the convergence of the method and present experiments on two very different structured prediction tasks: 3D image segmentation and word alignment, illustrating the favorable scaling properties of our algorithm.},
author = {Taskar, Ben and Lacoste-Julien, Simon and Jordan, Michael I.},
doi = {10.1.1.62.4214},
file = {:home/arnold/Documents/pdf-library/Library/Taskar, Lacoste-Julien, Jordan{\_}2006{\_}Structured Prediction, Dual Extragradient and Bregman Projections.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {The Journal of Machine Learning Research},
keywords = {Computational,Information-Theoretic Learning with,Machine Vision,Natural Language Processing,Theory {\&} Algorithms},
pages = {1627--1653},
title = {{Structured Prediction, Dual Extragradient and Bregman Projections}},
volume = {7},
year = {2006}
}
@article{Taskar2003a,
abstract = {In typical classification tasks, we seek a function which assigns a label to a single object. Kernel-based approaches, such as support vector machines (SVMs), which maximize the margin of confidence of the classifier, are the method of choice for many such tasks. Their popularity stems both from the ability to use high-dimensional feature spaces, and from their strong theoretical guarantees. However, many real-world tasks involve sequential, spatial, or structured data, where multiple labels must be assigned. Existing kernel-based methods ignore structure in the problem, assigning labels independently to each object, losing much useful information. Conversely, probabilistic graphical models, such as Markov networks, can represent correlations between labels, by exploiting problem structure, but cannot handle high-dimensional feature spaces, and lack strong theoretical generalization guarantees. In this paper, we present a new framework that combines the advantages of both approaches: Maximum margin Markov (M3 ) networks incorporate both kernels, which efficiently deal with high-dimensional features, and the ability to capture correlations in structured data. We present an efficient algorithm for learning M3 networks based on a compact quadratic program formulation. We provide a new theoretical bound for generalization in structured domains. Experiments on the task of handwritten character recognition and collective hypertext classification demonstrate very significant gains over previous approaches.},
author = {Taskar, Ben and Guestrin, Carlos and Koller, Daphne},
doi = {10.1.1.129.8439},
file = {:home/arnold/Documents/pdf-library/Library/Taskar, Guestrin, Koller{\_}2003{\_}Max-margin Markov networks.pdf:pdf},
isbn = {1049-5258},
journal = {Advances in Neural Information Processing Systems 16 - NIPS'03},
pages = {25--32},
title = {{Max-margin Markov networks}},
url = {https://www.google.com/books?hl=en{\&}lr={\&}id=0F-9C7K8fQ8C{\&}oi=fnd{\&}pg=PA25{\&}dq=max+margin+markov+networks{\&}ots=THFtlWO741{\&}sig=Kuvbe{\_}fKGhmmB1kTCXm1cemhlmM{\%}5Cnhttp://papers.nips.cc/paper/2397-max-margin-markov-networks.pdf},
year = {2003}
}
@book{Alpern2003,
abstract = {The Theory of Search Games and Rendezvous widens the dimensions to the classical problem with the addition of an independent player of equal status to the searcher, who cares about being found or not being found. These multiple motives of searcher and hider are analytically and mathematically considered in the book's two foci: Search Games (Book I) and Rendezvous Theory (Book II). Shmuel Gal's work on Search Games (Gal, 1980) stimulated considerable research in a variety of fields including Computer Science, Engineering, Biology, and Economics. Steve Alpern's original formulation of the rendezvous search problem in 1976 and his formalization of the continuous version (Alpern, 1995) have led to much research in rendezvous theory in the past few years. New material is covered in both Search Games (Book I) and Rendezvous Theory (Book II). The book examines a whole variety of new configurations of theory and problems that arise from these two aspects of the analysis - resulting in a penetrating state-of-the-art treatment of this highly useful mathematical, analytical tool.},
author = {Alpern, Steve and Gal, Shmuel},
doi = {10.1007/b100809},
file = {:home/arnold/Documents/pdf-library/Library/Alpern, Gal{\_}2003{\_}The Theory of Search Games and Rendezvous.pdf:pdf},
isbn = {0-7923-7468-1},
issn = {10974199},
publisher = {Kluwer Academic Publishers},
title = {{The Theory of Search Games and Rendezvous}},
year = {2003}
}
@article{Baeza-Yates1993,
author = {Baeza-Yates, Ricardo A. and Culberson, Joseph C. and Rawlins, Gregory J. E.},
doi = {10.1006/inco.1993.1054},
file = {:home/arnold/Documents/pdf-library/Library/Baeza-Yates, Culberson, Rawlins{\_}1993{\_}Searching in the Plane.pdf:pdf},
issn = {0890-5401},
journal = {Information and Computation},
number = {2},
pages = {234--252},
title = {{Searching in the Plane}},
volume = {106},
year = {1993}
}
@article{Alpern1999,
author = {Alpern, Steve and Baston, V. J. and Essegaier, Skander},
file = {:home/arnold/Documents/pdf-library/Library/Alpern, Baston, Essegaier{\_}1999{\_}Rendezvous search on a graph.pdf:pdf},
issn = {00219002},
journal = {Journal of Applied Probability},
keywords = {Rendezvous search,Search game},
number = {1},
pages = {223--231},
title = {{Rendezvous search on a graph}},
volume = {36},
year = {1999}
}
@article{Brooks2011,
abstract = {Hamiltonian dynamics can be used to produce distant proposals for the Metropolis algorithm, thereby avoiding the slow exploration of the state space that results from the diffusive behaviour of simple random-walk proposals. Though originating in physics, Hamiltonian dynamics can be applied to most problems with continuous state spaces by simply introducing fictitious "momentum" variables. A key to its usefulness is that Hamiltonian dynamics preserves volume, and its trajectories can thus be used to define complex mappings without the need to account for a hard to compute Jacobian factor - a property that can be exactly maintained even when the dynamics is approximated by discretizing time. In this review, discuss theoretical and practical aspects of Hamiltonian Monte Carlo, and present some of its variations, including using windows of states for approximations, tempering during the course of a trajectory to handle isolated modes, and short-cut methods that prevent useless trajectories form taking much computation time.},
archivePrefix = {arXiv},
arxivId = {arXiv:1206.1901v1},
author = {Brooks, Steve and Gelman, Andrew and Jones, Galin L. and Meng, Xiao-Li},
doi = {10.1201/b10905},
eprint = {arXiv:1206.1901v1},
file = {:home/arnold/Documents/pdf-library/Library/Brooks et al.{\_}2011{\_}Handbook of Markov Chain Monte Carlo.pdf:pdf},
isbn = {978-1-4200-7941-8},
issn = {{\textless}null{\textgreater}},
journal = {Handbook of Markov Chain Monte Carlo},
keywords = {hamiltonian dynamics,mcmc},
pages = {113--162},
pmid = {25246403},
publisher = {Chapman {\&} Hall/CRC},
title = {{Handbook of Markov Chain Monte Carlo}},
url = {http://www.crcnetbase.com/doi/book/10.1201/b10905},
volume = {20116022},
year = {2011}
}
@article{Chipman2014,
author = {Chipman, Hugh and George, Edward I and Mcculloch, Robert E and Clyde, M and Foster, Dean P and Stine, Robert},
doi = {10.1214/lnms/1215540964},
file = {:home/arnold/Documents/pdf-library/Library/Chipman et al.{\_}2014{\_}The Practical Implementation of Bayesian Model Selection.pdf:pdf},
isbn = {0-940600-52-8},
journal = {Lecture Notes-Monograph Series},
number = {2001},
pages = {65--134},
title = {{The Practical Implementation of Bayesian Model Selection}},
volume = {38},
year = {2014}
}
@incollection{Neal2011,
abstract = {Hamiltonian dynamics can be used to produce distant proposals for the Metropolis algorithm, thereby avoiding the slow exploration of the state space that results from the diffusive behaviour of simple random-walk proposals. Though originating in physics, Hamiltonian dynamics can be applied to most problems with continuous state spaces by simply introducing fictitious "momentum" variables. A key to its usefulness is that Hamiltonian dynamics preserves volume, and its trajectories can thus be used to define complex mappings without the need to account for a hard to compute Jacobian factor - a property that can be exactly maintained even when the dynamics is approximated by discretizing time. In this review, discuss theoretical and practical aspects of Hamiltonian Monte Carlo, and present some of its variations, including using windows of states for approximations, tempering during the course of a trajectory to handle isolated modes, and short-cut methods that prevent useless trajectories form taking much computation time.},
archivePrefix = {arXiv},
arxivId = {arXiv:1206.1901v1},
author = {Neal, Radford M},
booktitle = {Handbook of Markov Chain Monte Carlo},
doi = {10.1201/b10905},
eprint = {arXiv:1206.1901v1},
file = {:home/arnold/Documents/pdf-library/Library/Neal{\_}2011{\_}MCMC using Hamiltonian dynamics.pdf:pdf},
isbn = {978-1-4200-7941-8},
issn = {{\textless}null{\textgreater}},
keywords = {hamiltonian dynamics,mcmc},
pages = {113--162},
pmid = {25246403},
title = {{MCMC using Hamiltonian dynamics}},
url = {http://www.crcnetbase.com/doi/book/10.1201/b10905},
volume = {20116022},
year = {2011}
}
@article{Sutton2010a,
abstract = {Often we wish to predict a large number of variables that depend on each other as well as on other observed variables. Structured prediction methods are essentially a combination of classification and graphical modeling, combining the ability of graphical models to compactly model multivariate data with the ability of classification methods to perform prediction using large sets of input features. This tutorial describes conditional random fields, a popular probabilistic method for structured prediction. CRFs have seen wide application in natural language processing, computer vision, and bioinformatics. We describe methods for inference and parameter estimation for CRFs, including practical issues for implementing large scale CRFs. We do not assume previous knowledge of graphical modeling, so this tutorial is intended to be useful to practitioners in a wide variety of fields.},
archivePrefix = {arXiv},
arxivId = {1011.4088},
author = {Sutton, Charles and McCallum, Andrew},
doi = {10.1561/2200000013},
eprint = {1011.4088},
file = {:home/arnold/Documents/pdf-library/Library/Sutton, McCallum{\_}2010{\_}An Introduction to Conditional Random Fields.pdf:pdf},
isbn = {9781601985729},
issn = {1935-8237},
journal = {Machine Learning},
number = {4},
pages = {267--373},
pmid = {19008334},
title = {{An Introduction to Conditional Random Fields}},
url = {http://arxiv.org/abs/1011.4088{\%}5Cnhttp://www.arxiv.org/pdf/1011.4088.pdf},
volume = {4},
year = {2010}
}
@article{Joachims2009a,
abstract = {Discriminative training approaches like structural SVMs have shown much promise for building highly complex and accurate models in areas like natural language processing, protein structure prediction, and information retrieval. However, current training algorithms are computationally expensive or intractable on large datasets. To overcome this bottleneck, this paper explores how cutting-plane methods can provide fast training not only for classification SVMs, but also for structural SVMs. We show that for an equivalent “1-slack” reformulation of the linear SVM training problem, our cutting-plane method has time complexity linear in the number of training examples. In particular, the number of iterations does not depend on the number of training examples, and it is linear in the desired precision and the regularization parameter. Furthermore, we present an extensive empirical evaluation of the method applied to binary classification, multi-class classification, HMM sequence tagging, and CFG parsing. The experiments show that the cutting-plane algorithm is broadly applicable and fast in practice. On large datasets, it is typically several orders of magnitude faster than conventional training methods derived from decomposition methods like SVM-light, or conventional cutting-plane methods. Implementations of our methods are available at                   www.joachims.org                                  .},
author = {Joachims, Thorsten and Finley, Thomas and Yu, Chun Nam John},
doi = {10.1007/s10994-009-5108-8},
file = {:home/arnold/Documents/pdf-library/Library/Joachims, Finley, Yu{\_}2009{\_}Cutting-plane training of structural SVMs.pdf:pdf},
isbn = {0885-6125},
issn = {08856125},
journal = {Machine Learning},
keywords = {Structural SVMs,Structured output prediction,Support vector machines,Training algorithms},
number = {1},
pages = {27--59},
title = {{Cutting-plane training of structural SVMs}},
volume = {77},
year = {2009}
}
@incollection{Lacoste-Julien2015i,
author = {Lacoste-Julien, Simon},
booktitle = {Introduction to Graphical Models},
file = {:home/arnold/Documents/pdf-library/Library/Lacoste-Julien{\_}2015{\_}Lecture 6 - Directed and Undirected Graphical Models.pdf:pdf},
pages = {1--9},
title = {{Lecture 6 - Directed and Undirected Graphical Models}},
year = {2015}
}
@incollection{Lacoste-Julien2015j,
author = {Lacoste-Julien, Simon},
booktitle = {Introduction to Graphical Models},
file = {:home/arnold/Documents/pdf-library/Library/Lacoste-Julien{\_}2015{\_}Lecture 5 - Information Theory, Exponential Families.pdf:pdf},
pages = {1--12},
title = {{Lecture 5 - Information Theory, Exponential Families}},
year = {2015}
}
@incollection{Lacoste-Julien2015k,
author = {Lacoste-Julien, Simon},
booktitle = {Introduction to Graphical Models},
file = {:home/arnold/Documents/pdf-library/Library/Lacoste-Julien{\_}2015{\_}Lecture 7 - Sum Product Algorithm and HMMs.pdf:pdf},
number = {November 2014},
pages = {1--17},
title = {{Lecture 7 - Sum Product Algorithm and HMMs}},
volume = {1},
year = {2015}
}
@incollection{Lacoste-Julien2015l,
author = {Lacoste-Julien, Simon},
booktitle = {Introduction to Graphical Models},
file = {:home/arnold/Documents/pdf-library/Library/Lacoste-Julien{\_}2015{\_}Lecture 3 - k-means, EM, Gaussian Mixture, Graph Theory.pdf:pdf},
pages = {1--15},
title = {{Lecture 3 - k-means, EM, Gaussian Mixture, Graph Theory}},
year = {2015}
}
@incollection{Lacoste-Julien2015m,
author = {Lacoste-Julien, Simon},
booktitle = {Introduction to Graphical Models},
file = {:home/arnold/Documents/pdf-library/Library/Lacoste-Julien{\_}2015{\_}Lecture 4 - Directed and Undirected Graphical Models.pdf:pdf},
pages = {1--14},
title = {{Lecture 4 - Directed and Undirected Graphical Models}},
volume = {0},
year = {2015}
}
@incollection{Lacoste-Julien2015n,
author = {Lacoste-Julien, Simon},
booktitle = {Introduction to Graphical Models},
file = {:home/arnold/Documents/pdf-library/Library/Lacoste-Julien{\_}2015{\_}Lecture 1 - Probabilistic Graphical Models.pdf:pdf},
pages = {1--15},
title = {{Lecture 1 - Probabilistic Graphical Models}},
volume = {100},
year = {2015}
}
@incollection{Lacoste-Julien2015o,
author = {Lacoste-Julien, Simon},
booktitle = {Introduction to Graphical Models},
file = {:home/arnold/Documents/pdf-library/Library/Lacoste-Julien{\_}2015{\_}Lecture 2 - Single Node Models.pdf:pdf},
pages = {1--8},
title = {{Lecture 2 - Single Node Models}},
year = {2015}
}
@incollection{Lacoste-Julien2015p,
author = {Lacoste-Julien, Simon},
booktitle = {Introduction to Graphical Models},
file = {:home/arnold/Documents/pdf-library/Library/Lacoste-Julien{\_}2015{\_}Lecture 8 - Learning in Graphical Models {\&} MC Methods.pdf:pdf},
pages = {1--13},
title = {{Lecture 8 - Learning in Graphical Models {\&} MC Methods}},
year = {2015}
}
@incollection{Lacoste-Julien2015q,
author = {Lacoste-Julien, Simon},
booktitle = {Introduction to Graphical Models},
file = {:home/arnold/Documents/pdf-library/Library/Lacoste-Julien{\_}2015{\_}Lecture 10 - Bayesian Methods.pdf:pdf},
number = {July},
pages = {1--69},
title = {{Lecture 10 - Bayesian Methods}},
year = {2015}
}
@incollection{Lacoste-Julien2017be,
author = {Lacoste-Julien, Simon},
booktitle = {Structured Prediction - IFT 6085 Notes},
file = {:home/arnold/Documents/pdf-library/Library/Lacoste-Julien{\_}2017{\_}Lecture 5 - scribbles.pdf:pdf},
pages = {1--8},
title = {{Lecture 5 - scribbles}},
year = {2017}
}
@article{Krause2012a,
abstract = {Submodularity1 is a property of set functions with deep theoretical consequences and far– reaching applications. At first glance it appears very similar to concavity, in other ways it resembles convexity. It appears in a wide variety of applications: in Computer Science it ... $\backslash$n},
author = {Krause, Andreas and Golovin, Daniel},
doi = {10.1017/CBO9781139177801.004},
file = {:home/arnold/Documents/pdf-library/Library/Krause, Golovin{\_}2012{\_}Submodular function maximization.pdf:pdf},
isbn = {9781139177801},
issn = {{\textless}null{\textgreater}},
journal = {Tractability: Practical Approaches to Hard Problems},
pages = {71--104},
title = {{Submodular function maximization}},
volume = {3},
year = {2012}
}
@incollection{Lacoste-Julien2017bf,
author = {Lacoste-Julien, Simon},
booktitle = {Structured Prediction - IFT 6085 Notes},
file = {:home/arnold/Documents/pdf-library/Library/Lacoste-Julien{\_}2017{\_}Lecture 1 - Scribbles.pdf:pdf},
pages = {1--6},
title = {{Lecture 4 - Scribbles}},
year = {2017}
}
@article{Lacoste-Julien2012a,
abstract = {In this note, we present a new averaging technique for the projected stochastic subgradient method. By using a weighted average with a weight of t+1 for each iterate w{\_}t at iteration t, we obtain the convergence rate of O(1/t) with both an easy proof and an easy implementation. The new scheme is compared empirically to existing techniques, with similar performance behavior.},
archivePrefix = {arXiv},
arxivId = {arXiv:1212.2002v2},
author = {Lacoste-Julien, S and Schmidt, Mark and Bach, Francis},
eprint = {arXiv:1212.2002v2},
journal = {arXiv preprint arXiv:1212.2002},
number = {c},
pages = {1--8},
title = {{A simpler approach to obtaining an O(1/t) convergence rate for the projected stochastic subgradient method}},
url = {http://arxiv.org/abs/1212.2002},
volume = {2},
year = {2012}
}
@article{Lacoste-Julien2013a,
abstract = {We propose a randomized block-coordinate variant of the classic Frank-Wolfe algorithm for convex optimization with block-separable constraints. Despite its lower iteration cost, we show that it achieves a similar convergence rate in duality gap as the full Frank-Wolfe algorithm. We also show that, when applied to the dual structural support vector machine (SVM) objective, this yields an online algorithm that has the same low iteration complexity as primal stochastic subgradient methods. However, unlike stochastic subgradient methods, the block-coordinate Frank-Wolfe algorithm allows us to compute the optimal step-size and yields a computable duality gap guarantee. Our experiments indicate that this simple algorithm outperforms competing structural SVM solvers.},
archivePrefix = {arXiv},
arxivId = {1207.4747},
author = {Lacoste-Julien, Simon and Jaggi, Martin and Schmidt, Mark and Pletscher, Patrick},
eprint = {1207.4747},
journal = {International Conference on Machine Learning},
pages = {9},
title = {{Block-Coordinate Frank-Wolfe Optimization for Structural SVMs}},
url = {http://arxiv.org/abs/1207.4747},
volume = {28},
year = {2013}
}
@article{Singh2007,
abstract = {In many sensing applications, including environmental monitoring, measurement systems must cover a large space with only limited sensing resources. One approach to achieve required sensing coverage is to use robots to convey sensors within this space. Planning the motion of these robots - coordinating their paths in order to maximize the amount of information collected while placing bounds on their resources (e.g., path length or energy capacity) - is aNP-hard problem. In this paper, we present an efficient path planning algorithm that coordinates multiple robots, each having a resource constraint, to maximize the "informativeness" of their visited locations. In particular, we use a Gaussian Process to model the underlying phenomenon, and use the mutual information between the visited locations and remainder of the space to characterize the amount of information collected. We provide strong theoretical approximation guarantees for our algorithm by exploiting the submodularity property of mutual information. In addition, we improve the efficiency of our approach by extending the algorithm using branch and bound and a region-based decomposition of the space. We provide an extensive empirical analysis of our algorithm, comparing with existing heuristics on datasets from several real world sensing applications.},
author = {Singh, Amarjeet and Kaiser, William and Batalin, Maxim and Krause, Andreas and Guestrin, Carlos},
file = {:home/arnold/Documents/pdf-library/Library/Singh et al.{\_}2007{\_}Efficient planning of informative paths for multiple robots.pdf:pdf},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {multi-robot systems,planning under uncertainty,probabilistic reasoning,sensor networks},
pages = {2204--2211},
title = {{Efficient planning of informative paths for multiple robots}},
year = {2007}
}
@article{Jaggi2013,
abstract = {We provide stronger and more general primal-dual convergence results for Frank- Wolfe-type algorithms (a.k.a. conditional gradient) for constrained convex optimiza- tion, enabled by a simple framework of du- ality gap certificates. Our analysis also holds if the linear subproblems are only solved ap- proximately (as well as if the gradients are inexact), and is proven to be worst-case opti- mal in the sparsity of the obtained solutions. On the application side, this allows us to unify a large variety of existing sparse greedy methods, in particular for optimization over convex hulls of an atomic set, even if those sets can only be approximated, including sparse (or structured sparse) vectors or ma- trices, low-rank matrices, permutation matri- ces, or max-norm bounded matrices. We present a new general framework for con- vex optimization over matrix factorizations, where every Frank-Wolfe iteration will con- sist of a low-rank update, and discuss the broad application areas of this approach.},
author = {Jaggi, Martin},
file = {:home/arnold/Documents/pdf-library/Library/Jaggi{\_}2013{\_}Revisiting Frank-Wolfe Projection-Free Sparse Convex Optimization.pdf:pdf},
journal = {Proceedings of the 30th International Conference on Machine Learning},
keywords = {{\{}{\#}{\}}4April4Mendeley-Unclassified},
pages = {427--435},
title = {{Revisiting Frank-Wolfe: Projection-Free Sparse Convex Optimization}},
volume = {28},
year = {2013}
}
@misc{Jaggi2014,
author = {Jaggi, Martin and Harachaoui, Zaid},
booktitle = {ICML Workshop on Sparsity},
keywords = {nk-wolfe and},
title = {{Frank-Wolfe and Greedy Optimization I}},
year = {2014}
}
@misc{Jaggi2014a,
author = {Jaggi, Martin and Harachaoui, Zaid},
booktitle = {ICML Workshop on Sparsity},
title = {{Frank-Wolfe and Greedy Optimization II}},
year = {2014}
}
@misc{Vedaldi2013a,
abstract = {This tutorial introduces Structured Support Vector Machines (SSVMs) as a tool to effectively learn functions over arbitrary spaces. For example, one can use a SSVM to rank a set of items by decreasing relevance, to localise an object such as a cat in an image, or to estimate the pose of a human in a video. The tutorial reviews the standard notion of SVM and shows how this can be extended to arbitrary output spaces, introducing the corresponding learning formulations. It then gives a complete example on how to design and learn a SSVM with off-the-shelf solvers in MATLAB. The last part discusses how such solvers can be implemented, focusing in particular on the cutting plane and BMRM algorithms. 2 / 85},
author = {Vedaldi, Andrea},
keywords = {SSVM},
mendeley-tags = {SSVM},
title = {{Flexible discriminative learning with structured output support vector machines}},
year = {2013}
}
@incollection{Lacoste-Julien2017bg,
author = {Lacoste-Julien, Simon},
booktitle = {Structured Prediction - IFT 6085 Notes},
file = {:home/arnold/Documents/pdf-library/Library/Lacoste-Julien{\_}2017{\_}Lecture 3 - Scribbles.pdf:pdf},
pages = {1--7},
title = {{Lecture 3 - Scribbles}},
year = {2017}
}
@incollection{Lacoste-Julien2017bh,
author = {Lacoste-Julien, Simon},
booktitle = {Structured Prediction - IFT 6085 Notes},
file = {:home/arnold/Documents/pdf-library/Library/Lacoste-Julien{\_}2017{\_}Lecture 2 - Scribbles.pdf:pdf},
pages = {1--6},
title = {{Lecture 2 - Scribbles}},
year = {2017}
}
@article{Mccallum2006a,
author = {Mccallum, Andrew and Pal, Chris and Druck, Greg and Wang, Xuerui},
journal = {American Association for Artificial Intelligence},
title = {{Multi-Conditional Learning : Generative / Discriminative Training for Clustering and Classification}},
year = {2006}
}
@article{Souffriau2010,
abstract = {This paper introduces a Path Relinking metaheuristic approach for solving the Team Orienteering Problem (TOP), a particular routing problem in which a score is earned for visiting a location. The objective is to maximise the sum of the scores, while not exceeding a time budget Tmax for travelling to the selected locations. In the case of the simple Orienteering Problem (OP), a single route connecting all selected locations should be followed; in TOP m routes are required and the length of each route is restricted to Tmax. A fast and a slow variant of the approach are tested using a large set of test instances from the literature; they are compared to other state-of-the-art approaches. The fast variant achieves an average gap of 0.39{\%} to the best known solutions in 5.0 s of calculation time, while the slow variant achieves a 0.04{\%} gap within 272.8 s. Moreover, next to achieving most of the best known solutions for many testproblems, the slow variant improved the best known results in five instances. {\textcopyright} 2009 Elsevier Ltd. All rights reserved.},
author = {Souffriau, Wouter and Vansteenwegen, Pieter and {Vanden Berghe}, Greet and {Van Oudheusden}, Dirk},
doi = {10.1016/j.cor.2009.05.002},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Souffriau et al.{\_}2010{\_}A Path Relinking approach for the Team Orienteering Problem.pdf:pdf},
issn = {03050548},
journal = {Computers and Operations Research},
keywords = {Metaheuristic,Path Relinking,Team Orienteering Problem},
number = {11},
pages = {1853--1859},
publisher = {Elsevier},
title = {{A Path Relinking approach for the Team Orienteering Problem}},
url = {http://dx.doi.org/10.1016/j.cor.2009.05.002},
volume = {37},
year = {2010}
}
@incollection{Lacoste-Julien2017bi,
author = {Lacoste-Julien, Simon},
booktitle = {Structured Prediction - IFT 6085 Notes},
file = {:home/arnold/Documents/pdf-library/Library/Lacoste-Julien{\_}2017{\_}Lecture 1 - Scribbles.pdf:pdf},
pages = {1--5},
title = {{Lecture 1 - Scribbles}},
year = {2017}
}
@article{LeCun2006a,
abstract = {Energy-Based Models (EBMs) capture dependencies between variables by associating a scalar energy to each configuration of the variables. Inference consists in clamping the value of observed variables and finding configurations of the remaining variables that minimize the energy. Learning consists in finding an energy function in which observed configurations of the variables are given lower energies than unobserved ones. The EBM approach provides a common theoretical framework for many learning models, including traditional discriminative and generative approaches, as well as graph-transformer networks, conditional random fields, maximum margin Markov networks, and several manifold learning methods. Probabilistic models must be properly normalized, which sometimes requires evaluating intractable integrals over the space of all possible variable configurations. Since EBMs have no requirement for proper normalization, this problem is naturally circumvented. EBMs can be viewed as a form of non-probabilistic factor graphs, and they provide considerably more flexibility in the design of architectures and training criteria than probabilistic approaches.},
author = {LeCun, Yann and Chopra, Sumit and Hadsell, Raia and Ranzato, Marc'Aurelio and Huang, Fu Jie},
doi = {10.1198/tech.2008.s913},
isbn = {9780262026178},
issn = {{\textless}null{\textgreater}},
journal = {Predicting Structured Data},
pages = {191--246},
title = {{A Tutorial on Energy-Based Learning}},
year = {2006}
}
@book{Jordan2017,
author = {Jordan, Michael I.},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Jordan{\_}2017{\_}An Introduction to Probabilistic Graphical Models (Manuscript).pdf:pdf},
title = {{An Introduction to Probabilistic Graphical Models (Manuscript)}},
year = {2017}
}
@article{Ke2008,
abstract = {The team orienteering problem (TOP) involves finding a set of paths from the starting point to the ending point such that the total collected reward received from visiting a subset of locations is maximized and the length of each path is restricted by a pre-specified limit. In this paper, an ant colony optimization (ACO) approach is proposed for the team orienteering problem. Four methods, i.e., the sequential, deterministic-concurrent and random-concurrent and simultaneous methods, are proposed to construct candidate solutions in the framework of ACO. We compare these methods according to the results obtained on well-known problems from the literature. Finally, we compare the algorithm with several existing algorithms. The results show that our algorithm is promising. ?? 2007 Elsevier Ltd. All rights reserved.},
author = {Ke, Liangjun and Archetti, Claudia and Feng, Zuren},
doi = {10.1016/j.cie.2007.10.001},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Ke, Archetti, Feng{\_}2008{\_}Ants can solve the team orienteering problem.pdf:pdf},
issn = {03608352},
journal = {Computers and Industrial Engineering},
keywords = {Ant colony optimization,Ant system,Heuristics,Team orienteering problem},
number = {3},
pages = {648--665},
title = {{Ants can solve the team orienteering problem}},
volume = {54},
year = {2008}
}
@article{Vansteenwegen2011,
abstract = {During the last decade, a number of challenging applications in logistics, tourism and other fields were modelled as orienteering problems (OP). In the orienteering problem, a set of vertices is given, each with a score. The goal is to determine a path, limited in length, that visits some vertices and maximises the sum of the collected scores. In this paper, the literature about the orienteering problem and its applications is reviewed. The OP is formally described and many relevant variants are presented. All published exact solution approaches and (meta) heuristics are discussed and compared. Interesting open research questions concerning the OP conclude this paper. ?? 2010 Elsevier B.V. All rights reserved.},
author = {Vansteenwegen, Pieter and Souffriau, Wouter and Oudheusden, Dirk Van},
doi = {10.1016/j.ejor.2010.03.045},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Vansteenwegen, Souffriau, Oudheusden{\_}2011{\_}The orienteering problem A survey.pdf:pdf},
isbn = {03772217},
issn = {03772217},
journal = {European Journal of Operational Research},
keywords = {Combinatorial optimisation,Orienteering problem,Survey},
number = {1},
pages = {1--10},
publisher = {Elsevier B.V.},
title = {{The orienteering problem: A survey}},
url = {http://dx.doi.org/10.1016/j.ejor.2010.03.045},
volume = {209},
year = {2011}
}
@article{Binney2010,
abstract = {We present a path planning method for autonomous underwater vehicles in order to maximize mutual information. We adapt a method previously used for surface vehicles, and extend it to deal with the unique characteristics of underwater vehicles. We show how to generate near-optimal paths while ensuring that the vehicle stays out of high-traffic areas during predesignated time intervals. In our objective function we explicitly account for the fact that underwater vehicles typically take measurements while moving, and that they do not have the ability to communicate until they resurface. We present field results from ocean trials on planning paths for a specific AUV, an underwater glider.},
author = {Binney, Jonathan and Krause, Andreas and Sukhatme, Gaurav S},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Binney, Krause, Sukhatme{\_}2010{\_}Informative Path Planning for an Autonomous Underwater Vehicle.pdf:pdf;:home/arnold/Documents/pdf-library/Library/Binney, Krause, Sukhatme{\_}2010{\_}Informative Path Planning for an Autonomous Underwater Vehicle.pdf:pdf},
isbn = {9781424450404},
pages = {4791--4796},
title = {{Informative Path Planning for an Autonomous Underwater Vehicle}},
year = {2010}
}
@article{Yilmaz2008,
abstract = {The goal of adaptive sampling in the ocean is to predict the types and locations of additional ocean measurements that would be most useful to collect. Quantitatively, what is most useful is defined by an objective function and the goal is then to optimize this objective under the constraints of the available observing network. Examples of objectives are better oceanic understanding, to improve forecast quality, or to sample regions of high interest. This work provides a new path-planning scheme for the adaptive sampling problem. We define the path-planning problem in terms of an optimization framework and propose a method based on mixed integer linear programming (MILP). The mathematical goal is to find the vehicle path that maximizes the line integral of the uncertainty of field estimates along this path. Sampling this path can improve the accuracy of the field estimates the most. While achieving this objective, several constraints must be satisfied and are implemented. They relate to vehicle motion, intervehicle coordination, communication, collision avoidance, etc. The MILP formulation is quite powerful to handle different problem constraints and flexible enough to allow easy extensions of the problem. The formulation covers single- and multiple-vehicle cases as well as single- and multiple-day formulations. The need for a multiple-day formulation arises when the ocean sampling mission is optimized for several days ahead. We first introduce the details of the formulation, then elaborate on the objective function and constraints, and finally, present a varied set of examples to illustrate the applicability of the proposed method.},
author = {Yilmaz, N.K. and Evangelinos, Constantinos and Lermusiaux, P. and Patrikalakis, N.M.},
doi = {10.1109/JOE.2008.2002105},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Yilmaz et al.{\_}2008{\_}Path Planning of Autonomous Underwater Vehicles for Adaptive Sampling Using Mixed Integer Linear Programming.pdf:pdf},
isbn = {0001404105},
issn = {0364-9059},
journal = {IEEE Journal of Oceanic Engineering},
keywords = {AUV,adaptive sampling,path planning,underwater},
number = {4},
pages = {522--537},
title = {{Path Planning of Autonomous Underwater Vehicles for Adaptive Sampling Using Mixed Integer Linear Programming}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4768634},
volume = {33},
year = {2008}
}
@article{Gendreau1998,
abstract = {The Selective Traveling Salesman Problem (STSP) is defined on a graph in which profits are associated with vertices and costs are associated with edges. Some vertices are compulsory. The aim is to construct a tour of maximal profit including all compulsory vertices and whose cost does not exceed a preset constant. We developed several classes of valid inequalities for the symmetric STSP and used them in a branch-and-cut algorithm. Depending on problem parameters, the proposed algorithm can solve instances involving up to 300 vertices. {\textcopyright} 1998 John Wiley {\&} Sons, Inc. Networks 32: 263-273, 1998.},
author = {Gendreau, Michel and Laporte, Gilbert and Semet, Fr�d�ric},
doi = {10.1002/(SICI)1097-0037(199812)32:4<263::AID-NET3>3.0.CO;2-Q},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Gendreau, Laporte, Semet{\_}1998{\_}A branch-and-cut algorithm for the undirected selective traveling salesman problem.pdf:pdf},
isbn = {0028-3045},
issn = {0028-3045},
journal = {Networks},
keywords = {branch-and-cut algorithm,orienteering,selective traveling salesman problem},
number = {4},
pages = {263--273},
title = {{A branch-and-cut algorithm for the undirected selective traveling salesman problem}},
url = {http://doi.wiley.com/10.1002/(SICI)1097-0037(199812)32:4{\%}3C263::AID-NET3{\%}3E3.0.CO;2-Q},
volume = {32},
year = {1998}
}
@article{bowman2015generating,
author = {Bowman, Samuel R and Vilnis, Luke and Vinyals, Oriol and Dai, Andrew M and Jozefowicz, Rafal and Bengio, Samy},
journal = {arXiv preprint arXiv:1511.06349},
title = {{Generating sentences from a continuous space}},
year = {2015}
}
@inproceedings{bergstra2013hyperopt,
author = {Bergstra, James and Yamins, Dan and Cox, David D},
booktitle = {Proceedings of the 12th Python in Science Conference},
organization = {Citeseer},
pages = {13--20},
title = {{Hyperopt: A python library for optimizing the hyperparameters of machine learning algorithms}},
year = {2013}
}
@inproceedings{lau2015unsupervised,
author = {Lau, Jey Han and Clark, Alexander and Lappin, Shalom},
booktitle = {Proceedings of the 53rd Annual Conference of the Association for Computational Linguistics, Beijing, China},
title = {{Unsupervised prediction of acceptability judgements}},
year = {2015}
}
@article{Kingma2014a,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik and Ba, Jimmy},
eprint = {1412.6980},
journal = {arXiv:1412.6980 [cs.LG]},
pages = {1--15},
title = {{Adam: A method for stochastic optimization}},
url = {http://arxiv.org/abs/1412.6980},
year = {2014}
}
@article{fabius2014variational,
author = {Fabius, Otto and van Amersfoort, Joost R},
journal = {arXiv preprint arXiv:1412.6581},
title = {{Variational recurrent auto-encoders}},
year = {2014}
}
@article{Sutskever2014,
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excel-lent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Fi-nally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
archivePrefix = {arXiv},
arxivId = {1409.3215},
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
doi = {10.1007/s10107-014-0839-0},
eprint = {1409.3215},
file = {:home/arnold/Documents/pdf-library/Library/Sutskever, Vinyals, Le{\_}2014{\_}Sequence to sequence learning with neural networks.pdf:pdf},
isbn = {1409.3215},
issn = {09205691},
journal = {Advances in Neural Information Processing Systems (NIPS)},
pages = {3104--3112},
pmid = {2079951},
title = {{Sequence to sequence learning with neural networks}},
url = {http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural},
year = {2014}
}
@article{DzmitryBahdana2014,
abstract = {Neural machine translation is a recently proposed approach to machine transla-tion. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neu-ral machine translation often belong to a family of encoder–decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architec-ture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
archivePrefix = {arXiv},
arxivId = {1409.0473},
author = {{Dzmitry Bahdana} and Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
doi = {10.1146/annurev.neuro.26.041002.131047},
eprint = {1409.0473},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Dzmitry Bahdana et al.{\_}2014{\_}Neural Machine Translation By Jointly Learning To Align and Translate.pdf:pdf},
isbn = {0147-006X (Print)},
issn = {0147-006X},
journal = {Iclr 2015},
keywords = {Neural machine translation is a recently proposed,Unlike the traditional statistical machine transla,a source sentence into a fixed-length vector from,and propose to extend this by allowing a model to,bottleneck in improving the performance of this ba,for parts of a source sentence that are relevant t,having to form these parts as a hard segment expli,machine translation often belong to a family of en,maximize the translation performance. The models p,phrase-based system on the task of English-to-Fren,qualitative analysis reveals that the (soft-)align,the neural machine,translation aims at building a single neural netwo,translation. In this paper,we achieve a translation performance comparable to,we conjecture that the use of a fixed-length vecto,well with our intuition,without},
pages = {1--15},
pmid = {14527267},
title = {{Neural Machine Translation By Jointly Learning To Align and Translate}},
url = {http://arxiv.org/abs/1409.0473v3},
year = {2014}
}
@article{Cho2014,
abstract = {In this paper, we propose a novel neural network model called RNN Encoder--Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder--Decoder as an additional feature in the existing linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
archivePrefix = {arXiv},
arxivId = {1406.1078},
author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
doi = {10.3115/v1/D14-1179},
eprint = {1406.1078},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Cho et al.{\_}2014{\_}Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation.pdf:pdf},
isbn = {9781937284961},
issn = {09205691},
journal = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
keywords = {decoder,for statistical machine translation,rning phrase representations using,rnn encoder},
pages = {1724--1734},
pmid = {2079951},
title = {{Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation}},
url = {http://arxiv.org/abs/1406.1078},
year = {2014}
}
@article{Fabius2015,
abstract = {In this paper we propose a model that combines the strengths of RNNs and SGVB: the Variational Recurrent Auto-Encoder (VRAE). Such a model can be used for efficient, large scale unsupervised learning on time series data, mapping the time series data to a latent vector representation. The model is generative, such that data can be generated from samples of the latent space. An important contribution of this work is that the model can make use of unlabeled data in order to facilitate supervised training of RNNs by initialising the weights and network state.},
archivePrefix = {arXiv},
arxivId = {1412.6581},
author = {Fabius, Otto and van Amersfoort, Joost R.},
eprint = {1412.6581},
file = {:home/arnold/Documents/pdf-library/Library/Fabius, van Amersfoort{\_}2015{\_}Variational Recurrent Auto-Encoders.pdf:pdf},
journal = {Iclr},
number = {2013},
pages = {1--5},
title = {{Variational Recurrent Auto-Encoders}},
url = {http://arxiv.org/abs/1412.6581},
year = {2015}
}
@misc{Devroye2016,
author = {Devroye, Luc},
file = {:home/arnold/Documents/pdf-library/Library/Devroye{\_}2016{\_}Assignment 8 - Entropy.pdf:pdf},
title = {{Assignment 8 - Entropy}},
year = {2016}
}
@incollection{Devroye2016a,
author = {Devroye, Luc},
booktitle = {Probabilistic Analysis of Algorithms},
file = {:home/arnold/Documents/pdf-library/Library/Devroye{\_}2016{\_}Markov Chains Rate of Convergence.pdf:pdf},
title = {{Markov Chains: Rate of Convergence}},
year = {2016}
}
@article{Shkurti2015,
author = {Shkurti, Florian and Dudek, Gregory},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Shkurti, Dudek{\_}2015{\_}Sampling topologically distinct paths via Hamiltonian dynamics.pdf:pdf},
number = {i},
title = {{Sampling topologically distinct paths via Hamiltonian dynamics}},
year = {2015}
}
@misc{Devroye,
author = {Devroye, Luc},
file = {:home/arnold/Documents/pdf-library/Library/Devroye{\_}Unknown{\_}Doc (4).Pdf.pdf:pdf},
title = {{Doc (4).Pdf}}
}
@misc{Devroye2016b,
author = {Devroye, Luc},
file = {:home/arnold/Documents/pdf-library/Library/Devroye{\_}2016{\_}Assignment 6 - Markov Chains.pdf:pdf},
title = {{Assignment 6 - Markov Chains}},
year = {2016}
}
@article{Snoek2012,
abstract = {超パラメータのベイズ最適化},
archivePrefix = {arXiv},
arxivId = {1206.2944},
author = {Snoek, Jasper and Larochelle, Hugo and Adams, Rp},
doi = {2012arXiv1206.2944S},
eprint = {1206.2944},
isbn = {9781627480031},
issn = {10495258},
journal = {Nips},
pages = {1--9},
pmid = {9377276},
title = {{Practical Bayesian Optimization of Machine Learning Algorithms.}},
url = {https://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf},
year = {2012}
}
@article{Andrychowicz2016,
abstract = {The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.},
archivePrefix = {arXiv},
arxivId = {1606.04474},
author = {Andrychowicz, Marcin and Denil, Misha and Gomez, Sergio and Hoffman, Matthew W. and Pfau, David and Schaul, Tom and de Freitas, Nando},
doi = {10.1007/s10115-008-0151-5},
eprint = {1606.04474},
isbn = {1011500801515},
journal = {Nips},
pages = {1--16},
title = {{Learning To Learn by Gradient Descent by Gradient Descent}},
url = {http://arxiv.org/abs/1606.04474},
year = {2016}
}
@article{Wen2009,
abstract = {Current isometric embedding approaches are topologically unstable when confronted with noisy data, as where the neighborhood is critically distorted. Based on the cognitive law, a relative transformation (RT), which improves the distinction between data points and diminishes the impact of noise on isometric embedding approaches, is proposed. As the constructed space from large scale data by RT is high-dimensional, local relative transformation (LRT) is further proposed. Subsequently, a new isometric embedding approach is developed by using LRT to construct a better neighborhood graph with fewer short-circuit edges, while the embedding is still performed in the original space. This approach has significantly increased performance and reduced running time. The proposed approach was validated by experiments on challenging benchmark data sets. ?? 2008 Elsevier B.V. All rights reserved.},
author = {Wen, Guihua and Jiang, Lijun and Wen, Jun},
doi = {10.1016/j.patrec.2008.09.005},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Wen, Jiang, Wen{\_}2009{\_}Local relative transformation with application to isometric embedding.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Cognitive law,Isometric embedding,Local relative transformation,Manifold learning,Neighborhood graph,Relative transformation},
number = {3},
pages = {203--211},
publisher = {Elsevier B.V.},
title = {{Local relative transformation with application to isometric embedding}},
url = {http://dx.doi.org/10.1016/j.patrec.2008.09.005},
volume = {30},
year = {2009}
}
@misc{Jackie2016,
author = {Cheung, Jackie},
file = {:home/arnold/Documents/pdf-library/Library/Cheung{\_}2016{\_}Natural Language Generation.pdf:pdf},
title = {{Natural Language Generation}},
year = {2016}
}
@article{Chomsky1959,
abstract = {A great many linguists and philosophers concerned with language have expressed the hope that their studies might ultimately be embedded in a framework provided by behaviorist psychology, and that refractory areas of investigation, particularly those in which meaning is involved, will in this way be opened up to fruitful exploration. Since this volume is the first large-scale attempt to incorporate the major aspects of linguistic behavior within a behaviorist framework, it merits and will undoubtedly receive careful attention. Skinner is noted for his contributions to the study of animal behavior. The book under review is the product of study of linguistic behavior extending over more than twenty years. Earlier versions of it have been fairly widely circulated, and there are quite a few references in the psychological literature to its major ideas.},
author = {Chomsky, Noam},
doi = {citeulike-article-id:263746},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Chomsky{\_}1959{\_}A Review of B.F. Skinner's Verbal Behavior.pdf:pdf},
isbn = {0631232540},
issn = {00978507},
journal = {Readings in the Psychology of Language PrenticeHall},
pages = {142--143},
title = {{A Review of B.F. Skinner's Verbal Behavior}},
url = {http://www.chomsky.info/articles/1967----.htm},
volume = {35},
year = {1959}
}
@article{Iyyer2015,
author = {Iyyer, M and Manjunatha, V},
file = {:home/arnold/Documents/pdf-library/Library/Iyyer, Manjunatha{\_}2015{\_}Deep unordered composition rivals syntactic methods for text classification.pdf:pdf},
journal = {Proceedings of the 53rd  {\ldots}},
title = {{Deep unordered composition rivals syntactic methods for text classification}},
url = {https://www.cs.colorado.edu/{~}jbg/docs/2015{\_}acl{\_}dan.pdf},
year = {2015}
}
@article{Kumar2015,
abstract = {Most tasks in natural language processing can be cast into question answering (QA) problems over language input. We introduce the dynamic memory network (DMN), a neural network architecture which processes input sequences and questions, forms episodic memories, and generates relevant answers. Questions trigger an iterative attention process which allows the model to condition its attention on the inputs and the result of previous iterations. These results are then reasoned over in a hierarchical recurrent sequence model to generate answers. The DMN can be trained end-to-end and obtains state-of-the-art results on several types of tasks and datasets: question answering (Facebook's bAbI dataset), text classification for sentiment analysis (Stanford Sentiment Treebank) and sequence modeling for part-of-speech tagging (WSJ-PTB). The training for these different tasks relies exclusively on trained word vector representations and input-question-answer triplets.},
archivePrefix = {arXiv},
arxivId = {1506.07285},
author = {Kumar, Ankit and Irsoy, Ozan and Ondruska, Peter and Iyyer, Mohit and Bradbury, James and Gulrajani, Ishaan and Zhong, Victor and Paulus, Romain and Socher, Richard},
eprint = {1506.07285},
file = {:home/arnold/Documents/pdf-library/Library/Kumar et al.{\_}2015{\_}Ask Me Anything Dynamic Memory Networks for Natural Language Processing.pdf:pdf},
journal = {Nips},
title = {{Ask Me Anything: Dynamic Memory Networks for Natural Language Processing}},
url = {http://arxiv.org/abs/1506.07285{\%}5Cnhttp://www.arxiv.org/pdf/1506.07285.pdf},
year = {2015}
}
@article{Kingma2014a,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik and Ba, Jimmy},
eprint = {1412.6980},
file = {:home/arnold/Documents/pdf-library/Library/Kingma, Ba{\_}2014{\_}Adam A method for stochastic optimization.pdf:pdf},
journal = {arXiv:1412.6980 [cs.LG]},
pages = {1--15},
title = {{Adam: A method for stochastic optimization}},
url = {http://arxiv.org/abs/1412.6980},
year = {2014}
}
@article{Gavin2014,
author = {Gavin, Henri P and Scruggs, Jeffrey T},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Gavin, Scruggs{\_}2014{\_}Constrained Optimization Using Lagrange Multipliers.pdf:pdf},
pages = {1--9},
title = {{Constrained Optimization Using Lagrange Multipliers}},
year = {2014}
}
@article{Langkilde1998,
abstract = {We describe novel aspects of a new natural language generator called Nitrogen. This generator has a highly flexible input representation that allows a spectrum of input from syntactic to semantic depth, and shifts the burden of many linguistic decisions to the statistical post-processor. The generation algorithm is compositional, making it efficient, yet it also handels non-compositional aspects of language. Nitrogen's design maks it robust and scalable, operating with lexicons and knowledge bases of one hundred thousand entities.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Langkilde, I and Knight, K},
doi = {10.3115/980845.980963},
eprint = {arXiv:1011.1669v3},
file = {:home/arnold/Documents/pdf-library/Library/Langkilde, Knight{\_}1998{\_}Generation that exploits corpus-based statistical knowledge.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics-Volume 1},
pages = {704--710},
pmid = {25246403},
title = {{Generation that exploits corpus-based statistical knowledge}},
url = {http://dl.acm.org/citation.cfm?id=980963},
volume = {1},
year = {1998}
}
@incollection{Gallager2012,
author = {Gallager, R G},
file = {:home/arnold/Documents/pdf-library/Library/Gallager{\_}2012{\_}Countable-State Markov Cains.pdf:pdf},
pages = {228--262},
title = {{Countable-State Markov Cains}},
year = {2012}
}
@article{Barzilay2008,
abstract = {This article proposes a novel framework for representing and measuring local coherence. Central to this approach is the entity-grid representation of discourse, which captures patterns of entity distribution in a text. The algorithm introduced in the article automatically abstracts a text into a set of entity transition sequences and records distributional, syntactic, and referential information about discourse entities. We re-conceptualize coherence assessment as a learning task and show that our entity-based representation is well-suited for ranking-based generation and text classification tasks. Using the proposed representation, we achieve good performance on text ordering, summary coherence evaluation, and readability assessment.},
author = {Barzilay, Regina and Lapata, Mirella},
doi = {10.1162/coli.2008.34.1.1},
isbn = {1932432515},
issn = {0891-2017},
journal = {Computational Linguistics},
number = {1},
pages = {1--34},
title = {{Modeling Local Coherence: An Entity-Based Approach}},
volume = {34},
year = {2008}
}
@article{Arel2010,
abstract = {Reinforcement Learning (RL) is a type of Machine Learning algorithms and it enables the agent to determine the ideal behavior from its own experience. From robot control to autonomous navigation, Reinforcement Learning algorithms have been applied to address increasing difficult problems. In recent studies, a number of papers have shown great success of RL in the field of production control, finance, scheduling, communication and auto vehicle control. However, in most cases, the performance of these algorithms heavily rely on the quality of the handcrafted features. This drawback limits the application scope of traditional Reinforcement Learning algorithms, since some problems have high dimensional state space and are difficult to hand-engineered. For instance, it is a long standing challenge for traditional RL algorithms to process high dimensional sensory data like vision and voice. In 2006, the Deep Learning (DL) algorithms were established and have been further developed in recent years. The Convolutional Neural Network is one of the Deep Learning models that could extract high dimensional features direct from the raw pixels, and have been successfully applied in computer vision. It is nature for us to think whether the traditional Reinforcement Learning algorithms could benefit from it.},
archivePrefix = {arXiv},
arxivId = {1507.04296},
author = {Arel, Itamar},
eprint = {1507.04296},
isbn = {9781509006205},
journal = {Ppt},
number = {August},
pages = {1--3},
title = {{Deep reinforcement learning}},
year = {2010}
}
@article{Waggoner2014,
abstract = {The classic problems of testing uniformity of and learning a discrete distribution, given access to independent samples from it, are examined under general {\$}\backslashell{\_}p{\$} metrics. The intuitions and results often contrast with the classic {\$}\backslashell{\_}1{\$} case. For {\$}p {\textgreater} 1{\$}, we can learn and test with a number of samples that is independent of the support size of the distribution: With an {\$}\backslashell{\_}p{\$} tolerance {\$}\backslashepsilon{\$}, {\$}O(\backslashmax\backslash{\{} \backslashsqrt{\{}1/\backslashepsilon{\^{}}q{\}}, 1/\backslashepsilon{\^{}}2 \backslash{\}}){\$} samples suffice for testing uniformity and {\$}O(\backslashmax\backslash{\{} 1/\backslashepsilon{\^{}}q, 1/\backslashepsilon{\^{}}2\backslash{\}}){\$} samples suffice for learning, where {\$}q=p/(p-1){\$} is the conjugate of {\$}p{\$}. As this parallels the intuition that {\$}O(\backslashsqrt{\{}n{\}}){\$} and {\$}O(n){\$} samples suffice for the {\$}\backslashell{\_}1{\$} case, it seems that {\$}1/\backslashepsilon{\^{}}q{\$} acts as an upper bound on the "apparent" support size. For some {\$}\backslashell{\_}p{\$} metrics, uniformity testing becomes easier over larger supports: a 6-sided die requires fewer trials to test for fairness than a 2-sided coin, and a card-shuffler requires fewer trials than the die. In fact, this inverse dependence on support size holds if and only if {\$}p {\textgreater} \backslashfrac{\{}4{\}}{\{}3{\}}{\$}. The uniformity testing algorithm simply thresholds the number of "collisions" or "coincidences" and has an optimal sample complexity up to constant factors for all {\$}1 \backslashleq p \backslashleq 2{\$}. Another algorithm gives order-optimal sample complexity for {\$}\backslashell{\_}{\{}\backslashinfty{\}}{\$} uniformity testing. Meanwhile, the most natural learning algorithm is shown to have order-optimal sample complexity for all {\$}\backslashell{\_}p{\$} metrics. The author thanks Cl$\backslash$'{\{}e{\}}ment Canonne for discussions and contributions to this work.},
archivePrefix = {arXiv},
arxivId = {1412.2314},
author = {Waggoner, Bo},
doi = {10.1145/2688073.2688095},
eprint = {1412.2314},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Waggoner{\_}2014{\_}Testing and Learning of Discrete Distributions.pdf:pdf},
isbn = {9781450333337},
keywords = {discrete distri-,learning,property testing,uniformity testing},
pages = {24},
title = {{Testing and Learning of Discrete Distributions}},
url = {http://arxiv.org/abs/1412.2314},
year = {2014}
}
@article{Kac1966,
author = {Kac, Mark},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Kac{\_}1966{\_}Can one hear the shape of a drum.pdf:pdf},
isbn = {1755593562},
issn = {1364-5021},
journal = {The american mathematical monthly},
title = {{Can one hear the shape of a drum?}},
year = {1966}
}
@article{Manderson2015,
author = {Manderson, Travis and Li, Jimmy and Cort, David and Dudek, Natasha and Meger, David},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Manderson et al.{\_}2015{\_}Towards Autonomous Robotic Coral Reef Health Assessment.pdf:pdf},
journal = {Field and Service Robotics},
title = {{Towards Autonomous Robotic Coral Reef Health Assessment}},
year = {2015}
}
@misc{Cheung2016b,
author = {Cheung, Jackie},
file = {:home/arnold/Documents/pdf-library/Library/Cheung{\_}2016{\_}Coreference Resolution - Lecture 17.pdf:pdf},
title = {{Coreference Resolution - Lecture 17}},
year = {2016}
}
@misc{Cheung2016,
author = {Cheung, Jackie},
file = {:home/arnold/Documents/pdf-library/Library/Cheung{\_}2016{\_}Compositional Semantics Quantification and Underspecification - Lecture 16.pdf:pdf},
title = {{Compositional Semantics: Quantification and Underspecification - Lecture 16}},
year = {2016}
}
@misc{Cheung2016c,
author = {Cheung, Jackie},
file = {:home/arnold/Documents/pdf-library/Library/Cheung{\_}2016{\_}Compositional Semantics Montagovian Semantics and Lambda Calculus - Lecture 15.pdf:pdf},
title = {{Compositional Semantics: Montagovian Semantics and Lambda Calculus - Lecture 15}},
year = {2016}
}
@article{Levine2013,
abstract = {Direct policy search can e ectively scale to high-dimensional systems, but complex policies with hundreds of parameters often present a challenge for such methods, requir- ing numerous samples and often falling into poor local optima. We present a guided pol- icy search algorithm that uses trajectory op- timization to direct policy learning and avoid poor local optima. We show how di erential dynamic programming can be used to gener- ate suitable guiding samples, and describe a regularized importance sampled policy opti- mization that incorporates these samples into the policy search. We evaluate the method by learning neural network controllers for planar swimming, hopping, and walking, as well as simulated 3D humanoid running.},
author = {Levine, Sergey and Koltun, Vladlen},
journal = {Proceedings of the 30th International Conference on Machine Learning},
pages = {1--9},
title = {{Guided Policy Search}},
url = {http://jmlr.org/proceedings/papers/v28/levine13.html},
volume = {28},
year = {2013}
}
@article{Rao,
author = {Rao, Malvika and Dudek, Gregory and Whitesides, Sue},
journal = {Workshop on the Algorithmic Foundations of Robotics},
keywords = {icle},
title = {{Randomized Algorithms for Minimum Distance Localization}}
}
@article{Bahdanau2016,
abstract = {We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a $\backslash$textit{\{}critic{\}} network that is trained to predict the value of an output token, given the policy of an $\backslash$textit{\{}actor{\}} network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling.},
archivePrefix = {arXiv},
arxivId = {1607.07086},
author = {Bahdanau, Dzmitry and Brakel, Philemon and Xu, Kelvin and Goyal, Anirudh and Lowe, Ryan and Pineau, Joelle and Courville, Aaron and Bengio, Yoshua},
eprint = {1607.07086},
journal = {arXiv:1607.07086v1 [cs.LG]},
number = {2015},
pages = {1--9},
title = {{An Actor-Critic Algorithm for Sequence Prediction}},
url = {http://arxiv.org/abs/1607.07086},
year = {2016}
}
@article{Rao2005,
author = {Rao, Malvika and Dudek, Gregory and Whitesides, Sue},
doi = {10.1109/ROBOT.2005.1570478},
isbn = {078038914X},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {2438--2445},
title = {{Minimum distance localization for a robot with limited visibility}},
volume = {2005},
year = {2005}
}
@article{Rao2007,
abstract = {The problem of minimum distance localization in environments that may contain self-similarities is addressed. Amobile robot is placed at an unknown location inside a 2D self-similar polygonal environment P. The robot has a map of P and can compute visibility data through sensing. However, the self-similarities in the environment mean that the same visibility data may correspond to several different locations. The goal, therefore, is to determine the robot's true initial location while minimizing the distance traveled by the robot. Two randomized approximation algorithms are presented that solve minimum distance localization. The performance of the proposed algorithms is evalu- ated empirically.},
author = {Rao, M. and Dudek, G. and Whitesides, S.},
doi = {10.1177/0278364907081234},
isbn = {9783540257288},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
keywords = {ambigu-,ity,localization,optimal path,randomized algorithms,sensing},
number = {9},
pages = {917--933},
title = {{Randomized Algorithms for Minimum Distance Localization}},
url = {http://ijr.sagepub.com/cgi/doi/10.1177/0278364907081234},
volume = {26},
year = {2007}
}
@article{Beijbom2015,
author = {Beijbom, Oscar},
doi = {10.1300/J122v22n03_06},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Beijbom{\_}2015{\_}Automated Annotation of Coral Reef Survey Images.pdf:pdf},
isbn = {9789460912993},
issn = {0194-262X},
journal = {UC San Diego Thesis and Dissertations},
pages = {b8984742},
title = {{Automated Annotation of Coral Reef Survey Images}},
url = {http://escholarship.org/uc/item/0rd0r3wd{\%}0ALocal},
year = {2015}
}
@incollection{Devroye2016c,
author = {Devroye, Luc},
booktitle = {Probabilistic Analysis of AlgorithmsProbabilistic Analysis of Algorithms},
file = {:home/arnold/Documents/pdf-library/Library/Devroye{\_}2016{\_}Markov Chains.pdf:pdf},
title = {{Markov Chains}},
year = {2016}
}
@article{Fabius2015a,
abstract = {In this paper we propose a model that combines the strengths of RNNs and SGVB: the Variational Recurrent Auto-Encoder (VRAE). Such a model can be used for efficient, large scale unsupervised learning on time series data, mapping the time series data to a latent vector representation. The model is generative, such that data can be generated from samples of the latent space. An important contribution of this work is that the model can make use of unlabeled data in order to facilitate supervised training of RNNs by initialising the weights and network state.},
archivePrefix = {arXiv},
arxivId = {1412.6581},
author = {Fabius, Otto and van Amersfoort, Joost R.},
eprint = {1412.6581},
file = {:home/arnold/Documents/pdf-library/Library/Fabius, van Amersfoort{\_}2015{\_}Variational Recurrent Auto-Encoders.pdf:pdf},
journal = {Iclr},
number = {2013},
pages = {1--5},
title = {{Variational Recurrent Auto-Encoders}},
url = {http://arxiv.org/abs/1412.6581},
year = {2015}
}
@book{Penrose2003,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Penrose, Mathew},
booktitle = {Oxford Studies in Probability},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Penrose{\_}2003{\_}Random Geometric Graphs.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {icle},
pmid = {25246403},
title = {{Random Geometric Graphs}},
volume = {1},
year = {2003}
}
@article{Chambers2008,
abstract = {Essential to the selection of the next target for gaze or attention is the ability to compare the strengths of multiple competing stimuli (bottom-up information) and to signal the strongest one. Although the optic tectum (OT) has been causally implicated in stimulus selection, how it computes the strongest stimulus is unknown. Here, we demonstrate that OT neurons in the barn owl systematically encode the relative strengths of simultaneously occurring stimuli independently of sensory modality. Moreover, special "switch-like" responses of a subset of neurons abruptly increase when the stimulus inside their receptive field becomes the strongest one. Such responses are not predicted by responses to single stimuli and, indeed, are eliminated in the absence of competitive interactions. We demonstrate that this sensory transformation substantially boosts the representation of the strongest stimulus by creating a binary discrimination signal, thereby setting the stage for potential winner-take-all target selection for gaze and attention.},
author = {Chambers, Nathanael and Jurafsky, Dan},
doi = {10.1.1.143.1555},
file = {:home/arnold/Documents/pdf-library/Library/Chambers, Jurafsky{\_}2008{\_}Unsupervised learning of narrative event chains.pdf:pdf},
isbn = {9781932432046},
journal = {Proceedings of the Association of Computational Linguistics},
number = {14},
pages = {789--797},
title = {{Unsupervised learning of narrative event chains}},
url = {http://acl.eldoc.ub.rug.nl/mirror/P/P08/P08-1090.pdf},
volume = {31},
year = {2008}
}
@book{Penrose2003a,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Penrose, Mathew},
booktitle = {Oxford Studies in Probability},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {icle},
pmid = {25246403},
title = {{Random Geometric Graphs}},
volume = {1},
year = {2003}
}
@misc{Devroye2016d,
author = {Devroye, Luc},
file = {:home/arnold/Documents/pdf-library/Library/Devroye{\_}2016{\_}Assignment 5 - Random Geometric Graphs.pdf:pdf},
title = {{Assignment 5 - Random Geometric Graphs}},
year = {2016}
}
@incollection{Devroye2016e,
author = {Devroye, Luc},
booktitle = {Probabilistic Analysis of Algorithms},
file = {:home/arnold/Documents/pdf-library/Library/Devroye{\_}2016{\_}Network Routing.pdf:pdf},
title = {{Network Routing}},
year = {2016}
}
@book{Motwani1995,
abstract = {For many applications, a randomized algorithm is either the simplest or the fastest algorithm available, and sometimes both. This book introduces the basic concepts in the design and analysis of randomized algorithms. The first part of the text presents basic tools such as probability theory and probabilistic analysis that are frequently used in algorithmic applications. Algorithmic examples are also given to illustrate the use of each tool in a concrete setting. In the second part of the book, each chapter focuses on an important area to which randomized algorithms can be applied, providing a comprehensive and representative selection of the algorithms that might be used in each of these areas. Although written primarily as a text for advanced undergraduates and graduate students, this book should also prove invaluable as a reference for professionals and researchers.},
author = {Motwani, Rajeev and Raghavan, Prabhakar},
doi = {10.1017/CBO9780511814075},
file = {:home/arnold/Documents/pdf-library/Library/Motwani, Raghavan{\_}1995{\_}Randomized Algorithms.pdf:pdf},
isbn = {{\textless}null{\textgreater}},
pages = {476},
title = {{Randomized Algorithms}},
url = {http://books.google.com/books?id=QKVY4mDivBEC{\&}printsec=frontcover{\&}dq=intitle:Randomized+Algorithms{\&}hl={\&}cd=1{\&}source=gbs{\_}api{\%}5Cnpapers2://publication/uuid/6D02ACEE-0CF3-4AF1-8C5D-860A25939B63},
year = {1995}
}
@misc{Kot2015,
author = {Kot, Ales and Foss, Langdon and Bellaire, Jordie},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Kot, Foss, Bellaire{\_}2015{\_}The Surface.pdf:pdf},
title = {{The Surface}},
year = {2015}
}
@article{Besag1986,
abstract = {A continuous two-dimensional region is partitioned into a fine rectangular array of sites or "pixels", each pixel having a particular "colour" belonging to a prescribed finite set. The true colouring of the region is unknown but, associated with each pixel, there is a possibly multivariate record which conveys imperfect information about its colour according to a known statistical model. The aim is to reconstruct the true scene, with the additional knowledge that pixels close together tend to have the same or similar colours. In this paper, it is assumed that the local characteristics of the true scene can be represented by a non-degenerate Markov random field. Such information can be combined with the records by Bayes' theorem and the true scene can be estimated according to standard criteria. However, the computational burden is enormous and the reconstruction may reflect undesirable large-scale properties of the random field. Thus, a simple, iterative method of reconstruction is proposed, which does not depend on these large-scale characteristics. The method is illustrated by computer simulations in which the original scene is not directly related to the assumed random field. Some complications, including parameter estimation, are discussed. Potential applications are mentioned briefly.},
author = {Besag, Julian},
doi = {10.1080/02664769300000059},
file = {:home/arnold/Documents/pdf-library/Library/Besag{\_}1986{\_}On the statistical analysis of dirty pictures.pdf:pdf},
isbn = {ISSN{\~{}}{\~{}}0035-9246},
issn = {00359246},
journal = {Journal of the Royal Statistical Society. Series B ( {\ldots}},
number = {3},
pages = {259--302},
title = {{On the statistical analysis of dirty pictures}},
url = {http://www.jstor.org/stable/10.2307/2345426},
volume = {48},
year = {1986}
}
@article{Kot2014,
author = {Kot, Ales},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Kot{\_}2014{\_}An Emergency.pdf:pdf},
journal = {Zero},
title = {{An Emergency}},
year = {2014}
}
@article{Mikolov2011,
abstract = {Better results by using Backpropagation throught time and better speed by using classes.},
author = {Mikolov, Tomas and Kombrink, S},
doi = {10.1109/ICASSP.2011.5947611},
file = {:home/arnold/Documents/pdf-library/Library/Mikolov, Kombrink{\_}2011{\_}Extensions of recurrent neural network language model.pdf:pdf},
isbn = {9781457705397},
issn = {1520-6149},
journal = {Icassp},
pages = {5528--5531},
title = {{Extensions of recurrent neural network language model}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5947611},
year = {2011}
}
@misc{Kot2015a,
author = {Kot, Ales},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Kot{\_}2015{\_}Wolf.pdf:pdf},
title = {{Wolf}},
year = {2015}
}
@article{Kot2015b,
author = {Kot, Ales},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Kot{\_}2015{\_}Material - Volume 1.pdf:pdf},
journal = {Material},
title = {{Material - Volume 1}},
year = {2015}
}
@incollection{Devroye2016f,
author = {Devroye, Luc},
booktitle = {Probabilistic Analysis of Algorithms},
file = {:home/arnold/Documents/pdf-library/Library/Devroye{\_}2016{\_}Random Geometric Graphs.pdf:pdf},
title = {{Random Geometric Graphs}},
year = {2016}
}
@article{Lau2015,
author = {Lau, Jey Han and Clark, Alexander and Lappin, Shalom},
doi = {10.3115/v1/P15-1156},
file = {:home/arnold/Documents/pdf-library/Library/Lau, Clark, Lappin{\_}2015{\_}Unsupervised Prediction of Acceptability Judgements.pdf:pdf},
isbn = {9781941643723},
journal = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing},
pages = {1618--1628},
title = {{Unsupervised Prediction of Acceptability Judgements}},
url = {http://www.aclweb.org/anthology/P15-1156},
volume = {1},
year = {2015}
}
@misc{Devroye2016g,
author = {Devroye, Luc},
file = {:home/arnold/Documents/pdf-library/Library/Devroye{\_}2016{\_}Assignment 4 - Random Graphs.pdf:pdf},
title = {{Assignment 4 - Random Graphs}},
year = {2016}
}
@article{Chen2016a,
abstract = {This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods.},
archivePrefix = {arXiv},
arxivId = {1606.03657},
author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
eprint = {1606.03657},
file = {:home/arnold/Documents/pdf-library/Library/Chen et al.{\_}2016{\_}InfoGAN Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets.pdf:pdf},
journal = {arXiv},
title = {{InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets}},
url = {http://arxiv.org/abs/1606.03657},
year = {2016}
}
@misc{Cheung2016,
author = {Cheung, Jackie},
file = {:home/arnold/Documents/pdf-library/Library/Cheung{\_}2016{\_}More Lexical Semantics - Lecture 14.pdf:pdf},
title = {{More Lexical Semantics - Lecture 14}},
year = {2016}
}
@article{Rao2007a,
author = {Rao, Malvika and Dudek, Gregory and Whitesides, Sue},
doi = {10.1177/0278364907081234},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Rao, Dudek, Whitesides{\_}2007{\_}Randomized Algorithms for Minimum Distance Localization.pdf:pdf},
journal = {The International Journal of Robotics Research},
keywords = {ambiguity,localization,optimal path,randomized algorithms,sensing},
title = {{Randomized Algorithms for Minimum Distance Localization}},
year = {2007}
}
@article{Rao2004,
author = {Rao, Malvika and Dudek, Gregory and Whitesides, Sue},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Rao, Dudek, Whitesides{\_}2004{\_}Randomized Algorithms for Minimum Distance Localization.pdf:pdf},
journal = {International Workshop on the Algorithmic Foundations of Robotics},
number = {3},
pages = {1--16},
title = {{Randomized Algorithms for Minimum Distance Localization}},
year = {2004}
}
@book{Neil2012,
author = {Neil, Drew},
file = {:home/arnold/Documents/pdf-library/Library/Neil{\_}2012{\_}Pracical Vim.pdf:pdf},
isbn = {9781934356982},
pages = {346},
title = {{Pracical Vim}},
year = {2012}
}
@misc{Cheung2016b,
author = {Cheung, Jackie},
file = {:home/arnold/Documents/pdf-library/Library/Cheung{\_}2016{\_}Topics in Parsing Context and Markovization Dependency Parsing - Lecture 12.pdf:pdf},
title = {{Topics in Parsing: Context and Markovization; Dependency Parsing - Lecture 12}},
year = {2016}
}
@misc{Cheung2016a,
author = {Cheung, Jackie},
file = {:home/arnold/Documents/pdf-library/Library/Cheung{\_}2016{\_}Lexical Semantics - Lecture 13.pdf:pdf},
title = {{Lexical Semantics - Lecture 13}},
year = {2016}
}
@article{The,
author = {The, O N and Of, Evolution and Graphs, Random},
file = {:home/arnold/Documents/MendeleyDesktop/Library/The, Of, Graphs{\_}Unknown{\_}Introduction Our aim is to study the probable structure of a random graph.pdf:pdf},
title = {{Introduction Our aim is to study the probable structure of a random graph}}
}
@article{Simo-Serra2015,
abstract = {Deep learning has revolutionalized image-level tasks such as classification, but patch-level tasks, such as correspondence, still rely on hand-crafted features, e.g. SIFT. In this paper we use Convolutional Neural Net-works (CNNs) to learn discriminant patch representations and in particular train a Siamese network with pairs of (non-)corresponding patches. We deal with the large num-ber of potential pairs with the combination of a stochastic sampling of the training set and an aggressive mining strat-egy biased towards patches that are hard to classify. By using the L 2 distance during both training and test-ing we develop 128-D descriptors whose euclidean dis-tances reflect patch similarity, and which can be used as a drop-in replacement for any task involving SIFT. We demon-strate consistent performance gains over the state of the art, and generalize well against scaling and rotation, perspec-tive transformation, non-rigid deformation, and illumina-tion changes. Our descriptors are efficient to compute and amenable to modern GPUs, and are publicly available.},
author = {Simo-Serra, Edgar and Trulls, Eduard and Ferraz, Luis and Kokkinos, Iasonas and Fua, Pascal and Moreno-Noguer, Francesc},
doi = {10.1109/ICCV.2015.22},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Simo-Serra et al.{\_}2015{\_}Discriminative Learning of Deep Convolutional Feature Point Descriptors.pdf:pdf},
isbn = {978-1-4673-8391-2},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {118--126},
title = {{Discriminative Learning of Deep Convolutional Feature Point Descriptors}},
url = {http://www.cv-foundation.org/openaccess/content{\_}iccv{\_}2015/html/Simo-Serra{\_}Discriminative{\_}Learning{\_}of{\_}ICCV{\_}2015{\_}paper.html},
year = {2015}
}
@misc{Cheung2016c,
author = {Cheung, Jackie},
file = {:home/arnold/Documents/pdf-library/Library/Cheung{\_}2016{\_}The CKY Parsing Algorithm and PCFGs - Lecture 11.pdf:pdf},
title = {{The CKY Parsing Algorithm and PCFGs - Lecture 11}},
year = {2016}
}
@article{Levine2016,
abstract = {Policy search methods can allow robots to learn control policies for a wide range of tasks, but practical applications of policy search often require hand-engineered components for perception, state estimation, and low-level control. In this paper, we aim to answer the following question: does training the perception and control systems jointly end-to-end provide better performance than training each component separately? To this end, we develop a method that can be used to learn policies that map raw image observations directly to torques at the robot's motors. The policies are represented by deep convolutional neural networks (CNNs) with 92,000 parameters, and are trained using a guided policy search method, which transforms policy search into supervised learning, with supervision provided by a simple trajectory-centric reinforcement learning method. We evaluate our method on a range of real-world manipulation tasks that require close coordination between vision and control, such as screwing a cap onto a bottle, and present simulated comparisons to a range of prior policy search methods.},
archivePrefix = {arXiv},
arxivId = {1504.00702},
author = {Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1504.00702},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Levine et al.{\_}2016{\_}End-to-End Training of Deep Visuomotor Policies.pdf:pdf},
isbn = {9781479969227},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Neural Networks,Optimal Control,Reinforcement Learning,Vision},
pages = {1--40},
pmid = {15003161},
title = {{End-to-End Training of Deep Visuomotor Policies}},
volume = {17},
year = {2016}
}
@article{Bowman2016,
abstract = {The standard unsupervised recurrent neural network language model (RNNLM) generates sentences one word at a time and does not work from an explicit global distributed sentence representation. In this work, we present an RNN-based variational autoencoder language model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate strong performance in the imputation of missing tokens, and explore many interesting properties of the latent sentence space.},
archivePrefix = {arXiv},
arxivId = {1511.06349},
author = {Bowman, Samuel R. and Vilnis, Luke and Vinyals, Oriol and Dai, Andrew M. and Jozefowicz, Rafal and Bengio, Samy},
eprint = {1511.06349},
file = {:home/arnold/Documents/pdf-library/Library/Bowman et al.{\_}2016{\_}Generating Sentences from a Continuous Space.pdf:pdf},
journal = {Iclr},
pages = {1--13},
title = {{Generating Sentences from a Continuous Space}},
url = {http://arxiv.org/abs/1511.06349},
year = {2016}
}
@article{Walter2013,
abstract = {—This paper proposes an algorithm that enables robots to efficiently learn human-centric models of their envi-ronment from natural language descriptions. Typical semantic mapping approaches augment metric maps with higher-level properties of the robot's surroundings (e.g., place type, object locations), but do not use this information to improve the metric map. The novelty of our algorithm lies in fusing high-level knowledge, conveyed by speech, with metric information from the robot's low-level sensor streams. Our method jointly estimates a hybrid metric, topological, and semantic representation of the environment. This semantic graph provides a common framework in which we integrate concepts from natural language descrip-tions (e.g., labels and spatial relations) with metric observations from low-level sensors. Our algorithm efficiently maintains a factored distribution over semantic graphs based upon the stream of natural language and low-level sensor information. We evaluate the algorithm's performance and demonstrate that the incorporation of information from natural language increases the metric, topological and semantic accuracy of the recovered environment model.},
author = {Walter, Matthew R and Hemachandra, Sachithra and Homberg, Bianca and Tellex, Stefanie and Teller, Seth},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Walter et al.{\_}2013{\_}Learning Semantic Maps from Natural Language Descriptions.pdf:pdf},
journal = {Robotics Science and Systems},
pages = {1--8},
title = {{Learning Semantic Maps from Natural Language Descriptions}},
url = {papers3://publication/uuid/77B21DAA-4F22-4E45-96A5-F134DA3457B7},
year = {2013}
}
@article{Oswald2014,
abstract = {For several applications, robots and other computer systems must provide route descriptions to humans. These descriptions should be natural and intuitive for the human users. In this paper, we present an algorithm that learns how to provide good route descriptions from a corpus of human-written directions. Using inverse reinforcement learning, our algorithm learns how to select the information for the description depending on the context of the route segment. The algorithm then uses the learned policy to generate directions that imitate the style of the descriptions provided by humans, thus taking into account personal as well as cultural preferences and special requirements of the particular user group providing the learning demonstrations. We evaluate our approach in a user study and show that the directions generated by our policy sound similar to human-given directions and substantially more natural than directions provided by commercial web services.},
author = {Oswald, Stefan and Kretzschmar, Henrik and Burgard, Wolfram and Stachniss, Cyrill},
doi = {10.1109/ICRA.2014.6907334},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Oswald et al.{\_}2014{\_}Learning to give route directions from human demonstrations.pdf:pdf},
isbn = {978-1-4799-3685-4},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {3303--3308},
title = {{Learning to give route directions from human demonstrations}},
year = {2014}
}
@article{Barnard2003,
abstract = {We present a new approach for modeling multi-modal data sets, focusing$\backslash$non the specific case of segmented images with associated text. Learning$\backslash$nthe joint distribution of image regions and words has many applications.$\backslash$nWe consider in detail predicting words associated with whole images$\backslash$n(auto-annotation) and corresponding to particular image regions (region$\backslash$nnaming). Auto-annotation might help organize and access large collections$\backslash$nof images. Region naming is a model of object recognition as a process$\backslash$nof translating image regions to words, much as one might translate$\backslash$nfrom one language to another. Learning the relationships between$\backslash$nimage regions and semantic correlates (words) is an interesting example$\backslash$nof multi-modal data mining, particularly because it is typically$\backslash$nhard to apply data mining techniques to collections of images. We$\backslash$ndevelop a number of models for the joint distribution of image regions$\backslash$nand words, including several which explicitly learn the correspondence$\backslash$nbetween regions and words. We study multi-modal and correspondence$\backslash$nextensions to Hofmann's hierarchical clustering/aspect model, a translation$\backslash$nmodel adapted from statistical machine translation (Brown et al.),$\backslash$nand a multi-modal extension to mixture of latent Dirichlet allocation$\backslash$n(MoM-LDA). All models are assessed using a large collection of annotated$\backslash$nimages of real scenes. We study in depth the difficult problem of$\backslash$nmeasuring performance. For the annotation task, we look at prediction$\backslash$nperformance on held out data. We present three alternative measures,$\backslash$noriented toward different types of task. Measuring the performance$\backslash$nof correspondence methods is harder, because one must determine whether$\backslash$na word has been placed on the right region of an image. We can use$\backslash$nannotation performance as a proxy measure, but accurate measurement$\backslash$nrequires hand labeled data, and thus must occur on a smaller scale.$\backslash$nWe show results using both an annotation proxy, and manually labeled$\backslash$ndata.},
author = {Barnard, Kobus and Duygulu, Pinar and Forsyth, David and de Freitas, Nando and Blei, David M and Jordan, Michael I},
doi = {10.1162/153244303322533214},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Barnard et al.{\_}2003{\_}Matching Words and Pictures.pdf:pdf},
isbn = {1533-7928},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
pages = {1107--1135},
title = {{Matching Words and Pictures}},
volume = {3},
year = {2003}
}
@incollection{Devroye1998,
author = {Devroye, Luc},
booktitle = {Probabilistic methods for algorithmic discrete mathematics},
file = {:home/arnold/Documents/pdf-library/Library/Devroye{\_}1998{\_}Branching Processes and Their Applications in the Analysis of Tree Structures and Tree Algorithms.pdf:pdf},
title = {{Branching Processes and Their Applications in the Analysis of Tree Structures and Tree Algorithms}},
year = {1998}
}
@incollection{Devroye2016h,
author = {Devroye, Luc},
booktitle = {Probabilistic Analysis of Algorithms},
file = {:home/arnold/Documents/pdf-library/Library/Devroye{\_}2016{\_}Random Graphs.pdf:pdf},
title = {{Random Graphs}},
year = {2016}
}
@book{Habib1998,
author = {Habib, Michel and McDiarmid, Colin and Ramirez-Alfonsin, Jorge and Reed, Bruce},
file = {:home/arnold/Documents/pdf-library/Library/Habib et al.{\_}1998{\_}Probabilistic methods for algorithmic discrete mathematics.pdf:pdf},
title = {{Probabilistic methods for algorithmic discrete mathematics}},
year = {1998}
}
@misc{Cheung2016d,
author = {Cheung, Jackie},
file = {:home/arnold/Documents/pdf-library/Library/Cheung{\_}2016{\_}Final Project Instructions.pdf:pdf},
title = {{Final Project Instructions}},
year = {2016}
}
@misc{Cheung2016e,
author = {Cheung, Jackie},
file = {:home/arnold/Documents/pdf-library/Library/Cheung{\_}2016{\_}COMP599 ASSIGNMENT 2.pdf:pdf},
title = {{COMP599: ASSIGNMENT 2}},
year = {2016}
}
@misc{Cheung2016f,
author = {Cheung, Jackie},
file = {:home/arnold/Documents/pdf-library/Library/Cheung{\_}2016{\_}English Syntax and Context Free Grammars - Lecture 10.pdf:pdf},
title = {{English Syntax and Context Free Grammars - Lecture 10}},
year = {2016}
}
@incollection{Polland2015,
author = {Polland, David},
booktitle = {Empirical Processes},
file = {:home/arnold/Documents/pdf-library/Library/Polland{\_}2015{\_}A Few Good Inequalities.pdf:pdf},
number = {November},
title = {{A Few Good Inequalities}},
url = {http://www.stat.yale.edu/{~}pollard/Books/Mini/},
year = {2015}
}
@article{Sutton2010b,
abstract = {Often we wish to predict a large number of variables that depend on each other as well as on other observed variables. Structured prediction methods are essentially a combination of classification and graphical modeling, combining the ability of graphical models to compactly model multivariate data with the ability of classification methods to perform prediction using large sets of input features. This tutorial describes conditional random fields, a popular probabilistic method for structured prediction. CRFs have seen wide application in natural language processing, computer vision, and bioinformatics. We describe methods for inference and parameter estimation for CRFs, including practical issues for implementing large scale CRFs. We do not assume previous knowledge of graphical modeling, so this tutorial is intended to be useful to practitioners in a wide variety of fields.},
author = {Sutton, Charles and McCallum, Andrew},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Sutton, McCallum{\_}2010{\_}An Introduction to Conditional Random Fields(2).pdf:pdf},
journal = {arXiv.org},
number = {xx},
pages = {1--87},
title = {{An Introduction to Conditional Random Fields}},
url = {http://arxiv.org/abs/1011.4088{\%}5Cnfile:///Users/skeizer/Library/Application Support/Papers2/Articles/2010/Sutton/arXiv.org 2010 Sutton.pdf{\%}5Cnpapers2://publication/uuid/F49D61CE-23D4-4A15-9E66-37318E1871F7},
volume = {stat.ML},
year = {2010}
}
@misc{Cheung2016a,
author = {Cheung, Jackie},
file = {:home/arnold/Documents/pdf-library/Library/Cheung{\_}2016{\_}Sequence Modelling with Features Linear-Chain Conditional Random Fields - Lecture 9.pdf:pdf;:home/arnold/Documents/pdf-library/Library/Cheung{\_}2016{\_}Sequence Modelling with Features Linear-Chain Conditional Random Fields - Lecture 9(2).pdf:pdf},
title = {{Sequence Modelling with Features: Linear-Chain Conditional Random Fields - Lecture 9}},
year = {2016}
}
@article{Kipf2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1609.02907v1},
author = {Kipf, Thomas N and Welling, Max},
eprint = {arXiv:1609.02907v1},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Kipf, Welling{\_}2016{\_}Semi-Supervised Classification with Graph Convolutional Networks.pdf:pdf},
pages = {1--10},
title = {{Semi-Supervised Classification with Graph Convolutional Networks}},
year = {2016}
}
@article{Defferrard2016,
abstract = {Convolutional neural networks (CNNs) have greatly improved state-of-the-art performances in a number of fields, notably computer vision and natural language processing. In this work, we are interested in generalizing the formulation of CNNs from low-dimensional regular Euclidean domains, where images (2D), videos (3D) and audios (1D) are represented, to high-dimensional irregular domains such as social networks or biological networks represented by graphs. This paper introduces a formulation of CNNs on graphs in the context of spectral graph theory. We borrow the fundamental tools from the emerging field of signal processing on graphs, which provides the necessary mathematical background and efficient numerical schemes to design localized graph filters efficient to learn and evaluate. As a matter of fact, we introduce the first technique that offers the same computational complexity than standard CNNs, while being universal to any graph structure. Numerical experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs, as long as the graph is well-constructed.},
archivePrefix = {arXiv},
arxivId = {1606.09375},
author = {Defferrard, Micha{\"{e}}l and Bresson, Xavier and Vandergheynst, Pierre},
eprint = {1606.09375},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Defferrard, Bresson, Vandergheynst{\_}2016{\_}Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering.pdf:pdf},
pages = {1--14},
title = {{Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering}},
url = {http://arxiv.org/abs/1606.09375},
year = {2016}
}
@incollection{Devroye2016i,
author = {Devroye, Luc},
booktitle = {Probabilistic Analysis of Algorithms},
chapter = {3},
file = {:home/arnold/Documents/pdf-library/Library/Devroye{\_}2016{\_}Records and Their Applications.pdf:pdf},
title = {{Records and Their Applications}},
year = {2016}
}
@incollection{Devroye2016j,
author = {Devroye, Luc},
booktitle = {Probabilistic Analysis of Algorithms},
chapter = {4},
file = {:home/arnold/Documents/pdf-library/Library/Devroye{\_}2016{\_}Branching Processes.pdf:pdf},
title = {{Branching Processes}},
year = {2016}
}
@incollection{Devroye2016k,
author = {Devroye, Luc},
booktitle = {Probabilistic Analysis of Algorithms},
chapter = {1,2},
file = {:home/arnold/Documents/pdf-library/Library/Devroye{\_}2016{\_}Introduction, Probabililty.pdf:pdf},
title = {{Introduction, Probabililty}},
year = {2016}
}
@misc{Devroye2016l,
author = {Devroye, Luc},
file = {:home/arnold/Documents/pdf-library/Library/Devroye{\_}2016{\_}Assignment 3 - Branching Processes.pdf:pdf},
title = {{Assignment 3 - Branching Processes}},
year = {2016}
}
@article{Dosovitskiy2014,
abstract = {We train generative 'up-convolutional' neural networks which are able to generate images of objects given object style, viewpoint, and color. We train the networks on rendered 3D models of chairs, tables, and cars. Our experiments show that the networks do not merely learn all images by heart, but rather find a meaningful representation of 3D models allowing them to assess the similarity of different models, interpolate between given views to generate the missing ones, extrapolate views, and invent new objects not present in the training set by recombining training instances, or even two different object classes. Moreover, we show that such generative networks can be used to find correspondences between different objects from the dataset, outperforming existing approaches on this task.},
archivePrefix = {arXiv},
arxivId = {1411.5928},
author = {Dosovitskiy, Alexey and Springenberg, Jost Tobias and Tatarchenko, Maxim and Brox, Thomas},
doi = {10.1109/CVPR.2015.7298761},
eprint = {1411.5928},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Dosovitskiy et al.{\_}2014{\_}Learning to Generate Chairs, Tables and Cars with Convolutional Networks.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {arXiv preprint arXiv:1411.5928},
pages = {1--14},
pmid = {25246403},
title = {{Learning to Generate Chairs, Tables and Cars with Convolutional Networks}},
year = {2014}
}
@article{Cheung2016g,
author = {Cheung, Jackie},
file = {:home/arnold/Documents/pdf-library/Library/Cheung{\_}2016{\_}Part of Speech Tagging Algorithms - Lecture 8.pdf:pdf},
title = {{Part of Speech Tagging: Algorithms - Lecture 8}},
year = {2016}
}
@article{Lee2016,
author = {Lee, S. K. and Fekete, S. P. and McLurkin, J.},
doi = {10.1177/0278364915624974},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Lee, Fekete, McLurkin{\_}2016{\_}Structured triangulation in multi-robot systems Coverage, patrolling, Voronoi partitions, and geodesic center.pdf:pdf},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
keywords = {distributed algorithm,localization,multi-robot system,navigation,robotic network,space coverage},
pages = {1--27},
title = {{Structured triangulation in multi-robot systems: Coverage, patrolling, Voronoi partitions, and geodesic centers}},
url = {http://ijr.sagepub.com/cgi/doi/10.1177/0278364915624974},
year = {2016}
}
@article{Sutton2013,
abstract = {CiteSeerX - Scientific documents that cite the following paper: Reinforcement learning: An introduction, chapter 11},
archivePrefix = {arXiv},
arxivId = {1603.02199},
author = {Sutton, R S and Barto, A G},
doi = {10.1109/TNN.1998.712192},
eprint = {1603.02199},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Sutton, Barto{\_}2013{\_}Draft-2 Reinforcement learning an introduction.pdf:pdf},
isbn = {9780262193986},
issn = {1045-9227},
journal = {Neural Networks IEEE Transactions on},
keywords = {Artificial intelligence,Cybernetics,Signal process},
number = {5},
pages = {1054},
pmid = {18255791},
title = {{[Draft-2] Reinforcement learning : an introduction}},
volume = {9},
year = {2013}
}
@misc{Cheung2016h,
author = {Cheung, Jackie},
file = {:home/arnold/Documents/pdf-library/Library/Cheung{\_}2016{\_}Part of Speech Tagging - Lecture 7.pdf:pdf},
title = {{Part of Speech Tagging - Lecture 7}},
year = {2016}
}
@article{Arora2014,
author = {Arora, Sanjeev and Ge, Rong},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Arora, Ge{\_}2014{\_}Building Topic Models Based on Anchor Words.pdf:pdf},
pages = {1--13},
title = {{Building Topic Models Based on Anchor Words}},
url = {http://cs.stanford.edu/{~}rishig/courses/ref/l9b.pdf},
year = {2014}
}
@article{Reed2003,
author = {Reed, Bruce},
doi = {10.1145/765568.765571},
file = {:home/arnold/Documents/pdf-library/Library/Reed{\_}2003{\_}The height of a random binary search tree.pdf:pdf},
issn = {00045411},
journal = {Journal of the ACM},
number = {3},
pages = {306--332},
title = {{The height of a random binary search tree}},
url = {http://portal.acm.org/citation.cfm?doid=765568.765571},
volume = {50},
year = {2003}
}
@inproceedings{Kingma2014,
abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
archivePrefix = {arXiv},
arxivId = {1312.6114},
author = {Kingma, Diederik P and Welling, Max},
booktitle = {ICLR},
eprint = {1312.6114},
file = {:home/arnold/Documents/pdf-library/Library/Kingma, Welling{\_}2014{\_}Auto-Encoding Variational Bayes.pdf:pdf},
isbn = {1312.6114v10},
issn = {1312.6114v10},
number = {Ml},
pages = {1--14},
title = {{Auto-Encoding Variational Bayes}},
url = {http://arxiv.org/abs/1312.6114},
year = {2014}
}
@misc{Cheung2016n,
author = {Cheung, Jackie},
file = {:home/arnold/Documents/pdf-library/Library/Cheung{\_}2016{\_}Feature Extraction and Classification - Lecture 5.pdf:pdf},
title = {{Feature Extraction and Classification - Lecture 5}},
year = {2016}
}
@misc{Cheung2016l,
author = {Cheung, Jackie},
file = {:home/arnold/Documents/pdf-library/Library/Cheung{\_}2016{\_}COMP599 ASSIGNMENT 1.pdf:pdf},
pages = {3--5},
title = {{COMP599: ASSIGNMENT 1}},
year = {2016}
}
@book{Wilf1990,
abstract = {This book is about generating functions and some of their uses in discrete mathematics. The subject is so vast that I have not attempted to give a comprehensive discussion. Instead I have tried only to communicate some of the main ideas.},
author = {Wilf, Herbert S.},
booktitle = {The American Mathematical Monthly},
doi = {10.2307/2324771},
file = {:home/arnold/Documents/pdf-library/Library/Wilf{\_}1990{\_}Generatingfunctionology.pdf:pdf},
isbn = {1568812795},
issn = {00029890},
number = {9},
pages = {864},
title = {{Generatingfunctionology}},
url = {http://www.jstor.org/stable/2324771?origin=crossref},
volume = {97},
year = {1990}
}
@misc{Cheung2016o,
author = {Cheung, Jackie},
file = {:home/arnold/Documents/pdf-library/Library/Cheung{\_}2016{\_}Classification II Python Recap - Lecture 6.pdf:pdf},
title = {{Classification II + Python Recap - Lecture 6}},
year = {2016}
}
@misc{Cheung2016k,
author = {Cheung, Jackie},
file = {:home/arnold/Documents/pdf-library/Library/Cheung{\_}2016{\_}Introduction to Natural Language Processing - Lecture 1.pdf:pdf},
title = {{Introduction to Natural Language Processing - Lecture 1}},
year = {2016}
}
@misc{Cheung2016j,
author = {Cheung, Jackie},
file = {:home/arnold/Documents/pdf-library/Library/Cheung{\_}2016{\_}Words Language Modelling By N-Grams - Lecture 3.pdf:pdf},
title = {{Words: Language Modelling By N-Grams - Lecture 3}},
year = {2016}
}
@misc{Tran2016,
author = {Tran, Dustin},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Tran{\_}2016{\_}Edward A library for probabilistic modeling, inference, and criticism.pdf:pdf},
title = {{Edward : A library for probabilistic modeling, inference, and criticism}},
year = {2016}
}
@misc{Cheung2016i,
author = {Cheung, Jackie},
file = {:home/arnold/Documents/pdf-library/Library/Cheung{\_}2016{\_}Morphology Word Formation , FSAs and FSTs - Lecture 2.pdf:pdf},
title = {{Morphology: Word Formation , FSAs and FSTs - Lecture 2}},
year = {2016}
}
@misc{Cheung2016m,
author = {Cheung, Jackie},
file = {:home/arnold/Documents/pdf-library/Library/Cheung{\_}2016{\_}Language Modelling Smoothing and Model Complexity - Lecture 4.pdf:pdf},
title = {{Language Modelling: Smoothing and Model Complexity - Lecture 4}},
year = {2016}
}
@misc{Devroye2016m,
author = {Devroye, Luc},
file = {:home/arnold/Documents/pdf-library/Library/Devroye{\_}2016{\_}Assignment 2 - Records.pdf:pdf},
title = {{Assignment 2 - Records}},
year = {2016}
}
@book{Grimmett2001,
author = {Grimmett, Geoffrey and Stirzaker, David},
file = {:home/arnold/Documents/pdf-library/Library/Grimmett, Stirzaker{\_}2001{\_}Probability and Random Processes.pdf:pdf},
title = {{Probability and Random Processes}},
url = {http://optima.skku.ac.kr/courses/2013S/random/ch2.pdf},
year = {2001}
}
@article{Ke2016,
abstract = {In the probabilistic topic models, the quantity of interest---a low-rank matrix consisting of topic vectors---is hidden in the text corpus matrix, masked by noise, and the Singular Value Decomposition (SVD) is a potentially useful tool for learning such a low-rank matrix. However, the connection between this low-rank matrix and the singular vectors of the text corpus matrix are usually complicated and hard to spell out, so how to use SVD for learning topic models faces challenges. In this paper, we overcome the challenge by revealing a surprising insight: there is a low-dimensional simplex structure which can be viewed as a bridge between the low-rank matrix of interest and the SVD of the text corpus matrix, and allows us to conveniently reconstruct the former using the latter. Such an insight motivates a new SVD approach to learning topic models, which we analyze with delicate random matrix theory and derive the rate of convergence. We support our methods and theory numerically, using both simulated data and real data.},
archivePrefix = {arXiv},
arxivId = {1608.04478},
author = {Ke, Zheng Tracy},
eprint = {1608.04478},
number = {1},
title = {{A Geometrical Approach to Topic Model Estimation}},
url = {http://arxiv.org/abs/1608.04478},
year = {2016}
}
@book{Jurafsky2006,
author = {Jurafsky, Daniel S and Martin, James H},
doi = {10.1162/089120100750105975},
file = {:home/arnold/Documents/pdf-library/Library/Jurafsky, Martin{\_}2006{\_}Speech and Language Processing An Introduction to Natural Language Processing, Computational Linguistics, and Spee.pdf:pdf},
isbn = {9780135041963},
issn = {08912017},
pmid = {22146067},
title = {{Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition}},
year = {2006}
}
@article{DavisS.andMermelstein1980,
abstract = {Several parametric representations of the acoustic signal were compared with regard to word recognition performance in a syllable-oriented continuous speech recognition system. The vocabulary included many phonetically similar monosyllabic words, therefore the emphasis was on the ability to retain phonetically significant acoustic information in the face of syntactic and duration variations. For each parameter set (based on a mel-frequency cepstrum, a linear frequency cepstrum, a linear prediction cepstrum, a linear prediction spectrum, or a set of reflection coefficients), word templates were generated using an efficient dynamic warping method, and test data were time registered with the templates. A set of ten mel-frequency cepstrum coefficients computed every 6.4 ms resulted in the best performance, namely 96.5 percent and 95.0 percent recognition with each of two speakers. The superior performance of the mel-frequency cepstrum coefficients may be attributed to the fact that they better represent the perceptually relevant aspects of the short-term speech spectrum.},
author = {{Davis S. and Mermelstein}, P.},
doi = {10.1109/TASSP.1980.1163420},
issn = {0096-3518},
journal = {IEEE Transactions on Signal Processing},
number = {4},
pages = {357 -- 366},
title = {{Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences}},
volume = {28},
year = {1980}
}
@article{Bishop1999,
abstract = {One of the central issues in the use of principal component analysis (PCA) for data modelling is that of choosing the appropriate number of retained components. This problem was recently addressed through the formulation of a Bayesian treatment of PCA in terms of a probabilistic latent variable model. A central feature of this approach is that the effective dimensionality of the latent space is determined automatically as part of the Bayesian inference procedure. In common with most non-trivial Bayesian models, however, the required marginalizations are analytically intractable, and so an approximation scheme based on a local Gaussian representation of the posterior distribution was employed. In this paper we develop an alternative, variational formulation of Bayesian PCA, based on a factorial representation of the posterior distribution. This approach is computationally efficient, and unlike other approximation schemes, it maximizes a rigorous lower bound on the marginal log probability of the observed data.},
author = {Bishop, C M},
doi = {10.1049/cp:19991160},
isbn = {0 85296 721 7},
issn = {05379989},
journal = {9th International Conference on Artificial Neural Networks ICANN 99},
number = {470},
pages = {509--514},
title = {{Variational principal components}},
url = {http://link.aip.org/link/IEECPS/v1999/iCP470/p509/s1{\&}Agg=doi},
volume = {1999},
year = {1999}
}
@article{Johnson2013,
abstract = {Sampling inference methods are computationally difficult to scale for many models in part because global dependencies can reduce opportunities for parallel computation. Without strict conditional independence structure among variables, standard Gibbs sampling theory requires sample updates to be performed sequentially, even if dependence between most variables is not strong. Empirical work has shown that some models can be sampled effectively by going “Hogwild” and simply running Gibbs updates in parallel with only periodic global communication, but the successes and limitations of such a strategy are not well understood. As a step towards such an understanding, we study the Hogwild Gibbs sampling strategy in the context of Gaussian distributions. We develop a framework which provides convergence conditions and error bounds along with simple proofs and connections to methods in numerical linear algebra. In particular, we show that if the Gaussian precision matrix is generalized diagonally dominant, then any Hogwild Gibbs sampler, with any update schedule or allocation of variables to processors, yields a stable sampling process with the correct sample mean.},
author = {Johnson, Matthew J and Saunderson, James and Willsky, Alan S},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Johnson, Saunderson, Willsky{\_}2013{\_}Analyzing Hogwild Parallel Gaussian Gibbs Sampling.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {1--9},
title = {{Analyzing Hogwild Parallel Gaussian Gibbs Sampling}},
year = {2013}
}
@book{Brown1986,
author = {Brown, Lawrence D},
isbn = {0940600102},
title = {{Fundamentals of Statistical Exponential Families with Applications in Statistical Decision Theory}},
year = {1986}
}
@article{Wainwright2003,
author = {Wainwright, M.{\~{}}J. and Jordan, M.{\~{}}I.},
doi = {10.1561/2200000001},
issn = {1935-8237},
number = {649},
pages = {1--305},
title = {{Graphical Models, Exponential Families, and Variational Inference}},
volume = {1},
year = {2003}
}
@article{ChongWang2011,
abstract = {Researchers have access to large online archives of scientiﬁc arti- cles. As a consequence, ﬁnding relevant papers has become more difﬁcult. Newly formed online communities of researchers sharing citations provides a new way to solve this problem. In this paper, we develop an algorithm to recommend scientiﬁc articles to users of an online community. Our approach combines the merits of traditional collaborative ﬁltering and probabilistic topic modeling. It provides an interpretable latent structure for users and items, and can form recommendations about both existing and newly published articles. We study a large subset of data from CiteULike, a bibliography shar- ing service, and show that our algorithm provides a more effective recommender system than traditional collaborative ﬁltering.},
author = {{Chong Wang}, David M Blei},
doi = {10.1145/2020408.2020480},
isbn = {9781450308137},
issn = {14710072},
journal = {Kdd'11},
keywords = {Scientiﬁc article recommendation, To pic modeling,},
pages = {448--456},
pmid = {16990852},
title = {{Collaborative Topic Modeling for Recommending Scientiﬁc Articles}},
year = {2011}
}
@article{Lafferty2006,
abstract = {Topic models, such as latent Dirichlet allocation (LDA), can be useful tools for the statistical analysis of document collections and other discrete data. The LDA model assumes that the words of each document arise from a mixture of topics, each of which is a distribution over the vocabulary. A limitation of LDA is the inability to model topic correlation even though, for example, a document about genetics is more likely to also be about disease than x-ray astronomy. This limitation stems from the use of the Dirichlet distribution to model the variability among the topic proportions. In this paper we develop the correlated topic model (CTM), where the topic proportions exhibit correlation via the logistic normal distribution. We derive a mean-field variational inference algorithm for approximate posterior inference in this model, which is complicated by the fact that the logistic normal is not conjugate to the multinomial. The CTM gives a better fit than LDA on a collection of OCRed articles from the journal Science. Furthermore, the CTM provides a natural way of visualizing and exploring this and other unstructured data sets.},
archivePrefix = {arXiv},
arxivId = {arXiv:0712.1486v1},
author = {Lafferty, David M. Blei and John D.},
doi = {10.1145/1143844.1143859},
eprint = {arXiv:0712.1486v1},
isbn = {1595933832},
issn = {19326157},
journal = {Advances in Neural Information Processing Systems 18},
pages = {147--154},
title = {{Correlated Topic Models}},
url = {papers2://publication/uuid/1191CDB8-6BB3-4201-8EFB-6F7B8CBA0E8F},
year = {2006}
}
@article{Wang2013,
abstract = {Mean-field variational methods are widely used for approximate posterior inference in many prob-abilistic models. In a typical application, mean-field methods approximately compute the posterior with a coordinate-ascent optimization algorithm. When the model is conditionally conjugate, the coordinate updates are easily derived and in closed form. However, many models of interest—like the correlated topic model and Bayesian logistic regression—are nonconjugate. In these models, mean-field methods cannot be directly applied and practitioners have had to develop variational algorithms on a case-by-case basis. In this paper, we develop two generic methods for nonconju-gate models, Laplace variational inference and delta method variational inference. Our methods have several advantages: they allow for easily derived variational algorithms with a wide class of nonconjugate models; they extend and unify some of the existing algorithms that have been derived for specific models; and they work well on real-world data sets. We studied our methods on the correlated topic model, Bayesian logistic regression, and hierarchical Bayesian logistic regression.},
archivePrefix = {arXiv},
arxivId = {1209.4360},
author = {Wang, Chong and Blei, David M},
eprint = {1209.4360},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {Laplace approximations,nonconjugate models,the multivariate delta method,variational inference},
pages = {1005--1031},
title = {{Variational Inference in Nonconjugate Models}},
volume = {14},
year = {2013}
}
@article{Lim2015,
abstract = {In contrast to classic geometric motion planning, informative path planning (IPP) seeks a path for a robot to sense the world and gain information. In adaptive IPP, the robot chooses the next sensing location conditioned on all information acquired so far, and the robot's goal is to minimize the travel cost required for identifying a true hypothesis. Adaptive IPP is NP-hard, because the robot must trade-off information gain and travel cost optimally. In this paper we present Recursive Adaptive Identification (RAId), a new polynomial-time approximation algorithm for adaptive IPP. We prove a polylogarithmic approximation bound when the robot travels in a metric space. Furthermore, our experiments suggest that RAId is practical and provides good approximate solutions for two distinct robot planning tasks. Although RAId is designed primarily for noiseless observations, a simple extension allows it to handle some tasks with noisy observations.},
author = {Lim, Zhan Wei and Hsu, David and Lee, Wee Sun},
doi = {10.1007/978-3-319-16595-0_17},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Lim, Hsu, Lee{\_}2015{\_}Adaptive informative path planning in metric spaces.pdf:pdf},
isbn = {9783319165943},
issn = {1610742X},
journal = {Springer Tracts in Advanced Robotics},
keywords = {adaptive path planning,informative path planning,planning under uncertainty},
pages = {283--300},
title = {{Adaptive informative path planning in metric spaces}},
volume = {107},
year = {2015}
}
@article{Buntine2014,
abstract = {In topic modelling, various alternative priors have been developed, for instance asymmetric and symmetric priors for the document-topic and topic-word matrices respectively, the hierarchical Dirichlet process prior for the document-topic matrix and the hierarchical Pitman-Yor process prior for the topic-word matrix. For information retrieval, language models exhibiting word burstiness are important. Indeed, this burstiness effect has been show to help topic models as well, and this requires additional word probability vectors for each document. Here we show how to combine these ideas to develop high-performing non-parametric topic models exhibiting burstiness based on standard Gibbs sampling. Experiments are done to explore the behavior of the models under different conditions and to compare the algorithms with previously published. The full non-parametric topic models with burstiness are only a small factor slower than standard Gibbs sampling for LDA and require double the memory, making them very competitive. We look at the comparative behaviour of different models and present some experimental insights. {\textcopyright} 2014 ACM.},
author = {Buntine, Wray L. and Mishra, Swapnil},
doi = {10.1145/2623330.2623691},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Buntine, Mishra{\_}2014{\_}Experiments with non-parametric topic models.pdf:pdf},
isbn = {9781450329569},
journal = {Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '14},
keywords = {experimental results,non-parametric prior,topic modelling},
number = {August},
pages = {881--890},
title = {{Experiments with non-parametric topic models}},
url = {http://dl.acm.org/citation.cfm?doid=2623330.2623691},
year = {2014}
}
@article{Das2015,
abstract = {Robotic sampling is attractive in many field robotics applications that require persistent collection of physical samples for ex-situ analysis. Examples abound in the earth sciences in studies involving the collection of rock, soil, and water samples for laboratory analysis. In our test domain, marine ecosystem monitoring, detailed understanding of plankton ecology requires laboratory analysis of water samples, but predictions using physical and chemical properties measured in real-time by sensors aboard an autonomous underwater vehicle (AUV) can guide sample collection decisions. In this paper, we present a data-driven and opportunistic sampling strategy to minimize cumulative regret for batches of plankton samples acquired by an AUV over multiple surveys. Samples are labeled at the end of each survey, and used to update a probabilistic model that guides sampling during subsequent surveys. During a survey, the AUV makes irrevocable sample collection decisions online for a sequential stream of candidates, with no knowledge of the quality of future samples. In addition to extensive simulations using historical field data, we present results from a one-day field trial where beginning with a prior model learned from data collected and labeled in an earlier campaign, the AUV collected water samples with a high abundance of a pre-specified planktonic target. This is the first time such a field experiment has been carried out in its entirety in a data-driven fashion, in effect "closing the loop" on a significant and relevant ecosystem monitoring problem while allowing domain experts (marine ecologists) to specify the mission at a relatively high level.},
author = {Das, Jnaneshwar and Py, Frederic and Harvey, Julio B. J. and Ryan, John P. and Gellene, Alyssa and Graham, Rishi and Caron, David A. and Rajan, Kanna and Sukhatme, Gaurav S.},
doi = {10.1177/0278364915587723},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Das et al.{\_}2015{\_}Data-driven robotic sampling for marine ecosystem monitoring.pdf:pdf},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
keywords = {field robots,learning and adaptive systems,machine learning,marine robotics,robotic sampling},
number = {12},
pages = {1435--1452},
title = {{Data-driven robotic sampling for marine ecosystem monitoring}},
url = {http://ijr.sagepub.com/content/34/12/1435.full},
volume = {34},
year = {2015}
}
@article{Lake2015,
author = {Lake, B. M. and Salakhutdinov, R. and Tenenbaum, J. B.},
doi = {10.1126/science.aab3050},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Lake, Salakhutdinov, Tenenbaum{\_}2015{\_}Human-level concept learning through probabilistic program induction.pdf:pdf},
issn = {0036-8075},
journal = {Science},
month = {dec},
number = {6266},
pages = {1332--1338},
title = {{Human-level concept learning through probabilistic program induction}},
url = {http://www.sciencemag.org/cgi/doi/10.1126/science.aab3050},
volume = {350},
year = {2015}
}
@article{Rao2016,
author = {Rao, Dushyant and Bender, Asher and Williams, Stefan B and Pizarro, Oscar},
isbn = {9781467380256},
pages = {4230--4237},
title = {{Multimodal information-theoretic measures for autonomous exploration}},
year = {2016}
}
@article{Kolda2009,
author = {Kolda, Tamara G and Bader, Brett W},
doi = {10.1137/07070111X},
issn = {0036-1445},
journal = {SIAM Review},
keywords = {canonical decomposition (CANDECOMP),higher-order principal components analysis (Tucker,higher-order singular value decomposition (HOSVD),multilinear algebra,multiway arrays,parallel factors (PARAFAC),tensor decompositions},
month = {aug},
number = {3},
pages = {455--500},
title = {{Tensor Decompositions and Applications}},
url = {http://epubs.siam.org/doi/abs/10.1137/07070111X},
volume = {51},
year = {2009}
}
@article{Berry2007,
abstract = {In this paper we discuss the development and use of low-rank approximate nonnegative matrix factorization (NMF) algorithms for feature extraction and identification in the fields of text mining and spectral data analysis. The evolution and convergence properties of hybrid methods based on both sparsity and smoothness constraints for the resulting nonnegative matrix factors are discussed. The interpretability of NMF outputs in specific contexts are provided along with opportunities for future work in the modification of NMF algorithms for large-scale and time-varying datasets. Key words: nonnegative matrix factorization, text mining, spectral data analysis, email surveillance, conjugate gradient, constrained least squares.},
author = {Berry, Michael W and Browne, Murray and Langville, Amy N and Pauca, Vp and Plemmons, Robert J.},
doi = {10.1016/j.csda.2006.11.006},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Berry et al.{\_}2007{\_}Algorithms and applications for approximate nonnegative matrix factorization.pdf:pdf},
isbn = {0167-9473},
issn = {01679473},
journal = {Computational Statistics {\&} Data Analysis},
keywords = {by the national science,conjugate gradient,constrained least squares,email surveillance,foundation under grant,nonnegative matrix factorization,research supported in part,spectral data analysis,text mining},
number = {June 2006},
pages = {155--173},
pmid = {9126737},
title = {{Algorithms and applications for approximate nonnegative matrix factorization}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167947306004191{\%}5Cnhttp://www.sciencedirect.com/science/article/pii/S0167947306004191},
volume = {52},
year = {2007}
}
@article{Chang,
author = {Chang, Xiao-wen},
number = {1},
pages = {2--5},
title = {{Total LS Estimation}},
volume = {1}
}
@article{Number2011,
author = {Number, Condition},
pages = {139--192},
title = {{Linear Least - Squares Estimation :}},
volume = {2},
year = {2011}
}
@article{Cichocki2007,
abstract = {Nonnegative Matrix and Tensor Factorization (NMF/NTF) and Sparse Component Analysis (SCA) have already found many potential applications, especially in multi-way Blind Source Separation (BSS), multi-dimensional data analysis, model reduction and sparse signal/image representations. In this paper we propose a family of the modified Regularized Alternating Least Squares (RALS) algorithms for NMF/NTF. By incorporating regularization and penalty terms into the weighted Frobenius norm we are able to achieve sparse and/or smooth representations of the desired solution, and to alleviate the problem of getting stuck in local minima. We implemented the RALS algorithms in our NMFLAB/NTFLAB Matlab Toolboxes, and compared them with standard NMF algorithms. The proposed algorithms are characterized by improved efficiency and convergence properties, especially for large-scale problems.},
author = {Cichocki, Andrzej and Zdunek, Rafal},
doi = {10.1007/978-3-540-72395-0_97},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Cichocki, Zdunek{\_}2007{\_}Regularized Alternating Least Squares Algorithms for Non-negative Matrix Tensor.pdf:pdf},
isbn = {978-3-540-72394-3},
issn = {0302-9743},
journal = {Advances in Neural Networks, Lecture Notes in Computer Science},
number = {1},
pages = {793--802},
title = {{Regularized Alternating Least Squares Algorithms for Non-negative Matrix / Tensor}},
url = {http://www.springerlink.com/index/q8218lt427527826.pdf},
volume = {4493/2007},
year = {2007}
}
@article{Changa,
author = {Chang, Xiao-wen},
pages = {1--9},
title = {{Linear Algebra , Norms , Taylor Series , Matrix Factorizations}}
}
@article{Changb,
author = {Chang, Xiao-wen},
pages = {1--8},
title = {{Solving Large Sparse Linear Least Squares Problems}}
}
@article{Data2008,
author = {Data, Scientific},
journal = {Spring},
pages = {1--30},
title = {{Basic Concepts in Probability and Statistics}},
year = {2008}
}
@article{Hartley1965,
author = {Hartley, H. O. and Booker, Aaron},
journal = {The Annals of Mathematical Statistics},
number = {2},
pages = {638--650},
title = {{Nonlinear Least Squares Estimation}},
volume = {36},
year = {1965}
}
@article{Changc,
author = {Chang, Xiao-wen},
pages = {2--4},
title = {{Maximum-likelihood Estimation An example : MLE for univariate Gaussian}}
}
@article{Changd,
author = {Chang, Xiao-wen},
pages = {1--6},
title = {{Linear Minimum Mean Squares Estimation}}
}
@article{Change,
author = {Chang, Xiao-wen},
pages = {2--4},
title = {{Ill-conditioned Problems and Regularized LS Estimation}}
}
@article{Changf,
author = {Chang, Xiao-wen and Paige, Christopher C},
number = {4},
pages = {1--8},
title = {{discrete Kalman filtering}}
}
@article{Paige1982,
abstract = {LSQR finds a soulution x to the following problems: Unsymmetric equations:$\backslash$nLinear least squares: Damped least squares: The matrix A will normally$\backslash$nbe large and sparse. It is defined by means of a user-written subroutine$\backslash$nAPROD, whose essential function is to compute products of the form$\backslash$nAx and A{\^{}}{\{}T{\}} y for given vectors x and y.},
author = {Paige, Christopher C. and Saunders, Michael a.},
doi = {10.1145/355993.356000},
issn = {00983500},
journal = {ACM Transactions on Mathematical Software},
number = {2},
pages = {195--209},
title = {{Algorithm 583: LSQR: Sparse Linear Equations and Least Squares Problems}},
volume = {8},
year = {1982}
}
@article{Paige1982a,
abstract = {An iterative method is given for solving Ax {\~{}}ffi b and minU Ax - b 112, where the matrix A is large and sparse. The method is based on the bidiagonalization procedure of Golub and Kahan. It is analytically equivalent to the standard method of conjugate gradients, but possesses more favorable numerical properties. iable stopping criteria are derived, along with estimates of standard errors for x and the condition number of A. These are used in the FORTRAN implementation of the method, subroutine LSQR. Numerical tests are described comparing I{\~{}}QR with several other conjugate-gradient algori- thms, indicating that I{\~{}}QR is the most reliable algorithm when A is ill-conditioned. Categories and Subject Descriptors: G.1.2 Numerical Analysis: ApprorJmation-least squares approximation; G.1.3 Numerical Analysis: Numerical Linear Algebra-linear systems (direct and tterative methods); sparse and very large systems General Terms: Algorithms Additional Key Words and Phrases: analysis of variance The Algorithm: LSQR: Sparse Linear Equations and Least Square Problems. ACM Trans. Math. Softw. 8, 2 (June 1982). 1. INTRODUCTION},
author = {Paige, Christopher C. and Saunders, Michael a.},
doi = {10.1145/355984.355989},
isbn = {0098-3500},
issn = {00983500},
journal = {ACM Transactions on Mathematical Software},
number = {1},
pages = {43--71},
title = {{LSQR: An Algorithm for Sparse Linear Equations and Sparse Least Squares}},
url = {http://portal.acm.org/citation.cfm?doid=355984.355989},
volume = {8},
year = {1982}
}
@article{Fong2011,
author = {Fong, David C.-L. and Saunders, Michael A.},
journal = {SIAM Journal on Scientific Computing},
keywords = {10,10079687x,1137,15a06,65f10,65f20,65f22,65f25,65f35,65f50,93e24,ams subject classifications,conjugate-gradient method,doi,golub,iterative method,kahan process,krylov subspace method,least-squares problem,lsqr,minimum-residual method,minres,sparse matrix},
number = {5},
pages = {2950--2971},
title = {{LSMR: An iterative algorithm for sparse least-squares}},
volume = {33},
year = {2011}
}
@article{Gauss,
author = {Gauss, Karl},
isbn = {0471708585},
number = {2},
pages = {79--105},
title = {{Least squares estimation}},
volume = {1795}
}
@article{Ranganath2014,
abstract = {We describe $\backslash$textit{\{}deep exponential families{\}} (DEFs), a class of latent variable models that are inspired by the hidden structures used in deep neural networks. DEFs capture a hierarchy of dependencies between latent variables, and are easily generalized to many settings through exponential families. We perform inference using recent "black box" variational inference techniques. We then evaluate various DEFs on text and combine multiple DEFs into a model for pairwise recommendation data. In an extensive study, we show that going beyond one layer improves predictions for DEFs. We demonstrate that DEFs find interesting exploratory structure in large data sets, and give better predictive performance than state-of-the-art models.},
archivePrefix = {arXiv},
arxivId = {arXiv:1411.2581v1},
author = {Ranganath, Rajesh and Tang, Linpeng and Charlin, Laurent and Blei, David M},
eprint = {arXiv:1411.2581v1},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Ranganath et al.{\_}2014{\_}Deep Exponential Families (Appendix).pdf:pdf},
journal = {arXiv:1411.2581v1},
pages = {2--4},
title = {{Deep Exponential Families (Appendix)}},
year = {2014}
}
@article{Sim2004,
author = {Sim, Robert and Dudek, Gregory and Roy, Nicholas},
doi = {10.1109/ROBOT.2004.1308078},
isbn = {0-7803-8232-3},
issn = {10504729},
journal = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},
keywords = {exploration},
number = {April},
pages = {1758--1763},
title = {{Online Control Policy Optimization for Minimizing Map Uncertainty During Exploration}},
volume = {2},
year = {2004}
}
@article{Jordan1999,
abstract = {This paper presents a tutorial introduction to the use of variational$\backslash$nmethods for inference and learning in graphical models (Bayesian$\backslash$nnetworks and Markov random fields). We present a number of examples$\backslash$nof graphical models, including the QMR-DT database, the sigmoid belief$\backslash$nnetwork, the Boltzmann machine, and several variants of hidden Markov$\backslash$nmodels, in which it is infeasible to run exact inference algorithms.$\backslash$nWe then introduce variational methods, which exploit laws of large$\backslash$nnumbers to transform the original graphical model into a simplified$\backslash$ngraphical model in which inference is efficient. Inference in the$\backslash$nsimpified model provides bounds on probabilities of interest in the$\backslash$noriginal model. We describe a general framework for generating variational$\backslash$ntransformations based on convex duality. Finally we return to the$\backslash$nexamples and demonstrate how variational algorithms can be formulated$\backslash$nin each case.},
archivePrefix = {arXiv},
arxivId = {arXiv:1103.5254v3},
author = {Jordan, Michael I. and Ghahramani, Zoubin and Jaakkola, Tommi S. and Saul, Lawrence K.},
doi = {10.1023/A:1007665907178},
eprint = {arXiv:1103.5254v3},
isbn = {0262600323},
issn = {08856125},
journal = {Machine Learning},
keywords = {approximate infer-,bayesian networks,belief networks,boltzmann machines,ence,graphical models,hidden markov models,mean field methods,neural networks,probabilistic inference,variational methods},
number = {2},
pages = {183--233},
pmid = {10063011},
title = {{Introduction to variational methods for graphical models}},
volume = {37},
year = {1999}
}
@article{Kolda2011,
abstract = {Recent work on eigenvalues and eigenvectors for tensors of orderm≥3 has been motivated by applications in blind source separation, magnetic resonance imaging, molecular conformation, and more. In this paper, we consider methods for computing real symmetric-tensor eigenpairs of the formAxm−1=$\lambda$xsubject tox= 1, which is closely related to optimal rank-1 approximation of a symmetric tensor. Our contribution is a shifted symmetric higher-order power method (SS-HOPM), which we show is guaranteed to converge to a tensor eigenpair. SS-HOPM can be viewed as a generalization of the power iteration method for matrices or of the symmetric higherorder power method. Additionally, using fixed point analysis, we can characterize exactly which eigenpairs can and cannot be found by the method. Numerical examples are presented, including examples from an extension of the method to finding complex eigenpairs.},
archivePrefix = {arXiv},
arxivId = {1401.1183},
author = {Kolda, Tamara G and Mayo, Jackson R},
doi = {10.1137/140951758},
eprint = {1401.1183},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Kolda, Mayo{\_}2011{\_}Shifted Power Method for Computing Tensor Eigenpairs.pdf:pdf},
isbn = {0001405101},
issn = {0895-4798},
journal = {SIAM Journal on Matrix Analysis and Applications},
keywords = {(SS-HOPM),-eigenpairs,2,E-eigenpairs,Z-eigenpairs,l,rank-1 approximation,shifted symmetric higher-order power method,symmetric higher-order power method (S-HOPM),tensor eigenvalues},
number = {4},
pages = {1095--1124},
title = {{Shifted Power Method for Computing Tensor Eigenpairs}},
volume = {32},
year = {2011}
}
@article{Arora2012,
abstract = {Topic models provide a useful method for dimensionality reduction and exploratory data analysis in large text corpora. Most approaches to topic model inference have been based on a maximum likelihood objective. Efficient algorithms exist that approximate this objective, but they have no provable guarantees. Recently, algorithms have been introduced that provide provable bounds, but these algorithms are not practical because they are inefficient and not robust to violations of model assumptions. In this paper we present an algorithm for topic model inference that is both provable and practical. The algorithm produces results comparable to the best MCMC implementations while running orders of magnitude faster.},
archivePrefix = {arXiv},
arxivId = {arXiv:1212.4777v1},
author = {Arora, Sanjeev and Ge, R and Halpern, Y and Mimno, David},
eprint = {arXiv:1212.4777v1},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Arora et al.{\_}2012{\_}A practical algorithm for topic modeling with provable guarantees.pdf:pdf},
journal = {arXiv preprint arXiv: {\ldots}},
pages = {1--26},
title = {{A practical algorithm for topic modeling with provable guarantees}},
url = {http://arxiv.org/abs/1212.4777},
volume = {28},
year = {2012}
}
@article{Ranganath2014a,
abstract = {We describe $\backslash$textit{\{}deep exponential families{\}} (DEFs), a class of latent variable models that are inspired by the hidden structures used in deep neural networks. DEFs capture a hierarchy of dependencies between latent variables, and are easily generalized to many settings through exponential families. We perform inference using recent "black box" variational inference techniques. We then evaluate various DEFs on text and combine multiple DEFs into a model for pairwise recommendation data. In an extensive study, we show that going beyond one layer improves predictions for DEFs. We demonstrate that DEFs find interesting exploratory structure in large data sets, and give better predictive performance than state-of-the-art models.},
archivePrefix = {arXiv},
arxivId = {arXiv:1411.2581v1},
author = {Ranganath, Rajesh and Tang, Linpeng and Charlin, Laurent and Blei, David M},
eprint = {arXiv:1411.2581v1},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Ranganath et al.{\_}2014{\_}Deep Exponential Families.pdf:pdf},
journal = {arXiv:1411.2581v1},
title = {{Deep Exponential Families}},
volume = {38},
year = {2014}
}
@article{Pizarro2009,
abstract = {It is now common to quasi-automatically generate acoustic bathymetry and optical mosaics from instrumented Autonomous Underwater Vehicles (AUVs). However, further analysis and interpretation of gathered data is needed to address tasks such as habitat characterization and monitoring. This analysis stage is performed by human experts which limits the amount and speed of data processing. While it is unlikely that machines will match humans at fine-scale classification, machines can now perform preliminary, coarser classification to provide timely and relevant feedback to assist human decisions and enable adaptive AUV behavior. This paper presents a preliminary investigation into using a `bag of features' object recognition system for unsupervised clustering of marine habitat imagery. In addition to directly using the high dimensional signature vectors, we also perform clustering based on a low dimensional topic model of the images. We use an AUV transect in the Great Barrier Reef that covers distinct habitat to illustrate the behavior of hierarchical clustering using both representations. Results suggest that both approaches generate clusters of images that are easily recognizable by humans, with significant computational gains to be made by using a topic-based model.},
author = {Pizarro, Oscar O.R. and Williams, Stefan B. S.B. and Colquhoun, Jamie},
doi = {10.1109/OCEANSE.2009.5278260},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Pizarro, Williams, Colquhoun{\_}2009{\_}Topic-based habitat classification using visual data.pdf:pdf},
isbn = {978-1-4244-2522-8},
journal = {Oceans 2009-Europe},
pages = {1--8},
title = {{Topic-based habitat classification using visual data}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5278260{\%}5Cnhttp://ieeexplore.ieee.org.ezproxy1.library.usyd.edu.au/stamp/stamp.jsp?tp={\&}arnumber=5278260},
year = {2009}
}
@book{Wackerly2008,
author = {Wackerly, Dennis D. and Mendenhall, William III and Scheaffer, Richard L.},
edition = {7},
file = {:home/arnold/Documents/pdf-library/Library/Wackerly, Mendenhall, Scheaffer{\_}2008{\_}Mathematical Statistics with Applications.pdf:pdf},
isbn = {9780495385080},
publisher = {Thomson, Brooks/Cole},
title = {{Mathematical Statistics with Applications}},
year = {2008}
}
@book{Williams1991,
abstract = {This is a masterly introduction to the modern and rigorous theory of probability. The author adopts the martingale theory as his main theme and moves at a lively pace through the subject's rigorous foundations. Measure theory is introduced and then immediately exploited by being applied to real probability theory. Classical results, such as Kolmogorov's Strong Law of Large Numbers and Three-Series Theorem are proved by martingale techniques. A proof of the Central Limit Theorem is also given. The author's style is entertaining and inimitable with pedagogy to the fore. Exercises play a vital role; there is a full quota of interesting and challenging problems, some with hints.},
author = {Williams, D.},
doi = {10.1002/cne.22311},
file = {:home/arnold/Documents/pdf-library/Library/Williams{\_}1991{\_}Probability with Martingales.pdf:pdf},
isbn = {052140455X;},
issn = {10969861},
pages = {xvi+251},
pmid = {20394051},
title = {{Probability with Martingales}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20858127},
volume = {518},
year = {1991}
}
@article{Miah2007,
author = {Miah, Andy},
file = {:home/arnold/Documents/pdf-library/Library/Miah{\_}2007{\_}Posthumanism A critical history.pdf:pdf},
journal = {Medical Enhancements and Posthumanity},
pages = {1--29},
title = {{Posthumanism: A critical history}},
url = {http://www.researchgate.net/publication/226430836{\_}A{\_}Critical{\_}History{\_}of{\_}Posthumanism/file/32bfe50f41634817c7.pdf},
year = {2007}
}
@book{Questions,
author = {Questions, Fill-in-the-blank},
isbn = {4444444444444},
pages = {1--16},
title = {{Chapter 1 Overview and Descriptive Statistics}}
}
@book{Cormen2001,
author = {Cormen, Thomas and {Leiserson, Charles}, E. and Rivest, Ronald L. and Stein, Clifford},
booktitle = {Isbn: 0-262-03293- {\ldots}},
doi = {10.2307/2583667},
file = {:home/arnold/Documents/pdf-library/Library/Cormen et al.{\_}2001{\_}Introduction to Algorithms.pdf:pdf},
isbn = {0262032937},
issn = {01605682},
pmid = {16380287},
publisher = {MIT Press Cambridge, MA, USA},
title = {{Introduction to Algorithms}},
volume = {7},
year = {2001}
}
@book{Moler2005,
abstract = {This is a lively textbook for an introductory course in numerical methods, MATLAB, and technical computing, with an emphasis on the informed use of mathematical software. The presentation helps readers learn enough about the mathematical functions in MATLAB to use them correctly, appreciate their limitations, and modify them appropriately. The book makes extensive use of computer graphics, including interactive graphical expositions of numerical algorithms. It provides more than 70 M-files, which can be downloaded from the text Web site of the numerous exercises involve modifying and extending these programs. The topics covered include an introduction to MATLAB; linear equations; interpolation; zeros and roots; least squares; quadrature; ordinary differential equations; Fourier analysis; random numbers; eigenvalues and singular values; and partial differential equations. Motivating applications include modern problems from cryptography, touch-tone dialing, Google page-ranking, atmospheric science and image processing, as well as classical problems from physics and engineering.},
author = {Moler, Cleve},
booktitle = {Society for Industrial and Applied Mathematics},
doi = {10.5860/CHOICE.42-3475},
file = {:home/arnold/Documents/pdf-library/Library/Moler{\_}2005{\_}Numerical computing with MATLAB.pdf:pdf},
isbn = {6180339887498},
issn = {0009-4978},
number = {06},
pages = {42--3475--42--3475},
pmid = {12880804},
title = {{Numerical computing with MATLAB}},
url = {http://www.cro3.org/cgi/doi/10.5860/CHOICE.42-3475},
volume = {42},
year = {2005}
}
@book{Mendel,
author = {Mendel, Jerry M.},
file = {:home/arnold/Documents/pdf-library/Library/Mendel{\_}Unknown{\_}Lessons in Estimation Theory for Signal Processing Communications and Control.pdf:pdf},
isbn = {0131209817},
title = {{Lessons in Estimation Theory for Signal Processing Communications and Control}}
}
@book{Devore2008,
author = {Devore, Jay L.},
file = {:home/arnold/Documents/pdf-library/Library/Devore{\_}2008{\_}Probability and Statistics for Engineering and the Sciences.pdf:pdf},
isbn = {0495382175 9780495382171 9780495382232 049538223X},
pages = {735},
title = {{Probability and Statistics for Engineering and the Sciences}},
year = {2008}
}
@book{Lee2012,
abstract = {Computing the solution to Least Squares Problems is of great importance in a wide range of fields ranging from numerical linear algebra to econometrics and optimization. This paper aims to present numerically stable and computationally efficient algorithms for computing the solution to Least Squares Problems. In order to evaluate and compare the stability and efficiency of our proposed algorithms, the theoretical complexities and numerical results have been analyzed. Contents},
author = {Lee, Do Q},
file = {:home/arnold/Documents/pdf-library/Library/Lee{\_}2012{\_}Numerically Efficient Methods for Solving Least Squares Problems.pdf:pdf},
pages = {1--15},
title = {{Numerically Efficient Methods for Solving Least Squares Problems}},
year = {2012}
}
@book{Bishop2006,
abstract = {The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. A forthcoming companion volume will deal with practical aspects of pattern recognition and machine learning, and will include free software implementations of the key algorithms along with example data sets and demonstration programs. Christopher Bishop is Assistant Director at Microsoft Research Cambridge, and also holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, and was recently elected Fellow of the Royal Academy of Engineering. The author's previous textbook "Neural Networks for Pattern Recognition" has been widely adopted.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Bishop, Christopher M},
doi = {10.1117/1.2819119},
eprint = {0-387-31073-8},
file = {:home/arnold/Documents/pdf-library/Library/Bishop{\_}2006{\_}Pattern Recognition and Machine Learning.pdf:pdf},
isbn = {9780387310732},
issn = {10179909},
number = {4},
pages = {738},
pmid = {8943268},
title = {{Pattern Recognition and Machine Learning}},
url = {http://www.library.wisc.edu/selectedtocs/bg0137.pdf},
volume = {4},
year = {2006}
}
@book{RasmussenCarlEdwardWilliams2006,
abstract = {Gaussian processes (GPs) are natural generalisations of multivariate Gaussian random variables to infinite (countably or continuous) index sets. GPs have been applied in a large number of fields to a diverse range of ends, and very many deep theoretical analyses of various properties are available. This paper gives an introduction to Gaussian processes on a fairly elementary level with special emphasis on characteristics relevant in machine learning. It draws explicit connections to branches such as spline smoothing models and support vector machines in which similar ideas have been investigated. Gaussian process models are routinely used to solve hard machine learning problems. They are attractive because of their flexible non-parametric nature and computational simplicity. Treated within a Bayesian framework, very powerful statistical methods can be implemented which offer valid estimates of uncertainties in our predictions and generic model selection procedures cast as nonlinear optimization problems. Their main drawback of heavy computational scaling has recently been alleviated by the introduction of generic sparse approximations.13,78,31 The mathematical literature on GPs is large and often uses deep concepts which are not required to fully understand most machine learning applications. In this tutorial paper, we aim to present characteristics of GPs relevant to machine learning and to show up precise connections to other "kernel machines" popular in the community. Our focus is on a simple presentation, but references to more detailed sources are provided.},
archivePrefix = {arXiv},
arxivId = {026218253X},
author = {{Rasmussen, Carl Edward Williams}, Christopher K. I.},
doi = {10.1142/S0129065704001899},
eprint = {026218253X},
file = {:home/arnold/Documents/pdf-library/Library/Rasmussen, Carl Edward Williams{\_}2006{\_}Gaussian Processes for Machine Learning.pdf:pdf},
isbn = {026218253X},
issn = {0129-0657},
number = {2},
pages = {69--106},
pmid = {15112367},
publisher = {MIT Press Cambridge, MA, USA},
title = {{Gaussian Processes for Machine Learning}},
volume = {14},
year = {2006}
}
@article{Zhang2012,
author = {Zhang, Chicheng and Guti, E D and Asplund, Alexander and Pescatore, Linda},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Zhang et al.{\_}2012{\_}Tensor Decomposition for Topic Models An Overview and Implementation Tensor Decomposition Approach Intuition.pdf:pdf},
pages = {1--15},
title = {{Tensor Decomposition for Topic Models : An Overview and Implementation Tensor Decomposition Approach : Intuition}},
year = {2012}
}
@article{Anandkumar2015,
abstract = {This work considers a computationally and statistically efficient parameter estimation method for a wide class of latent variable models---including Gaussian mixture models, hidden Markov models, and latent Dirichlet allocation---which exploits a certain tensor structure in their low-order observable moments (typically, of second- and third-order). Specifically, parameter estimation is reduced to the problem of extracting a certain (orthogonal) decomposition of a symmetric tensor derived from the moments; this decomposition can be viewed as a natural generalization of the singular value decomposition for matrices. Although tensor decompositions are generally intractable to compute, the decomposition of these specially structured tensors can be efficiently obtained by a variety of approaches, including power iterations and maximization approaches (similar to the case of matrices). A detailed analysis of a robust tensor power method is provided, establishing an analogue of Wedin's perturbation theorem for the singular vectors of matrices. This implies a robust and computationally tractable estimation approach for several popular latent variable models.},
archivePrefix = {arXiv},
arxivId = {1210.7559},
author = {Anandkumar, Anima and Ge, Rong and Hsu, Daniel and Kakade, Sham M. and Telgarsky, Matus},
doi = {10.1007/978-3-319-24486-0_2},
eprint = {1210.7559},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Anandkumar et al.{\_}2015{\_}Tensor decompositions for learning latent variable models (A survey for ALT) Full Version.pdf:pdf},
isbn = {9783319244853},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {19--38},
title = {{Tensor decompositions for learning latent variable models (A survey for ALT) [Full Version]}},
volume = {9355},
year = {2015}
}
@article{Doshi-Velez2014,
abstract = {Originally designed to model text, topic modeling has become a powerful tool for uncovering latent structure in domains including medicine, finance, and vision. The goals for the model vary depending on the application: in some cases, the discovered topics may be used for prediction or some other downstream task. In other cases, the content of the topic itself may be of intrinsic scientific interest.   Unfortunately, even using modern sparse techniques, the discovered topics are often difficult to interpret due to the high dimensionality of the underlying space. To improve topic interpretability, we introduce Graph-Sparse LDA, a hierarchical topic model that leverages knowledge of relationships between words (e.g., as encoded by an ontology). In our model, topics are summarized by a few latent concept-words from the underlying graph that explain the observed words. Graph-Sparse LDA recovers sparse, interpretable summaries on two real-world biomedical datasets while matching state-of-the-art prediction performance.},
archivePrefix = {arXiv},
arxivId = {1410.4510},
author = {Doshi-Velez, Finale and Wallace, Byron and Adams, Ryan},
eprint = {1410.4510},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Doshi-Velez, Wallace, Adams{\_}2014{\_}Graph-Sparse LDA A Topic Model with Structured Sparsity.pdf:pdf},
keywords = {Novel Machine Learning Algorithms Track},
pages = {1--12},
title = {{Graph-Sparse LDA: A Topic Model with Structured Sparsity}},
url = {http://arxiv.org/abs/1410.4510},
year = {2014}
}
@article{Anandkumar2015,
abstract = {Topic modeling is a generalization of clustering that posits that observations (words in a document) are generated by multiple latent factors (topics), as opposed to just one. This increased representational power comes at the cost of a more challenging unsupervised learning problem of estimating the topic-word distributions when only words are observed, and the topics are hidden. This work provides a simple and efficient learning procedure that is guaranteed to recover the parameters for a wide class of topic models, including Latent Dirichlet Allocation (LDA). For LDA, the procedure correctly recovers both the topic-word distributions and the parameters of the Dirichlet prior over the topic mixtures, using only trigram statistics (i.e., third order moments, which may be estimated with documents containing just three words). The method, called Excess Correlation Analysis, is based on a spectral decomposition of low-order moments via two singular value decompositions (SVDs). Moreover, the algorithm is scalable, since the SVDs are carried out only on k  k matrices, where k is the number of latent factors (topics) and is typically much smaller than the dimension of the observation (word) space.},
archivePrefix = {arXiv},
arxivId = {1204.6703},
author = {Anandkumar, Anima and Foster, Dean P. and Hsu, Daniel and Kakade, Sham M. and Liu, Yi Kai},
doi = {10.1007/s00453-014-9909-1},
eprint = {1204.6703},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Anandkumar et al.{\_}2015{\_}A Spectral Algorithm for Latent Dirichlet Allocation.pdf:pdf},
isbn = {9781627480031},
issn = {14320541},
journal = {Algorithmica},
keywords = {Latent Dirichlet allocation,Method of moments,Mixture models,Topic models},
number = {1},
pages = {193--214},
title = {{A Spectral Algorithm for Latent Dirichlet Allocation}},
volume = {72},
year = {2015}
}
@incollection{Anandkumar2015a,
abstract = {26th International Conference, ALT 2015, Banff, AB, Canada, October 4-6, 2015, Proceedings},
address = {Banff, AB, Canada},
author = {Anandkumar, Anima and Ge, Rong and Hsu, Daniel and Kakade, Sham M. and Telgarsky, Matus},
booktitle = {Algorithmic Learning Theory},
doi = {10.1007/11564089},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Anandkumar et al.{\_}2015{\_}Tensor Decompositions for Learning Latent Variable Models (A Survey for ALT) Short Version.pdf:pdf},
isbn = {978-3-540-29242-5},
issn = {0302-9743},
keywords = {Theory {\&} Algorithms},
pages = {19--38},
publisher = {Springer International Publishing},
title = {{Tensor Decompositions for Learning Latent Variable Models (A Survey for ALT) [Short Version]}},
url = {http://eprints.pascal-network.org/archive/00002214/},
volume = {4},
year = {2015}
}
@article{Leonidas2007,
abstract = {In order to determine the role of subregions of the hippocampus in spatial working memory, this study combined selective neurotoxic lesions of the hippocampal subregions with a simple delayed nonmatching-to-place task on a radial maze in rats. Lesions of the dentate gyrus or the CA3, but not the CA1, subregion of the hippocampus induced a deficit in the acquisition of the task with short-term delays (i.e., 10 sec) and impaired performance of the task in a novel environment. All subregional lesions produced sustained impairment in performing the task with intermediate-term delays (i.e., 5 min) when rats were tested in a familiar environment. The results suggest a dynamic interaction among the dorsal hippocampal subregions in processing spatial working memory, with the time window (i.e., delay) of a task recognized as an essential controlling factor.},
author = {Leonidas, John C},
doi = {10.1148/radiol.2421060140},
issn = {00338419},
journal = {Radiology},
number = {1},
pages = {2},
pmid = {17185678},
title = {{Please wait ...}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17185678},
volume = {242},
year = {2007}
}
@article{KuhnHungarian,
author = {Kuhn, H W},
doi = {10.1002/nav.3800020109},
issn = {1931-9193},
journal = {Naval Research Logistics Quarterly},
number = {1-2},
pages = {83--97},
publisher = {Wiley Subscription Services, Inc., A Wiley Company},
title = {{The Hungarian method for the assignment problem}},
url = {http://dx.doi.org/10.1002/nav.3800020109},
volume = {2},
year = {1955}
}
@article{Rigby2010,
author = {Rigby, Paul and Pizarro, Oscar and William, Stefan B.},
doi = {10.1002/rob},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Rigby, Pizarro, William{\_}2010{\_}Toward Adaptive Benthic Habitat Mapping Using Gaussian Process Classificatio.pdf:pdf},
isbn = {9783902661623},
issn = {14746670},
journal = {Journal of Field Robotics},
keywords = {Backstepping,Non-linear control,Path-following},
number = {Special Issue: State of the Art in Maritime Autonomous Surface and Underwater Vehicles, Part 1},
pages = {741--758},
title = {{Toward Adaptive Benthic Habitat Mapping Using Gaussian Process Classificatio}},
volume = {27},
year = {2010}
}
@article{Paisley2014,
abstract = {We develop a nested hierarchical Dirichlet process (nHDP) for hierarchical topic modeling. The nHDP generalizes the nested Chinese restaurant process (nCRP) to allow each word to follow its own path to a topic node according to a document-specific distribution on a shared tree. This alleviates the rigid, single-path formulation assumed by the nCRP, allowing documents to easily express complex thematic borrowings. We derive a stochastic variational inference algorithm for the model, which enables efficient inference for massive collections of text documents. We demonstrate our algorithm on 1.8 million documents from The New York Times and 2.7 million documents from Wikipedia.},
archivePrefix = {arXiv},
arxivId = {1210.6738},
author = {Paisley, John and Wang, Chong and Blei, David and Jordan, Michael},
doi = {10.1109/TPAMI.2014.2318728},
eprint = {1210.6738},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Paisley et al.{\_}2014{\_}Nested Hierarchical Dirichlet Processes.pdf:pdf},
isbn = {0162-8828 VO - 37},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {read-sept1},
mendeley-tags = {read-sept1},
number = {99},
pages = {1--1},
title = {{Nested Hierarchical Dirichlet Processes}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6802355},
volume = {PP},
year = {2014}
}
@article{Bartcus2015,
author = {Bartcus, Marius},
title = {model-based clustering},
year = {2015}
}
@article{Armstrong2006,
abstract = {The benthic communities of the deep insular shelf at the Hind Bank marine conservation district (MCD), an important spawning aggregation site for groupers, were studied with the Seabed autonomous underwater vehicle (AUV) at depths between 32 and 54 m. Four digital phototransects provided data on benthic species composition and abundance of the insular shelf off St. Thomas, US Virgin Islands. Within the western side of the MCD, well-developed coral reefs with 43{\%} mean living coral cover were found. The Montastrea annularis complex was dominant at all four sites between 33 and 47 m, the depth range where reefs were present. Maximum coral cover found was 70{\%} at depths of 38-40 m. Quantitative determinations of sessile benthic populations, as well as the presence of motile megabenthic invertebrates and algae were obtained. The Seabed AUV provided new quantitative and descriptive information of a unique coral reef habitat found within this deeper insular shelf area. {\textcopyright} 2005 Elsevier Ltd. All rights reserved.},
author = {Armstrong, Roy a. and Singh, Hanumant and Torres, Juan and Nemeth, Richard S. and Can, Ali and Roman, Chris and Eustice, Ryan and Riggs, Lauren and Garcia-Moliner, Graciela},
doi = {10.1016/j.csr.2005.10.004},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Armstrong et al.{\_}2006{\_}Characterizing the deep insular shelf coral reef habitat of the Hind Bank marine conservation district (US Virgin.pdf:pdf},
isbn = {0278-4343},
issn = {02784343},
journal = {Continental Shelf Research},
keywords = {AUV,Atlantic,Coral reef,Deep hermatypic corals,US Virgin Islands},
number = {2},
pages = {194--205},
pmid = {686},
title = {{Characterizing the deep insular shelf coral reef habitat of the Hind Bank marine conservation district (US Virgin Islands) using the Seabed autonomous underwater vehicle}},
volume = {26},
year = {2006}
}
@article{Lambert2013,
abstract = {* Monitoring and assessment of the status and distribution of marine seabed habitats is needed to support existing and emerging environmental policy commitments. Traditional monitoring of habitats and associated species using grabs and trawls is costly and labour intensive and might usefully be complemented by cheaper and more readily automated methods that can be used at higher frequencies and/or on larger spatial scales. * We develop and apply two methods to measure seabed habitat complexity and demonstrate how they can be used to describe impacts (e.g. fishing gear impacts) and monitor recovery. The first method relies on the analysis of deviations in a laser line projected on the seabed. The second method is based on the pixel value distribution in seabed photographs. We use both methods to quantify the complexity created by different substrates and habitat-forming species and to establish links between habitat complexity and faunal diversity (richness) and abundance. * The habitat complexity index calculated with the laser line method provided a reliable index of complexity across a range of habitat types, showing a monotonic increase with coarseness of the substratum and the abundance of sessile epifauna. Pixel value distributions in the photographs did not reflect the increase in complexity due to sessile epifauna but only reflected substratum differences. * Results suggested that the laser line method would be suitable for monitoring the effect of disturbance on habitats ranging from gravelly sands to rock, and their subsequent recovery. The photographic method would be better suited to assessing complexity and heterogeneity of the substratum. Both methods complement conventional biological sampling and can be used at higher frequencies and/or on larger spatial scales per unit cost. * The laser line method has considerable potential to support demands for frequent monitoring of seabed habitats and human impacts at a range of spatial scales. It is less costly and labour intensive than existing approaches and can be deployed from vessels of many sizes.},
author = {Lambert, Gwladys I. and Jennings, Simon and Hinz, Hilmar and Murray, Lee G. and Lael, Parrott and Kaiser, Michel J. and Hiddink, Jan G.},
doi = {10.1111/2041-210x.12007},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Lambert et al.{\_}2013{\_}A comparison of two techniques for the rapid assessment of marine habitat complexity.pdf:pdf},
issn = {2041210X},
journal = {Methods in Ecology and Evolution},
keywords = {Biogenic habitat,Complexity,Epifauna,Fishing impact,Marine conservation,Monitoring},
number = {3},
pages = {226--235},
title = {{A comparison of two techniques for the rapid assessment of marine habitat complexity}},
volume = {4},
year = {2013}
}
@article{Seiler2012,
abstract = {We automatically mapped the distribution of temperate continental shelf rocky reef habitats with a high degree of confidence using colour, texture, rugosity and patchiness features extracted from images in conjunction with machine-learning algorithms. This demonstrated the potential of novel automation routines to expedite the complex and time-consuming process of seabed mapping. The random forests ensemble classifier outperformed other tree-based algorithms and also offered some valuable built-in model performance assessment tools. Habitat prediction using random forests performed most accurately when all 26 image-derived predictors were included in the model. This produced an overall habitat prediction accuracy of 84{\%} (with a kappa statistic of 0.793) when compared to nine distinct habitat classes assigned by a human annotator. Predictions for three habitat classes were all within the 95{\%} confidence intervals, indicating close agreement between observed and predicted habitat classes. Misclassified images were mostly unevenly, partially or insufficiently illuminated and came mostly from rugged terrains and during the autonomous underwater vehicle's obstacle avoidance manoeuvres. The remaining misclassified images were wrongly or inconsistently labelled by the human annotator. This study demonstrates the suitability of autonomous underwater vehicles to effectively sample benthic habitats and the ability of automated data handling techniques to extract and reliably process large volumes of seabed image data. Our methods for image feature extraction and classification are repeatable, cost-effective and well suited to studies that require non-extractive and/or co-located sampling, e.g. in marine reserves and for monitoring the recovery from physical impacts, e.g. from bottom fishing activities. The methods are transferable to other continental shelf areas and to other disciplines such as seabed geology. ?? 2012 Elsevier Ltd.},
author = {Seiler, Jan and Friedman, Ariell and Steinberg, Daniel and Barrett, Neville and Williams, Alan and Holbrook, Neil J.},
doi = {10.1016/j.csr.2012.06.003},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Seiler et al.{\_}2012{\_}Image-based continental shelf habitat mapping using novel automated data extraction techniques.pdf:pdf},
isbn = {0278-4343},
issn = {02784343},
journal = {Continental Shelf Research},
keywords = {Habitat predictors,Habitat variability,Monitoring,Random forests,Sirius,Tasmania},
pages = {87--97},
publisher = {Elsevier},
title = {{Image-based continental shelf habitat mapping using novel automated data extraction techniques}},
url = {http://dx.doi.org/10.1016/j.csr.2012.06.003},
volume = {45},
year = {2012}
}
@article{Lacharite2015,
abstract = {Autonomous and remotely operated underwater vehicles equipped with high-definition video and pho- tographic cameras are used to perform benthic surveys. These devices record fine-scale ({\textless} 1 m) seafloor features (seafloor complexity) and their local (10–100s m) variability (seafloor heterogeneity). Here, we introduce a methodology to efficiently process this optical imagery using object-based image analysis, which reduces the pixels in high-resolution digital images into a collection of “image-objects” of homoge- neous color and/or luminosity. This approach uses intuitive user-defined parameters and reproducible computer code, which aims to facilitate comparisons between habitats and geographic regions. We test this methodology with 511 images taken on the seafloor of a glaciated continental shelf (Gulf of Maine, northwest Atlantic), and describe three applications: (1) estimating percent cover of conspicuous epi- benthic fauna by building a Random Forest binary classifier assigning an identity to image-objects; (2) cor- relating image complexity (number of image-objects) with mean particle grain size; and (3) estimating seafloor heterogeneity from local variability in image complexity within and between two physiographic regions. Percent cover of epibenthic fauna estimated by the Random Forest binary classifier was in close agreement with the human visual assessment. Mean particle grain size (/ scale) was inversely correlated with image complexity (maximum Spearman's q520.89, p{\textless}0.01) with images dominated by pebbles, cobbles, boulders (low on / scale) yielding high image complexity. Predictive relationships of sediment composition were established using polynomial regression. Lastly, our approach could differentiate habi- tats within and between physiographic regions by using mean seafloor complexity and local variability along transects.},
author = {Lacharit{\'{e}}, Myriam and Metaxas, Anna and Lawton, Peter},
doi = {10.1002/lom3.10047},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Lacharit{\'{e}}, Metaxas, Lawton{\_}2015{\_}Using object-based image analysis to determine seafloor fine-scale features and complexity.pdf:pdf},
issn = {15415856},
journal = {Limnology and Oceanography: Methods},
number = {10},
pages = {553--567},
title = {{Using object-based image analysis to determine seafloor fine-scale features and complexity}},
url = {http://doi.wiley.com/10.1002/lom3.10047},
volume = {13},
year = {2015}
}
@article{Cairns2007,
author = {Cairns, Stephen D},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Cairns{\_}2007{\_}Deep Water Corals An Overview With Special Reference to Diversity and Distribution of Deep Water Scleractininan Corals.pdf:pdf},
journal = {Bulletin of Marine Science},
number = {3},
pages = {311--322},
title = {{Deep Water Corals : An Overview With Special Reference to Diversity and Distribution of Deep Water Scleractininan Corals}},
volume = {81},
year = {2007}
}
@article{Teixido2011,
abstract = {An important aspect of marine research is to quantify the areal coverage of benthic communities. It is technically feasible to efficiently obtain images of marine environments at different depths and benthic habitats over large spatial and temporal scales. Currently, there is a large and growing library of digital images to analyze, representing a valuable benthic ecological archive. Benthic coverage is the basis of studies on biodiversity, characterization of communities and evaluation of changes over temporal and spatial scales. However, there is still a lack of automatic or semi-automatic analytical methods for deriving ecologically relevant data from these images. We introduce a software program named Seascape to obtain semi-automatically segmented images (patch outlines) from underwater photographs of benthic communities, where each individual patch (species/categories) is routinely associated to its area cover and perimeter. Seascape is an analog to the classical and better known discipline of landscape ecology approach, which focuses on the concept that communities can be observed as a patch mosaic at any scale. The process starts with a hierarchical segmentation, using a color space criteria adapted to the problem of segmenting complex benthic images. As an endproduct, we obtain a set of images segmented into classified homogenous regions at different resolution levels (hierarchical segmentation). To illustrate the versatility and capacity of Seascape, we analyzed 4 digital images from different habitats and depths: coral reefs (Pacific Ocean), coralligenous communities (NW Mediterranean Sea), deep-water coral reefs (NW Mediterranean Sea) and the Antarctic continental shelf (Weddell Sea). The development of this semi-automatic outline tool and its use for classification constitute an important step forward in the analysis and processing time of underwater seabed images at any scale.},
author = {Teixid{\'{o}}, N{\'{u}}ria and Albajes-Eizagirre, Anton and Bolbo, Didier and {Le Hir}, Emilie and Demestre, Montse and Garrabou, Joaquim and Guigues, Laurent and Gili, Josep Maria and Piera, Jaume and Prelot, Thomas and Soria-Frisch, Aureli},
doi = {10.3354/meps09127},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Teixid{\'{o}} et al.{\_}2011{\_}Hierarchical segmentation-based software for cover classification analyses of seabed images (Seascape).pdf:pdf},
isbn = {0171-8630},
issn = {01718630},
journal = {Marine Ecology Progress Series},
keywords = {Area cover,Benthic communities,Digital photography,Hierarchical segmentation,Image analysis},
pages = {45--53},
title = {{Hierarchical segmentation-based software for cover classification analyses of seabed images (Seascape)}},
volume = {431},
year = {2011}
}
@article{Tong2012,
abstract = {Investigating the relationship between deep-water coral distribution and seabed topography is important for understanding the terrain habitat selection of these species and for the development of predictive habitat models. In this study, the distribution of the deep-water gorgonians, Paragorgia arborea and Primnoa resedaeformis, in relation to terrain variables at multiple scales of 30 m, 90 m and 170 m were investigated at R{\o}st Reef, Traena Reef and Sotbakken Reef on the Norwegian margin, with Ecological Niche Factor Analysis applied. To date, there have been few published studies investigating this aspect of gorgonian distribution. A similar correlation between the distribution of P. arborea and P. resedaeformis and each particular terrain variable was found at each study site, but the strength of the correlation between each variable and distribution differed by reef. The terrain variables of bathymetric position index (BPI) and curvature at analysis scales of 90 m or 170 m were most strongly linked to the distribution of both species at the three geographically distinct study sites. Both gorgonian species tended to inhabit local topographic highs across all three sites, particularly at Sotbakken Reef and Traena Reef, with both species observed almost exclusively on such topographic highs. The tendency for observed P. arborea to inhabit ridge crests at R{\o}st Reef was much greater than was indicated for P. resedaeformis. This investigation identifies the terrain variables which most closely correlate with distribution of these two gorgonian species, and analyzes their terrain habitat selection; further development of predictive habitat models may be considered essential for effective management of these species.},
author = {Tong, Ruiju and Purser, Autun and Unnithan, Vikram and Guinan, Janine},
doi = {10.1371/journal.pone.0043534},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Tong et al.{\_}2012{\_}Multivariate statistical analysis of distribution of deep-water gorgonian corals in relation to seabed topography on th.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
number = {8},
pages = {1--13},
pmid = {22912887},
title = {{Multivariate statistical analysis of distribution of deep-water gorgonian corals in relation to seabed topography on the norwegian margin}},
volume = {7},
year = {2012}
}
@article{Delezoide2011,
author = {Delezoide, Bertrand and Precioso, Frederic and Gosselin, Philippe and Redi, Miriam and Paris, S{\'{e}}bastien and Glotin, Herv{\'{e}}},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Delezoide et al.{\_}2011{\_}IRIM at TRECVID 2011 Semantic Indexing and Instance Search.pdf:pdf},
journal = {TRECVID Competition},
title = {{IRIM at TRECVID 2011: Semantic Indexing and Instance Search}},
year = {2011}
}
@article{DeLeo2014,
abstract = {The mapping of biodiversity on continental margins on landscape scales is highly relevant to marine spatial planning and conservation. Submarine canyons are widespread topographic features on continental and island margins that enhance benthic biomass across a range of oceanic provinces and productivity regimes. However, it remains unclear whether canyons enhance faunal biodiversity on landscape scales relevant to marine protected area (MPA) design. Furthermore, it is not known which physical attributes and heterogeneity metrics can provide good surrogates for large-scale mapping of canyon benthic biodiversity. To test mechanistic hypotheses evaluating the role of different canyon-landscape attributes in enhancing benthic biodiversity at different spatial scales we conducted 34 submersible dives in six submarine canyons and nearby slopes in the Hawaiian archipelago, sampling infaunal macrobenthos in a depth-stratified sampling design. We employed multivariate multiple regression models to evaluate sediment and topographic heterogeneity, canyon transverse profiles, and overall water mass variability as potential drivers of macrobenthic community structure and species richness. We find that variables related to habitat heterogeneity at medium (0.13km2) and large (15-33km2) spatial scales such as slope, backscatter reflectivity and canyon transverse profiles are often good predictors of macrobenthic biodiversity, explaining 16-30{\%} of the variance. Particulate organic carbon (POC) flux and distance from shore are also important variables, implicating food supply as a major predictor of canyon biodiversity. Canyons off the high Main Hawaiian Islands (Oahu and Moloka'i) are significantly affected by organic enrichment, showing enhanced infaunal macrobenthos abundance, whereas this effect is imperceptible around the low Northwest Hawaiian Islands (Nihoa and Maro Reef). Variable canyon alpha-diversity and high rates of species turnover (beta-diversity), particularly for polychaetes, suggest that canyons play important roles in maintaining high levels of regional biodiversity in the extremely oligotrophic system of the North Pacific Subtropical Gyre. This information is of key importance to the process of MPA design, suggesting that canyon habitats be explicitly included in marine spatial planning. ?? 2013 Elsevier Ltd.},
author = {{De Leo}, Fabio C. and Vetter, Eric W. and Smith, Craig R. and Rowden, Ashley a. and McGranaghan, Matthew},
doi = {10.1016/j.dsr2.2013.06.015},
file = {:home/arnold/Documents/MendeleyDesktop/Library/De Leo et al.{\_}2014{\_}Spatial scale-dependent habitat heterogeneity influences submarine canyon macrofaunal abundance and diversity off the.pdf:pdf;:home/arnold/Documents/MendeleyDesktop/De Leo et al.{\_}2014{\_}Spatial scale-dependent habitat heterogeneity influences submarine canyon macrofaunal abundance and diversity off (2).pdf:pdf},
isbn = {0967-0645},
issn = {09670645},
journal = {Deep-Sea Research Part II: Topical Studies in Oceanography},
keywords = {Biodiversity,Deep-sea,Habitat-types,Hawaii,Macrobenthos,Marine spatial planning,Multivariate analysis,Species turnover,Submarine canyons},
pages = {267--290},
publisher = {Elsevier},
title = {{Spatial scale-dependent habitat heterogeneity influences submarine canyon macrofaunal abundance and diversity off the Main and Northwest Hawaiian Islands}},
url = {http://dx.doi.org/10.1016/j.dsr2.2013.06.015},
volume = {104},
year = {2014}
}
@article{Leverette2005,
abstract = {Documentation of hundreds of locations for Canadian deep-water corals has been obtained through scientifi c initiatives and local fi shermen?s knowledge. Using these locations, as well as relevant oceanographic data, this study determined areas of suitable habitat for Paragorgia arborea and Primnoa resedaeformis along the Canadian Atlantic continental shelf and shelf break using predictive models. The study area included a band approximately 800 km long x 200 km wide from Cape Breton to the Gulf of Maine, and was chosen based on density of coral sites. Several environmental factors including slope, temperature, chlorophyll a, current speed and substrate may be important in determining suitable coral habitat and were included in the analysis. There are many different techniques used to model habitats, but frequently they are limited by the type of data available. Comparatively, few techniques using presence-only data are available. We utilized BioMapper, a program which uses Ecological Niche Factor Analysis (ENFA), to generate habitat suitability maps by relating data on species presence with background environmental data to determine the species? niche. We found that habitat requirements differed between the two species of coral. For Paragorgia arborea, the niche was highly specialized, and characterized by steeply sloped environments and rocky substrate. In contrast, for Primnoa resedaeformis, suitable habitat was more broadly distributed in the study area, and located in areas with high current speed, rocky substrates and an approximate temperature range between 5 and 10°C. This is the fi rst study to use predictive modelling to identify suitable habitat for deep-water coral, which may prove an important tool for the conservation of these organisms.},
author = {Leverette, Tanya L and Metaxas, Anna},
doi = {10.1007/3-540-27673-4_23},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Leverette, Metaxas{\_}2005{\_}Predicting habitat for two species of deep-water coral on the Canadian Atlantic continental shelf and slope.pdf:pdf;:home/arnold/Documents/MendeleyDesktop/Library/Leverette, Metaxas{\_}2005{\_}Predicting habitat for two species of deep-water coral on the Canadian Atlantic continental shelf and slope(2).pdf:pdf},
isbn = {3540241361},
journal = {Cold-water Corals and Ecosystems},
keywords = {BioMapper,Deep-water corals,Freiwald,Paragorgia arborea,Primnoa resedaeformis,mapping,predictive modelling},
pages = {467--479},
title = {{Predicting habitat for two species of deep-water coral on the Canadian Atlantic continental shelf and slope}},
year = {2005}
}
@article{Robert2014,
author = {Robert, Katleen and Jones, Daniel O.B. and Tyler, Paul a. and {Van Rooij}, David and a.I. Huvenne, Veerle},
doi = {10.1111/maec.12228},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Robert et al.{\_}2014{\_}Finding the hotspots within a biodiversity hotspot fine-scale biological predictions within a submarine canyon using.pdf:pdf;:home/arnold/Documents/MendeleyDesktop/Robert et al.{\_}2014{\_}Finding the hotspots within a biodiversity hotspot fine-scale biological predictions within a submarine canyon usi(2).pdf:pdf},
issn = {01739565},
journal = {Marine Ecology},
keywords = {biodiversity,deep-sea ecology,megafauna,predictive habitat modelling,submarine},
pages = {n/a--n/a},
title = {{Finding the hotspots within a biodiversity hotspot: fine-scale biological predictions within a submarine canyon using high-resolution acoustic mapping techniques}},
url = {http://doi.wiley.com/10.1111/maec.12228},
year = {2014}
}
@article{Greene1999,
abstract = {A standard, universally useful classification scheme for deepwater habitats needs to be established so that descriptions of these habitats can be accurately and efficiently applied among scientific disciplines In recent years many marine benthic habitats in deep water have been described using geophysical and biological data. These descriptions can vary from one investigator to another, which makes it difficult to compare habitats and associated biological assemblages among geographic regions. Using geophysical data collected with a variety of remote sensor systems and in situ biological and geologic observations, we have constructed a classification scheme that can be used in describing marine benthic habitats in deep water.},
author = {Greene, H. Gary and Yoklavich, Mary M. and Starr, Richard M. and O'Connell, Victoria M. and Wakefield, W. Waldo and Sullivan, Deidre E. and McRea, James E. and Cailliet, Gregor M.},
doi = {10.1016/S0399-1784(00)88957-4},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Greene et al.{\_}1999{\_}A classification scheme for deep seafloor habitats.pdf:pdf;:home/arnold/Documents/MendeleyDesktop/Library/Greene et al.{\_}1999{\_}A classification scheme for deep seafloor habitats.pdf:pdf},
isbn = {0399-1784},
issn = {03991784},
journal = {Oceanologica Acta},
keywords = {Benthic,Fisheries management,Habitat,Universal classification},
number = {6},
pages = {663--678},
pmid = {108},
title = {{A classification scheme for deep seafloor habitats}},
volume = {22},
year = {1999}
}
@article{Tittensor2009,
abstract = {ABSTRACT  Aim Globally, species distribution patterns in the deep sea are poorly resolved, with spatial coverage being sparse for most taxa and true absence data missing. Increasing human impacts on deep-sea ecosystems mean that reaching a better understanding of such patterns is becoming more urgent. Cold-water stony corals (Order Scleractinia) form structurally complex habitats (dense thickets or reefs) that can support a diversity of other associated fauna. Despite their widely accepted ecological importance, records of scleractinian corals on seamounts are patchy and simply not available for most of the global ocean. The objective of this paper is to model the global distribution of suitable habitat for stony corals on seamounts. Location Seamounts worldwide.  Methods We compiled a database containing all accessible records of scleractinian corals on seamounts. Two modelling approaches developed for presence-only data were used to predict global habitat suitability for seamount scleractinians: maximum entropy modelling (Maxent) and environmental niche factor analysis (ENFA). We generated habitat-suitability maps and used a crossvalidation process with a threshold-independent metric to evaluate the performance of the models.  Results Both models performed well in cross-validation, although the Maxent method consistently outperformed ENFA. Highly suitable habitat for seamount stony corals was predicted to occur at most modelled depths in the North Atlantic, and in a circumglobal strip in the Southern Hemisphere between 20  and 50  S and shallower than around 1500 m. Seamount summits in most other regions appeared much less likely to provide suitable habitat, except for small near-surface patches. The patterns of habitat suitability largely reflect current biogeographical knowledge. Environmental variables positively associated with high predicted habitat suitability included the aragonite saturation state, and oxygen saturation and concentration. By contrast, low levels of dissolved inorganic carbon, nitrate, phosphate and silicate were associated with high predicted suitability. High correlation among variables made assessing individual drivers difficult.  Main conclusions Our models predict environmental conditions likely to play a role in determining large-scale scleractinian coral distributions on seamounts, and provide a baseline scenario on a global scale. These results present a firstorder hypothesis that can be tested by further sampling. Given the high vulnerability of cold-water corals to human impacts, such predictions are crucial tools in developing worldwide conservation and management strategies for seamount ecosystems.},
author = {Tittensor, Derek P. and Baco, Amy R. and Brewin, Paul E. and Clark, Malcolm R. and Consalvey, Mireille and Hall-Spencer, Jason and Rowden, Ashley a. and Schlacher, Thomas and Stocks, Karen I. and Rogers, Alex D.},
doi = {10.1111/j.1365-2699.2008.02062.x},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Tittensor et al.{\_}2009{\_}Predicting global habitat suitability for stony corals on seamounts.pdf:pdf;:home/arnold/Documents/MendeleyDesktop/Library/Tittensor et al.{\_}2009{\_}Predicting global habitat suitability for stony corals on seamounts(2).pdf:pdf},
isbn = {1365-2699},
issn = {03050270},
journal = {Journal of Biogeography},
keywords = {Conservation biogeography,Distribution,ENFA,Habitat suitability,Maximum entropy,Modelling,Niche,Scleractinia,Seamount,Stony coral},
number = {6},
pages = {1111--1128},
title = {{Predicting global habitat suitability for stony corals on seamounts}},
volume = {36},
year = {2009}
}
@article{Davies2011,
abstract = {Predictive habitat models are increasingly being used by conservationists, researchers and governmental bodies to identify vulnerable ecosystems and species' distributions in areas that have not been sampled. However, in the deep sea, several limitations have restricted the widespread utilisation of this approach. These range from issues with the accuracy of species presences, the lack of reliable absence data and the limited spatial resolution of environmental factors known or thought to control deep-sea species' distributions. To address these problems, global habitat suitability models have been generated for five species of framework-forming scleractinian corals by taking the best available data and using a novel approach to generate high resolution maps of seafloor conditions. High-resolution global bathymetry was used to resample gridded data from sources such as World Ocean Atlas to produce continuous 30-arc second (∼1 km(2)) global grids for environmental, chemical and physical data of the world's oceans. The increased area and resolution of the environmental variables resulted in a greater number of coral presence records being incorporated into habitat models and higher accuracy of model predictions. The most important factors in determining cold-water coral habitat suitability were depth, temperature, aragonite saturation state and salinity. Model outputs indicated the majority of suitable coral habitat is likely to occur on the continental shelves and slopes of the Atlantic, South Pacific and Indian Oceans. The North Pacific has very little suitable scleractinian coral habitat. Numerous small scale features (i.e., seamounts), which have not been sampled or identified as having a high probability of supporting cold-water coral habitat were identified in all ocean basins. Field validation of newly identified areas is needed to determine the accuracy of model results, assess the utility of modelling efforts to identify vulnerable marine ecosystems for inclusion in future marine protected areas and reduce coral bycatch by commercial fisheries.},
author = {Davies, Andrew J. and Guinotte, John M.},
doi = {10.1371/journal.pone.0018483},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Davies, Guinotte{\_}2011{\_}Global habitat suitability for framework-forming cold-water corals.pdf:pdf;:home/arnold/Documents/MendeleyDesktop/Davies, Guinotte{\_}2011{\_}Global habitat suitability for framework-forming cold-water corals(3).pdf:pdf},
isbn = {1932-6203},
issn = {19326203},
journal = {PLoS ONE},
number = {4},
pmid = {21525990},
title = {{Global habitat suitability for framework-forming cold-water corals}},
volume = {6},
year = {2011}
}
@article{DeLeo2012,
abstract = {Submarine canyons are reported to be sites of enhanced fish biomass and productivity on continental margins. However, little is known about the effects of canyons on fish biodiversity, in particular on oceanic islands, which are imbedded in regions of low productivity. Using submersibles and high-definition video surveys, we investigated demersal fish assemblages in two submarine canyons and slope areas off the island of Moloka'i, Hawai'i, at depths ranging from 314 to 1100 m. We addressed the interactions between the abundance, species richness and composition of the fish assemblage, and organic matter input and habitat heterogeneity, testing the hypotheses that heterogeneous bottom habitats and higher organic matter input in canyons enhance demersal fish abundance, and species density, richness and diversity, thereby driving differences in assemblage structure between canyons and slopes. Sediment type, substrate inclination, water-mass properties (temperature and dissolved oxygen) and organic matter input (modeled POC flux and percent detritus occurrence) were put into multivariate multiple regression models to identify potential drivers of fish assemblage structure. A total of 824 fish were recorded during ∼13 h of video yielding 55 putative species. Macrouridae was the most diverse family with 13 species, followed by Congridae (5), Ophidiidae (4) and Halosauridae (3). Assemblage structure changed markedly with depth, with the most abrupt change in species composition occurring between the shallowest stratum (314–480 m) and intermediate and deep strata (571–719 m, 946–1100 m). Chlorophthalmus sp. dominated the shallow stratum, macrourids and synaphobranchid eels at intermediate depths, and halosaurs in the deepest stratum. Assemblages only differed significantly between canyon and slope habitats for the shallow stratum, and the deep stratum at one site. Dissolved oxygen explained the greatest proportion of variance in the multivariate data, followed by POC flux and percent organic-detritus occurrence. Fish abundances were generally higher in canyons but only statistically significant for the deepest stratum. Reduced fish abundances both in canyon and slope transects occurred at intermediate depths within the core of the oxygen minimum zone (OMZ). Species density, diversity and richness and abundance were usually higher in the canyons, but only statistically higher in the deepest stratum. Possible causes for increased abundance and species densities and richness in the deepest stratum in canyons include reduced disturbance at deeper depths. We conclude that submarine canyons on oceanic islands are likely to be sites of enhanced fish abundance and species richness, but that these enhancing effects are offset when oxygen concentrations fall below ∼0.7 ml l−1 in OMZs.},
author = {{De Leo}, Fabio C. and Drazen, Jeffrey C. and Vetter, Eric W. and Rowden, Ashley a. and Smith, Craig R.},
doi = {10.1016/j.dsr.2012.01.014},
file = {:home/arnold/Documents/MendeleyDesktop/Library/De Leo et al.{\_}2012{\_}The effects of submarine canyons and the oxygen minimum zone on deep-sea fish assemblages off Hawai'i.pdf:pdf;:home/arnold/Documents/MendeleyDesktop/De Leo et al.{\_}2012{\_}The effects of submarine canyons and the oxygen minimum zone on deep-sea fish assemblages off Hawai'i(2).pdf:pdf},
issn = {09670637},
journal = {Deep Sea Research Part I: Oceanographic Research Papers},
pages = {54--70},
title = {{The effects of submarine canyons and the oxygen minimum zone on deep-sea fish assemblages off Hawai'i}},
volume = {64},
year = {2012}
}
@article{Schlacher2010,
abstract = {Submarine canyons increase seascape diversity on continental margins and harbour diverse and abundant biota vulnerable to fishing. Because many canyons are fished, there is an increasing emphasis on including them in conservation areas on continental margins. Here we report on sponge diversity and bottom cover in three canyons of South-eastern Australia, test the performance of biological and abiotic surrogates, and evaluate how biological data from detailed seabed surveys can be used in conservation planning in these habitats. The biological data on sponge assemblage structure and species richness were obtained from 576 seafloor images taken between 148 and 472 m depth, yielding 65 morphospecies. Seafloor characteristics were similar within and between canyons, being almost exclusively composed of sediments with very few rocky substrates of higher relief. This environmental homogeneity did not, however, translate into biological uniformity of the megabenthos, and environmental factors were consequently poor predictors of biological features. By contrast, total bottom cover of sponges was highly correlated with species richness and served as a good proxy for species-level data in this situation. Design strategies that employ information on cover or richness of sponges provided a large dividend in conservation effort by dramatically reducing the number of spatial units required to achieve a specified conservation target of 50–90{\%} of species to be included in reserves. This demonstrates that image-derived data are useful for the design of reserves in the deep sea, particularly where extractive sampling is not warranted. Using biological data on the sponge megabenthos to identify conservation units can also minimise socio-economic costs to fisheries because of a smaller geographic and bathymetric ambit of conservation areas.},
author = {Schlacher, Thomas a. and Williams, Alan and Althaus, Franziska and Schlacher-Hoenlinger, Monika a.},
doi = {10.1111/j.1439-0485.2009.00286.x},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Schlacher et al.{\_}2010{\_}High-resolution seabed imagery as a tool for biodiversity conservation planning on continental margins.pdf:pdf;:home/arnold/Documents/MendeleyDesktop/Schlacher et al.{\_}2010{\_}High-resolution seabed imagery as a tool for biodiversity conservation planning on continental margins(2).pdf:pdf},
isbn = {1439-0485},
issn = {01739565},
journal = {Marine Ecology},
keywords = {Conservation planning,Continental margins,Deep-sea biodiversity,Marine protected areas,Sponges, megabenthos,Submarine canyons},
number = {1},
pages = {200--221},
title = {{High-resolution seabed imagery as a tool for biodiversity conservation planning on continental margins}},
volume = {31},
year = {2010}
}
@article{Davies2011a,
abstract = {Predictive habitat models are increasingly being used by conservationists, researchers and governmental bodies to identify vulnerable ecosystems and species' distributions in areas that have not been sampled. However, in the deep sea, several limitations have restricted the widespread utilisation of this approach. These range from issues with the accuracy of species presences, the lack of reliable absence data and the limited spatial resolution of environmental factors known or thought to control deep-sea species' distributions. To address these problems, global habitat suitability models have been generated for five species of framework-forming scleractinian corals by taking the best available data and using a novel approach to generate high resolution maps of seafloor conditions. High-resolution global bathymetry was used to resample gridded data from sources such as World Ocean Atlas to produce continuous 30-arc second (∼1 km(2)) global grids for environmental, chemical and physical data of the world's oceans. The increased area and resolution of the environmental variables resulted in a greater number of coral presence records being incorporated into habitat models and higher accuracy of model predictions. The most important factors in determining cold-water coral habitat suitability were depth, temperature, aragonite saturation state and salinity. Model outputs indicated the majority of suitable coral habitat is likely to occur on the continental shelves and slopes of the Atlantic, South Pacific and Indian Oceans. The North Pacific has very little suitable scleractinian coral habitat. Numerous small scale features (i.e., seamounts), which have not been sampled or identified as having a high probability of supporting cold-water coral habitat were identified in all ocean basins. Field validation of newly identified areas is needed to determine the accuracy of model results, assess the utility of modelling efforts to identify vulnerable marine ecosystems for inclusion in future marine protected areas and reduce coral bycatch by commercial fisheries.},
author = {Davies, Andrew J. and Guinotte, John M.},
doi = {10.1371/journal.pone.0018483},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Davies, Guinotte{\_}2011{\_}Global habitat suitability for framework-forming cold-water corals.pdf:pdf;:home/arnold/Documents/MendeleyDesktop/Davies, Guinotte{\_}2011{\_}Global habitat suitability for framework-forming cold-water corals(2).pdf:pdf},
isbn = {1932-6203},
issn = {19326203},
journal = {PLoS ONE},
number = {4},
pmid = {21525990},
title = {{Global habitat suitability for framework-forming cold-water corals}},
volume = {6},
year = {2011}
}
@article{Tittensor2009a,
abstract = {ABSTRACT  Aim Globally, species distribution patterns in the deep sea are poorly resolved, with spatial coverage being sparse for most taxa and true absence data missing. Increasing human impacts on deep-sea ecosystems mean that reaching a better understanding of such patterns is becoming more urgent. Cold-water stony corals (Order Scleractinia) form structurally complex habitats (dense thickets or reefs) that can support a diversity of other associated fauna. Despite their widely accepted ecological importance, records of scleractinian corals on seamounts are patchy and simply not available for most of the global ocean. The objective of this paper is to model the global distribution of suitable habitat for stony corals on seamounts. Location Seamounts worldwide.  Methods We compiled a database containing all accessible records of scleractinian corals on seamounts. Two modelling approaches developed for presence-only data were used to predict global habitat suitability for seamount scleractinians: maximum entropy modelling (Maxent) and environmental niche factor analysis (ENFA). We generated habitat-suitability maps and used a crossvalidation process with a threshold-independent metric to evaluate the performance of the models.  Results Both models performed well in cross-validation, although the Maxent method consistently outperformed ENFA. Highly suitable habitat for seamount stony corals was predicted to occur at most modelled depths in the North Atlantic, and in a circumglobal strip in the Southern Hemisphere between 20  and 50  S and shallower than around 1500 m. Seamount summits in most other regions appeared much less likely to provide suitable habitat, except for small near-surface patches. The patterns of habitat suitability largely reflect current biogeographical knowledge. Environmental variables positively associated with high predicted habitat suitability included the aragonite saturation state, and oxygen saturation and concentration. By contrast, low levels of dissolved inorganic carbon, nitrate, phosphate and silicate were associated with high predicted suitability. High correlation among variables made assessing individual drivers difficult.  Main conclusions Our models predict environmental conditions likely to play a role in determining large-scale scleractinian coral distributions on seamounts, and provide a baseline scenario on a global scale. These results present a firstorder hypothesis that can be tested by further sampling. Given the high vulnerability of cold-water corals to human impacts, such predictions are crucial tools in developing worldwide conservation and management strategies for seamount ecosystems.},
author = {Tittensor, Derek P. and Baco, Amy R. and Brewin, Paul E. and Clark, Malcolm R. and Consalvey, Mireille and Hall-Spencer, Jason and Rowden, Ashley a. and Schlacher, Thomas and Stocks, Karen I. and Rogers, Alex D.},
doi = {10.1111/j.1365-2699.2008.02062.x},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Tittensor et al.{\_}2009{\_}Predicting global habitat suitability for stony corals on seamounts.pdf:pdf;:home/arnold/Documents/MendeleyDesktop/Library/Tittensor et al.{\_}2009{\_}Predicting global habitat suitability for stony corals on seamounts(3).pdf:pdf},
isbn = {1365-2699},
issn = {03050270},
journal = {Journal of Biogeography},
keywords = {Conservation biogeography,Distribution,ENFA,Habitat suitability,Maximum entropy,Modelling,Niche,Scleractinia,Seamount,Stony coral},
number = {6},
pages = {1111--1128},
title = {{Predicting global habitat suitability for stony corals on seamounts}},
volume = {36},
year = {2009}
}
@article{DeLeo2014a,
abstract = {The mapping of biodiversity on continental margins on landscape scales is highly relevant to marine spatial planning and conservation. Submarine canyons are widespread topographic features on continental and island margins that enhance benthic biomass across a range of oceanic provinces and productivity regimes. However, it remains unclear whether canyons enhance faunal biodiversity on landscape scales relevant to marine protected area (MPA) design. Furthermore, it is not known which physical attributes and heterogeneity metrics can provide good surrogates for large-scale mapping of canyon benthic biodiversity. To test mechanistic hypotheses evaluating the role of different canyon-landscape attributes in enhancing benthic biodiversity at different spatial scales we conducted 34 submersible dives in six submarine canyons and nearby slopes in the Hawaiian archipelago, sampling infaunal macrobenthos in a depth-stratified sampling design. We employed multivariate multiple regression models to evaluate sediment and topographic heterogeneity, canyon transverse profiles, and overall water mass variability as potential drivers of macrobenthic community structure and species richness. We find that variables related to habitat heterogeneity at medium (0.13km2) and large (15-33km2) spatial scales such as slope, backscatter reflectivity and canyon transverse profiles are often good predictors of macrobenthic biodiversity, explaining 16-30{\%} of the variance. Particulate organic carbon (POC) flux and distance from shore are also important variables, implicating food supply as a major predictor of canyon biodiversity. Canyons off the high Main Hawaiian Islands (Oahu and Moloka'i) are significantly affected by organic enrichment, showing enhanced infaunal macrobenthos abundance, whereas this effect is imperceptible around the low Northwest Hawaiian Islands (Nihoa and Maro Reef). Variable canyon alpha-diversity and high rates of species turnover (beta-diversity), particularly for polychaetes, suggest that canyons play important roles in maintaining high levels of regional biodiversity in the extremely oligotrophic system of the North Pacific Subtropical Gyre. This information is of key importance to the process of MPA design, suggesting that canyon habitats be explicitly included in marine spatial planning. ?? 2013 Elsevier Ltd.},
author = {{De Leo}, Fabio C. and Vetter, Eric W. and Smith, Craig R. and Rowden, Ashley a. and McGranaghan, Matthew},
doi = {10.1016/j.dsr2.2013.06.015},
file = {:home/arnold/Documents/MendeleyDesktop/Library/De Leo et al.{\_}2014{\_}Spatial scale-dependent habitat heterogeneity influences submarine canyon macrofaunal abundance and diversity off the.pdf:pdf;:home/arnold/Documents/MendeleyDesktop/De Leo et al.{\_}2014{\_}Spatial scale-dependent habitat heterogeneity influences submarine canyon macrofaunal abundance and diversity off (3).pdf:pdf},
isbn = {0967-0645},
issn = {09670645},
journal = {Deep-Sea Research Part II: Topical Studies in Oceanography},
keywords = {Biodiversity,Deep-sea,Habitat-types,Hawaii,Macrobenthos,Marine spatial planning,Multivariate analysis,Species turnover,Submarine canyons},
pages = {267--290},
publisher = {Elsevier},
title = {{Spatial scale-dependent habitat heterogeneity influences submarine canyon macrofaunal abundance and diversity off the Main and Northwest Hawaiian Islands}},
url = {http://dx.doi.org/10.1016/j.dsr2.2013.06.015},
volume = {104},
year = {2014}
}
@article{Greene1999a,
abstract = {A standard, universally useful classification scheme for deepwater habitats needs to be established so that descriptions of these habitats can be accurately and efficiently applied among scientific disciplines In recent years many marine benthic habitats in deep water have been described using geophysical and biological data. These descriptions can vary from one investigator to another, which makes it difficult to compare habitats and associated biological assemblages among geographic regions. Using geophysical data collected with a variety of remote sensor systems and in situ biological and geologic observations, we have constructed a classification scheme that can be used in describing marine benthic habitats in deep water.},
author = {Greene, H. Gary and Yoklavich, Mary M. and Starr, Richard M. and O'Connell, Victoria M. and Wakefield, W. Waldo and Sullivan, Deidre E. and McRea, James E. and Cailliet, Gregor M.},
doi = {10.1016/S0399-1784(00)88957-4},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Greene et al.{\_}1999{\_}A classification scheme for deep seafloor habitats.pdf:pdf;:home/arnold/Documents/MendeleyDesktop/Library/Greene et al.{\_}1999{\_}A classification scheme for deep seafloor habitats.pdf:pdf},
isbn = {0399-1784},
issn = {03991784},
journal = {Oceanologica Acta},
keywords = {Benthic,Fisheries management,Habitat,Universal classification},
number = {6},
pages = {663--678},
pmid = {108},
title = {{A classification scheme for deep seafloor habitats}},
volume = {22},
year = {1999}
}
@article{Wang2007,
abstract = {In recent years, the language model Latent Dirichlet Allocation (LDA),$\backslash$nwhich clusters co-occurring words into topics, has been widely appled$\backslash$nin the computer vision field. However, many of these applications$\backslash$nhave difficulty with modeling the spatial and temporal structure$\backslash$namong visual words, since LDA assumes that a document is a “bag-of-words”.$\backslash$nIt is also critical to properly design “words” and “documents” when$\backslash$nusing a language model to solve vision problems. In this pa- per,$\backslash$nwe propose a topic model Spatial Latent Dirichlet Allocation (SLDA),$\backslash$nwhich better encodes spatial structure among visual words that are$\backslash$nessential for solving many vision problems. The spatial information$\backslash$nis not encoded in the value of visual words but in the design of$\backslash$ndocuments. Instead of knowing the partition of words into documents$\backslash$na priori, the word-document assignment becomes a random hidden variable$\backslash$nin SLDA. There is a generative procedure, where knowledge of spatial$\backslash$nstructure can be flexibly added as a prior, grouping visual words$\backslash$nwhich are close in space into the same document. We use SLDA to discover$\backslash$nobjects from a collection of images, and show it achieves better$\backslash$nperformance than LDA.},
author = {Wang, Xaiogang and Grimson, Eric},
journal = {Nips},
pages = {1--8},
title = {{Spatial Latent Dirichlet Allocation}},
year = {2007}
}
@article{Cao2007,
author = {Cao, L and Fei-Fei, L},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Cao, Fei-Fei{\_}2007{\_}Spatial coherent latent topic model for concurrent object segmentation and classification.pdf:pdf;:home/arnold/Documents/MendeleyDesktop/Cao, Fei-Fei{\_}2007{\_}Spatial coherent latent topic model for concurrent object segmentation and classification(2).pdf:pdf;:home/arnold/Documents/MendeleyDesktop/Cao, Fei-Fei{\_}2007{\_}Spatial coherent latent topic model for concurrent object segmentation and classification(3).pdf:pdf;:home/arnold/Documents/MendeleyDesktop/Cao, Fei-Fei{\_}2007{\_}Spatial coherent latent topic model for concurrent object segmentation and classification(4).pdf:pdf},
isbn = {9781424416318},
journal = {International Conference on Computer Vision},
title = {{Spatial coherent latent topic model for concurrent object segmentation and classification}},
year = {2007}
}
@article{Curtin2013,
abstract = {... Matthieu Brucher, Matthieu Perrot, and {\'{E}}douard Duch- esnay. Scikit - learn : Machine learning in Python. Journal of Machine Learning Research, 12: 2825–2830, 2011. Parikshit Ram and Alexander G. Gray. Density estimation trees. ... $\backslash$n},
archivePrefix = {arXiv},
arxivId = {1210.6293},
author = {Curtin, Ryan R and Cline, James R and Slagle, N P and March, William B and Ram, Parikshit and Mehta, Nishant a and Gray, Alexander G},
eprint = {1210.6293},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Curtin et al.{\_}2013{\_}MLPACK a scalable C machine learning library.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {The Journal of Machine Learning Research},
keywords = {Artificial Intelligence},
number = {1},
pages = {801--805},
title = {{MLPACK: a scalable C++ machine learning library}},
url = {http://dl.acm.org/citation.cfm?id=2567709.2502606{\%}5Cnpapers3://publication/uuid/9B33F4DA-F770-45BC-986E-B1461123A45A},
volume = {14},
year = {2013}
}
@article{Pietikainen1998,
abstract = {This chapter reviews and discusses various aspects of texture analysis. The concentration is on the various methods of extracting textural features from images. The geometric, random field, fractal, and signal processing models of texture are presented. The major classes of texture processing problems such as segmentation, classification, and shape from texture are discussed. The possible application areas of texture such as automated inspection, document processing, and remote sensing are summarized. A bibliography is provided at the end for further reading. Texture, segmentation, classification, shape, signal processing, fractals, random fields, Gabor filters, wavelet transform, gray level dependency matrix.},
author = {Pietikainen, Matti},
doi = {10.1142/9789812775320},
isbn = {9789812775320},
issn = {14707330},
journal = {World Scientific Publising Co.},
number = {July},
pages = {207--248},
pmid = {20605762},
title = {{Texture Analysis With Local Binary Patterns}},
year = {1998}
}
@article{Lee2007,
abstract = {Sparse coding provides a class of algorithms for finding succinct representations of stimuli; given only unlabeled input data, it discovers basis functions that capture higher-level features in the data. However, finding sparse codes remains a very difficult computational problem. In this paper, we present efficient sparse coding algorithms that are based on iteratively solving two convex optimization problems: an L1-regularized least squares problem and an L2-constrained least squares problem. We propose novel algorithms to solve both of these optimization problems. Our algorithms result in a significant speedup for sparse coding, allowing us to learn larger sparse codes than possible with previously described algorithms. We apply these algorithms to natural images and demonstrate that the inferred sparse codes exhibit end-stopping and non-classical receptive field surround suppression and, therefore, may provide a partial explanation for these two phenomena in V1 neurons. 1},
author = {Lee, Honglak and Battle, Alexis and Raina, Rajat and Ng, Andrew Y.},
doi = {10.1.1.69.2112},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Lee et al.{\_}2007{\_}Efficient sparse coding algorithms.pdf:pdf},
isbn = {0262195682},
issn = {10495258},
journal = {Advances in neural information {\ldots}},
number = {2},
pages = {801},
pmid = {17051527},
title = {{Efficient sparse coding algorithms}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.69.2112{\&}rep=rep1{\&}type=pdf{\%}5Cnhttp://books.nips.cc/papers/txt/nips19/NIPS2006{\_}0878.txt},
volume = {19},
year = {2007}
}
@article{Kaya2015,
author = {Kaya, Yılmaz and Ertuğrul, {\"{O}}mer Faruk and Tekin, Ramazan},
doi = {10.1016/j.asoc.2015.06.009},
issn = {15684946},
journal = {Applied Soft Computing},
keywords = {local binary patterns},
pages = {728--735},
title = {{Two novel local binary pattern descriptors for texture analysis}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1568494615003580},
volume = {34},
year = {2015}
}
@article{Liao2007,
abstract = {In this paper, we propose a novel representation, called Multi-scale Block Local Binary Pattern {\{}(MB-LBP),{\}} and apply it to face recognition. The Local Binary Pattern {\{}(LBP){\}} has been proved to be effective for image representation, but it is too local to be robust. In {\{}MB-LBP,{\}} the computation is done based on average values of block subregions, instead of individual pixels. In this way, {\{}MB-LBP{\}} code presents several advantages: (1) It is more robust than {\{}LBP;{\}} (2) it encodes not only microstructures but also macrostructures of image patterns, and hence provides a more complete image representation than the basic {\{}LBP{\}} operator; and (3) {\{}MB-LBP{\}} can be computed very efficiently using integral images. Furthermore, in order to reflect the uniform appearance of {\{}MB-LBP,{\}} we redefine the uniform patterns via statistical analysis. Finally, {\{}AdaBoost{\}} learning is applied to select most effective uniform {\{}MB-LBP{\}} features and construct face classifiers. Experiments on Face Recognition Grand Challenge {\{}(FRGC){\}} ver2.0 database show that the proposed {\{}MB-LBP{\}} method significantly outperforms other {\{}LBP{\}} based face recognition algorithms.},
author = {Liao, Shengcai and Zhu, Xiangxin and Lei, Zhen and Zhang, Lun and Li, Stan},
doi = {10.1007/978-3-540-74549-5_87},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Liao et al.{\_}2007{\_}Learning Multi-scale Block Local Binary Patterns for Face Recognition.pdf:pdf},
isbn = {978-3-540-74548-8},
issn = {0302-9743},
journal = {Advances in Biometrics},
keywords = {adaboost,face recognition,lbp,mb-lbp},
pages = {828--837},
title = {{Learning Multi-scale Block Local Binary Patterns for Face Recognition}},
year = {2007}
}
@article{Paris2010,
abstract = {This paper combines and proposes two novel multilevel spatial pyramidal (sp) features: spELBP (Extended Local Binary Pattern), spELBOP (Extended Local Binary Orientation Pattern) and spHOEE (Histogram of Oriented Edge Energy). These features feed state-of-the-art SVM algorithms for the localization of a robot in indoor environments. Two tasks are associated with the RobotVision@ICPR 2010 Challenge, the first one uses only a frame of stereoscopic images, the second takes into account the dynamics of the robot for improving results. Our scores are ranked 3{\textless}sup{\textgreater}rd{\textless}/sup{\textgreater} for Task1 and 1{\textless}sup{\textgreater}st{\textless}/sup{\textgreater} for Task2.},
author = {Paris, S{\'{e}}bastien and Glotin, Herv{\'{e}}},
doi = {10.1109/ICPR.2010.1143},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Paris, Glotin{\_}2010{\_}Pyramidal multi-level features for the RobotVision@ICPR 2010 challenge.pdf:pdf},
isbn = {9780769541099},
issn = {10514651},
journal = {Proceedings - International Conference on Pattern Recognition},
pages = {2949--2952},
pmid = {5595908},
title = {{Pyramidal multi-level features for the RobotVision@ICPR 2010 challenge}},
year = {2010}
}
@article{Paris2012a,
author = {Paris, S{\'{e}}bastien and Halkias, Xanadu and Glotin, Herv{\'{e}}},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Paris, Halkias, Glotin{\_}2012{\_}Sparse Coding for Histograms of Local Binary Patterns Applied for Image Categorization Toward a Bag-of-Scen.pdf:pdf},
isbn = {9784990644116},
issn = {1467322164},
journal = {International Conference on Pattern Recognition},
pages = {2817--2820},
title = {{Sparse Coding for Histograms of Local Binary Patterns Applied for Image Categorization : Toward a Bag-of-Scenes Analysis}},
year = {2012}
}
@article{Spampinato2015,
author = {Spampinato, C. and Pallazo, S. and Joalland, P.H. and Paris, S{\'{e}}bastien and Glotin, Herv{\'{e}}},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Spampinato et al.{\_}2015{\_}Fine-Grained Object Recognition in Underwater Visual Data.pdf:pdf},
title = {{Fine-Grained Object Recognition in Underwater Visual Data}},
year = {2015}
}
@article{Paris2012,
author = {Paris, Sebastien and Halkias, Xanadu and Glotin, Herv{\'{e}}},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Paris, Halkias, Glotin{\_}2012{\_}Efficient Bag of Scenes Analysis for Image Categorization.pdf:pdf},
isbn = {9789898565419},
journal = {International Conference on Pattern Recognition Applications and Methods},
keywords = {dictionary learning,fine-grained visual categorization,image categorization,lasso,linear,ltp,max-pooling,multi-scale lbp,non-parametric local,patterns,scenes categorization,sparse coding,spm},
title = {{Efficient Bag of Scenes Analysis for Image Categorization}},
year = {2012}
}
@article{Kelley2011,
abstract = {Sea ice extent in the Arctic Ocean diminished significantly during the first decade of the 2000s, most particularly in the Canada Basin where the loss of both multiyear and first-year ice was greater than in the other three subbasins. Using data collected during basin -wide ...},
author = {Kelley, Deborah S.},
doi = {10.5670/oceanog.2011.65},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Kelley{\_}2011{\_}Endeavour Segment of the Juan de Fuca Ridge.pdf:pdf},
isbn = {1042-8275},
issn = {10428275},
journal = {Oceanography},
number = {3},
pages = {162--173},
title = {{Endeavour Segment of the Juan de Fuca Ridge}},
url = {http://dx.doi.org/10.5670/oceanog.2011.80.},
volume = {24},
year = {2011}
}
@article{Glickson2007,
abstract = {[1] Detailed characterization of the Mothra Hydrothermal Field, the most southern and spatially extensive field on the Endeavour Segment of the Juan de Fuca Ridge, provides new insights into its geologic and hydrothermal development. Meter-scale bathymetry, side-scan sonar imagery, and direct dive observations show that Mothra is composed of six actively venting sulfide clusters spaced 40 - 200 m apart. Chimneys within each cluster have similar morphology and venting characteristics, and all clusters host a combination of active and extinct sulfide structures. Black smoker chimneys venting fluids above 300 degrees C are rare, while more common lower-temperature, diffusely venting chimneys support dense colonies of macrofauna and bacterial mat. Hydrothermal sediment and extinct sulfide debris cover 10 - 15 m of the seafloor surrounding each vent cluster, obscuring the underlying basaltic substrate of light to moderately sedimented pillow, lobate, sheet, and chaotic flows, basalt talus, and collapse terrain. Extinct sulfide chimneys and debris between the clusters indicate that hydrothermal flow was once more widespread and that it has shifted spatially over time. The most prominent structural features in the axial valley at Mothra are regional (020 degrees) trending faults and fissures and north-south trending collapse basins. The location of actively venting clusters within the field is controlled by ( 1) localization of fluid upflow along the western boundary fault zone, and diversion of these fluids by antithetic faults to feed vent clusters near the western valley wall, and ( 2) tapping of residual magmatic heat in the central part of the axial valley, which drives flow beneath vent clusters directly adjacent to the collapse basins 70 - 90 m east of the western valley wall. These processes form the basis for a model of axial valley and hydrothermal system development at Mothra, in which the field is initiated by an eruptive-diking episode and sustained through intense microseismicity and non-eruptive diking events.},
author = {Glickson, Deborah a. and Kelley, Deborah S. and Delaney, John R.},
doi = {10.1029/2007GC001588},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Glickson, Kelley, Delaney{\_}2007{\_}Geology and hydrothermal evolution of the mothra hydrothermal field, endeavour segment, Juan de Fuca Ridg.pdf:pdf},
isbn = {1525-2027},
issn = {15252027},
journal = {Geochemistry, Geophysics, Geosystems},
keywords = {Endeavour segment,Hydrothermal vents,Juan de Fuca Ridge,Seafloor geology,Seafloor morphology},
number = {6},
title = {{Geology and hydrothermal evolution of the mothra hydrothermal field, endeavour segment, Juan de Fuca Ridge}},
volume = {8},
year = {2007}
}
@article{Julesz1981,
abstract = {Research with texture pairs having identical second-order statistics has revealed that the pre-attentive texture discrimination system cannot globally process third- and higher-order statistics, and that discrimination is the result of a few local conspicuous features, called textons. It seems that only the first-order statistics of these textons have perceptual significance, and the relative phase between textons cannot be perceived without detailed scrutiny by focal attention.},
author = {Julesz, B},
doi = {10.1038/290091a0},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Julesz{\_}1981{\_}Textons, the elements of texture perception, and their interactions.pdf:pdf},
isbn = {0028-0836},
issn = {0028-0836},
journal = {Nature},
number = {5802},
pages = {91--97},
pmid = {7207603},
title = {{Textons, the elements of texture perception, and their interactions.}},
volume = {290},
year = {1981}
}
@article{Pizer1987,
abstract = {Adaptive histogram equalization (ahe) is a contrast enhancement method designed to be broadly applicable and having demonstrated effectiveness. However, slow speed and the overenhancement of noise it produces in relatively homogeneous regions are two problems. We report algorithms designed to overcome these and other concerns. These algorithms include interpolated ahe, to speed up the method on general purpose computers; a version of interpolated ahe designed to run in a few seconds on feedback processors; a version of full ahe designed to run in under one second on custom VLSI hardware; weighted ahe, designed to improve the quality of the result by emphasizing pixels' contribution to the histogram in relation to their nearness to the result pixel; and clipped ahe, designed to overcome the problem of overenhancement of noise contrast. We conclude that clipped ahe should become a method of choice in medical imaging and probably also in other areas of digital imaging, and that clipped ahe can be made adequately fast to be routinely applied in the normal display sequence.},
author = {Pizer, Stephen M. and Amburn, E. Philip and Austin, John D. and Cromartie, Robert and Geselowitz, Ari and Greer, Trey and {ter Haar Romeny}, Bart and Zimmerman, John B. and Zuiderveld, Karel},
doi = {10.1016/S0734-189X(87)80186-X},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Pizer et al.{\_}1987{\_}Adaptive histogram equalization and its variations.pdf:pdf},
isbn = {0734-189X},
issn = {0734189X},
journal = {Computer Vision, Graphics, and Image Processing},
number = {3},
pages = {355--368},
title = {{Adaptive histogram equalization and its variations}},
volume = {39},
year = {1987}
}
@article{Quiroga2006,
abstract = {The Teager-Kaiser operator is a discrete version of Teager's energy operator, advanced about 16 years ago. It is a filter of the moving window type and is commonly used as an estimator of the local energy contents of a signal; it is also used as a contrast enhancer of gray level images. We state some properties of a 2D version of the operator and its responses to common images. We characterize some of its root and preconstant images, and consider the case of separable images.},
author = {Quiroga, Julian and Restrepo, Alfredo and Wedefort, Lina and Velasco, Margarita},
doi = {10.1109/ICIP.2007.4379817},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Quiroga et al.{\_}2006{\_}On the 2D Teager-kaiser operator.pdf:pdf},
isbn = {1424414377},
issn = {15224880},
journal = {Proceedings - International Conference on Image Processing, ICIP},
keywords = {Image enhancement,Nonlinear filters},
pages = {269--272},
title = {{On the 2D Teager-kaiser operator}},
volume = {5},
year = {2006}
}
@article{Glotin,
author = {Glotin, Herv{\'{e}}},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Glotin{\_}Unknown{\_}Whale Cocktail Party Real-Time Multiple Tracking and Signal Analyses.pdf:pdf},
title = {{Whale Cocktail Party: Real-Time Multiple Tracking and Signal Analyses}}
}
@article{Boudraa2008,
abstract = {This paper describes a new method based on the 2D Teager- Kaiser Energy Operator (2DTKEO) for image contrast enhancement. The 2DTKEO reflects better the local activity than the amplitude of a classical edges detection operator. This quadratic filter is used to enhance high frequency information which is then combined with image gray values to estimate the edge strength value used in the enhancement process. This value is the average of the gray values by the energy activity at each pixel. Different examples of images are provided to demonstrate the Performance of the proposed method is demonstrated on synthetic and real images and the results compared to histogram equalization and to an edge- based contrast method.},
author = {a.-O. Boudraa and Diop, E.-H.S.},
doi = {10.1109/ICIP.2008.4712471},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Boudraa, Diop{\_}2008{\_}Image contrast enhancement based on 2D Teager-Kaiser operator.pdf:pdf},
isbn = {978-1-4244-1765-0},
issn = {1522-4880},
journal = {2008 15th IEEE International Conference on Image Processing},
keywords = {Image enhancement,Nonlinear filters,Operators},
pages = {3180--3183},
title = {{Image contrast enhancement based on 2D Teager-Kaiser operator}},
volume = {2},
year = {2008}
}
@article{Bengio2009a,
abstract = {Theoretical results suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g., in vision, language, and other AI-level tasks), one may need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This monograph discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.},
archivePrefix = {arXiv},
arxivId = {submit/0500581},
author = {Bengio, Yoshua},
doi = {10.1561/2200000006},
eprint = {0500581},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Bengio{\_}2009{\_}Learning Deep Architectures for AI.pdf:pdf},
isbn = {2200000006},
issn = {1935-8237},
journal = {Foundations and Trends{\textregistered} in Machine Learning},
number = {1},
pages = {1--127},
pmid = {17348934},
primaryClass = {submit},
title = {{Learning Deep Architectures for AI}},
volume = {2},
year = {2009}
}
@article{Hespanha2004,
author = {Hespanha, P},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Hespanha{\_}2004{\_}An Efficient MATLAB Algorithm for Graph Partitioning Technical Report Graph partitioning.pdf:pdf},
journal = {October},
pages = {1--8},
title = {{An Efficient MATLAB Algorithm for Graph Partitioning Technical Report Graph partitioning}},
url = {http://www.ece.ucsb.edu/{~}hespanha/published/tr-ell-gp.pdf},
year = {2004}
}
@article{Makhzani2013,
abstract = {Recently, it has been observed that when representations are learnt in a way that encourages sparsity, improved performance is obtained on classification tasks. These methods involve combinations of activation functions, sampling steps and different kinds of penalties. To investigate the effectiveness of sparsity by itself, we propose the k-sparse autoencoder, which is an autoencoder with linear activation function, where in hidden layers only the k highest activities are kept. When applied to the MNIST and NORB datasets, we find that this method achieves better classification results than denoising autoencoders, networks trained with dropout, and RBMs. k-sparse autoencoders are simple to train and the encoding stage is very fast, making them well-suited to large problem sizes, where conventional sparse coding algorithms cannot be applied.},
archivePrefix = {arXiv},
arxivId = {1312.5663},
author = {Makhzani, Alireza and Frey, Brendan},
eprint = {1312.5663},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Makhzani, Frey{\_}2013{\_}k-Sparse Autoencoders.pdf:pdf},
keywords = {()},
title = {{k-Sparse Autoencoders}},
url = {http://arxiv.org/abs/1312.5663},
year = {2013}
}
@article{Borghi,
author = {Borghi, Guido},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Borghi{\_}Unknown{\_}Autoencoder and k-Sparse Autoencoder with Caffe Libraries.pdf:pdf},
title = {{Autoencoder and k-Sparse Autoencoder with Caffe Libraries}}
}
@article{Krizhevsky2011,
abstract = {We show how to learn many layers of features on color images and we use these features to initialize deep autoencoders. We then use the autoencoders to map images to short binary codes. Using semantic hashing [6], 28-bit codes can be used to retrieve images that are similar to a query image in a time that is independent of the size of the database. This extremely fast retrieval makes it possible to search using multiple dierent transformations of the query image. 256-bit binary codes allow much more accurate matching and can be used to prune the set of images found using the 28-bit codes.},
author = {Krizhevsky, Alex and Hinton, Ge},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Krizhevsky, Hinton{\_}2011{\_}Using Very Deep Autoencoders for Content-Based Image Retrieval.pdf:pdf},
isbn = {9782874190445},
journal = {Esann},
title = {{Using Very Deep Autoencoders for Content-Based Image Retrieval}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.220.3054{\&}rep=rep1{\&}type=pdf},
year = {2011}
}
@article{Hinton2006,
abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such "autoencoder" networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
author = {Hinton, G E and Salakhutdinov, R R},
doi = {10.1126/science.1127647},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Hinton, Salakhutdinov{\_}2006{\_}Reducing the dimensionality of data with neural networks.pdf:pdf},
isbn = {3135786504},
issn = {0036-8075},
journal = {Science (New York, N.Y.)},
number = {5786},
pages = {504--507},
pmid = {16873662},
title = {{Reducing the dimensionality of data with neural networks.}},
volume = {313},
year = {2006}
}
@article{Vincent2008,
abstract = {Previous work has shown that the difficul- ties in learning deep generative or discrim- inative models can be overcome by an ini- tial unsupervised learning step that maps in- puts to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a rep- resentation based on the idea of making the learned representations robust to partial cor- ruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to ini- tialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising ad- vantage of corrupting the input of autoen- coders on a pattern classification benchmark suite.},
author = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
doi = {10.1145/1390156.1390294},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Vincent et al.{\_}2008{\_}Extracting and composing robust features with denoising autoencoders.pdf:pdf},
isbn = {978-1-60558-205-4},
journal = {Proceedings of the 25th international conference on Machine learning - ICML '08},
pages = {1096--1103},
title = {{Extracting and composing robust features with denoising autoencoders}},
url = {http://portal.acm.org/citation.cfm?doid=1390156.1390294},
year = {2008}
}
@article{Wan2012,
author = {Wan, L and Zhu, Leo and Fergus, R},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Wan, Zhu, Fergus{\_}2012{\_}A Hybrid Neural Network-Latent Topic Model.pdf:pdf},
journal = {Aistats},
pages = {1287--1294},
title = {{A Hybrid Neural Network-Latent Topic Model}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/AISTATS2012{\_}WanZF12.pdf},
volume = {22},
year = {2012}
}
@article{Fier2014,
author = {Fier, Ryan and Hoeberechts, Maia},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Fier, Hoeberechts{\_}2014{\_}Automatic Fish Counting System for Noisy Deep-Sea Videos.pdf:pdf},
isbn = {9781479949182},
keywords = {computer vision, deep-sea video, underwater video,},
title = {{Automatic Fish Counting System for Noisy Deep-Sea Videos}},
year = {2014}
}
@article{Hase1999,
abstract = {We propose a new method that removes snowfall noise from
successive images recorded by a TV camera. This method utilizes the size
and velocity of a moving object. The intensity of a pixel changes when
an object passes over the pixel. Snow particles pass a pixel very
quickly due to their small size. Therefore, after holding the odd number
of video frames, they are sorted by intensity in each pixel
independently. Then we can obtain the background intensity of the pixel
by taking the median intensity from them. This was confirmed by
experiments. We also proposed a stochastic model of this algorithm. By
comparison between a measurement value and the model, we obtained the
agreement with each other. Furthermore, we consider the effect for the
other moving objects. Experimental results are also shown},
author = {Hase, H. and Miyake, K. and Yoneda, M.},
doi = {10.1109/ICIP.1999.822927},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Hase, Miyake, Yoneda{\_}1999{\_}Real-time snowfall noise elimination.pdf:pdf},
isbn = {0-7803-5467-2},
journal = {Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348)},
number = {M},
pages = {406--409},
title = {{Real-time snowfall noise elimination}},
volume = {2},
year = {1999}
}
@article{Yang2009,
abstract = {Recently SVMs using spatial pyramid matching (SPM) kernel have been highly successful in image classification. Despite its popularity, these nonlinear SVMs have a complexity O(n2 {\~{}} n3) in training and O(n) in testing, where n is the training size, implying that it is nontrivial to scaleup the algorithms to handle more than thousands of training images. In this paper we develop an extension of the SPM method, by generalizing vector quantization to sparse coding followed by multi-scale spatial max pooling, and propose a linear SPM kernel based on SIFT sparse codes. This new approach remarkably reduces the complexity of SVMs to O(n) in training and a constant in testing. In a number of image categorization experiments, we find that, in terms of classification accuracy, the suggested linear SPM based on sparse coding of SIFT descriptors always significantly outperforms the linear SPM kernel on histograms, and is even better than the nonlinear SPM kernels, leading to state-of-the-art performance on several benchmarks by using a single type of descriptors.},
author = {Yang, Jianchao and Yu, Kai and Gong, Yihong and Huang, T},
doi = {10.1109/CVPR.2009.5206757},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Yang et al.{\_}2009{\_}Linear spatial pyramid matching using sparse coding for image classification.pdf:pdf},
isbn = {1063-6919 VO -},
issn = {1063-6919},
journal = {Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on},
keywords = {Computational complexity,Histograms,Image classification,Image coding,Image representation,Image segmentation,Kernel,SIFT descriptor,SIFT sparse codes,SPM kernel,Scanning probe microscopy,Testing,Vector quantization,computational complexity,image categorization,image classification,image matching,linear spatial pyramid matching,multiscale spatial max pooling,nonlinear SVM,sparse coding,support vector machines,training images,vector quantisation,vector quantization},
pages = {1794--1801},
title = {{Linear spatial pyramid matching using sparse coding for image classification}},
year = {2009}
}
@article{Chamroukhi2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1501.03347v1},
author = {Chamroukhi, Faicel and Bartcus, Marius and Glotin, Herv{\'{e}}},
eprint = {arXiv:1501.03347v1},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Chamroukhi, Bartcus, Glotin{\_}2015{\_}Dirichlet Process Parsimonious Mixtures for clustering.pdf:pdf},
keywords = {bayesian model,bayesian nonparametric learning,dirichlet,model-based clustering,parsimonious mixtures,process mixtures},
number = {0},
title = {{Dirichlet Process Parsimonious Mixtures for clustering}},
volume = {33},
year = {2015}
}
@article{Wallach2009,
abstract = {A natural evaluation metric for statistical topic models is the probability$\backslash$nof held-out documents given a trained model. While exact computation$\backslash$nof this probability is intractable, several estimators for this probability$\backslash$nhave been used in the topic modeling literature, including the harmonic$\backslash$nmean method and empirical likelihood method. In this paper, we demonstrate$\backslash$nexperimentally that commonly-used methods are unlikely to accurately$\backslash$nestimate the probability of held-out documents, and propose two alternative$\backslash$nmethods that are both accurate and efficient.},
author = {Wallach, Hanna M. and Murray, Iain and Salakhutdinov, Ruslan and Mimno, David},
doi = {10.1145/1553374.1553515},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Wallach et al.{\_}2009{\_}Evaluation Methods for Topic Models.pdf:pdf},
isbn = {9781605585161},
journal = {International Conference on Machine Learning},
keywords = {LDA Evaluation,LDA Theory},
mendeley-tags = {LDA Evaluation,LDA Theory},
number = {d},
pages = {1--8},
title = {{Evaluation Methods for Topic Models}},
url = {http://portal.acm.org/citation.cfm?doid=1553374.1553515},
year = {2009}
}
@article{Girdhar2012,
abstract = {We are interested in the task of online summarization of the data observed by a mobile robot, with the goal that these summaries could be then be used for applications such as surveillance, identifying samples to be collected by a planetary rover, and site inspections to detect anomalies. In this paper, we pose the summarization problem as an instance of the well known k-center problem, where the goal is to identify k observations so that the maximum distance of any observation from a summary sample is minimized. We focus on the online version of the summarization problem, which requires that the decision to add an incoming observation to the summary be made instantaneously. Moreover, we add the constraint that only a finite number of observed samples can be saved at any time, which allows for applications where the selection of a sample is linked to a physical action such as rock sample collection by a planetary rover. We show that the proposed online algorithm has performance comparable to the offline algorithm when used with real world data.},
author = {Girdhar, Yogesh and Dudek, Gregory},
doi = {10.1109/ICRA.2012.6224657},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Girdhar, Dudek{\_}2012{\_}Efficient on-line data summarization using extremum summaries.pdf:pdf},
isbn = {9781467314039},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
keywords = {LDA Evaluation},
mendeley-tags = {LDA Evaluation},
pages = {3490--3496},
title = {{Efficient on-line data summarization using extremum summaries}},
year = {2012}
}
@phdthesis{Girdhar2014,
author = {Girdhar, Yogesh},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Girdhar{\_}2014{\_}Unsupervised Semantic Perception, Summarization, and Autonomous Exploration for Robots in Unstructured Environments.pdf:pdf},
keywords = {BoW,LDA Evaluation,LDA Theory,ROST},
mendeley-tags = {BoW,LDA Evaluation,LDA Theory,ROST},
number = {July},
school = {McGill University},
title = {{Unsupervised Semantic Perception, Summarization, and Autonomous Exploration for Robots in Unstructured Environments}},
year = {2014}
}
@article{Girdhar2014a,
abstract = {We present a robotic exploration technique in which the goal is to learn a visual model that can be used to distinguish between different terrains and other visual com- ponents in an unknown environment. We use ROST, a realtime online spatiotemporal topic modeling framework to model these terrains using the observations made by the robot, and then use an information theoretic path planning technique to define the exploration path.We conduct experiments with aerial view and underwater datasets with millions of observations and varying path lengths, and find that paths that are biased towards locations with high topic perplexity produce better terrain models with high discriminative power.},
archivePrefix = {arXiv},
arxivId = {1310.6767},
author = {Girdhar, Yogesh and Whitney, David and Dudek, Gregory},
doi = {10.1109/ICRA.2014.6906913},
eprint = {1310.6767},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Girdhar, Whitney, Dudek{\_}2014{\_}Curiosity based exploration for learning terrain models.pdf:pdf},
isbn = {9781479936847},
issn = {10504729},
journal = {2014 IEEE International Conference on Robotics and Automation},
keywords = {LDA Evaluation,ROST,Terrain Modelling},
mendeley-tags = {LDA Evaluation,ROST,Terrain Modelling},
pages = {578--584},
title = {{Curiosity based exploration for learning terrain models}},
year = {2014}
}
@article{Kalmbach2013,
author = {Kalmbach, Arnold and Girdhar, Yogesh and Dudek, Gregory},
doi = {10.1109/ICRA.2013.6630948},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Kalmbach, Girdhar, Dudek{\_}2013{\_}Unsupervised environment recognition and modeling using sound sensing.pdf:pdf},
isbn = {9781467356411},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
keywords = {ROST},
mendeley-tags = {ROST},
pages = {2699--2704},
title = {{Unsupervised environment recognition and modeling using sound sensing}},
year = {2013}
}
@article{Girdhar2011,
abstract = {The idea of an online visual vocabulary is proposed. In contrast to the accepted strategy of generating vocabularies offline, using the k-means clustering over all the features extracted form all the images in a dataset, an online vocabulary is dynamic and evolves iteratively over time as new observations are made. Hence, it is much more suitable for online robotic applications, such as exploration, landmark detection, and SLAM, where the future is unknown. We present two different strategies for building online vocabularies. The first strategy produces a vocabulary, which optimizes the k-centres objective of minimizing the maximum distance of a a feature from the closest vocabulary word. The second strategy produces a vocabulary by randomly sampling from the current vocabulary and the features in the current observation. We show that both the algorithms are able to produce distance matrices which have positive rank correlation with distance matrices computed using an offline k-means vocabulary. We discover that the online random vocabulary is consistently effective at approximating the behaviour of the offline k-means vocabulary, at least for the moderate sized datasets we examine.},
author = {Girdhar, Yogesh and Dudek, Gregory},
doi = {10.1109/CRV.2011.32},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Girdhar, Dudek{\_}2011{\_}Online visual vocabularies.pdf:pdf},
isbn = {9780769543628},
journal = {Proceedings - 2011 Canadian Conference on Computer and Robot Vision, CRV 2011},
keywords = {BoW,bag-of-words,visual vocabulary},
mendeley-tags = {BoW},
pages = {191--196},
title = {{Online visual vocabularies}},
year = {2011}
}
@misc{Bullinaria2004,
author = {Bullinaria, John A.},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Bullinaria{\_}2004{\_}Self Organizing Maps Fundamentals.pdf:pdf},
keywords = {SOM,neural networks,self organizing maps,tutorial},
mendeley-tags = {SOM,neural networks,self organizing maps,tutorial},
pages = {1--15},
title = {{Self Organizing Maps : Fundamentals}},
year = {2004}
}
@article{Chang2009,
abstract = {Probabilistic topic models are a popular tool for the unsupervised analysis of text, providing both a predictive model of future text and a latent topic representation of the corpus. Practitioners typically assume that the latent space is semantically meaningful. It is used to check models, summarize the corpus, and guide exploration of its contents. However, whether the latent space is interpretable is in need of quantitative evaluation. In this paper, we present new quantitative methods for measuring semantic meaning in inferred topics. We back these measures with large-scale user studies, showing that they capture aspects of the model that are undetected by previous measures of model quality based on held-out likelihood. Surprisingly, topic models which perform better on held-out likelihood may infer less semantically meaningful topics.},
author = {Chang, Jonathan and Gerrish, Sean and Wang, Chong and Blei, David M},
doi = {10.1.1.100.1089},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Chang et al.{\_}2009{\_}Reading Tea Leaves How Humans Interpret Topic Models.pdf:pdf},
isbn = {9781615679119},
journal = {Advances in Neural Information Processing Systems 22},
keywords = {LDA Evaluation},
mendeley-tags = {LDA Evaluation},
pages = {288----296},
title = {{Reading Tea Leaves: How Humans Interpret Topic Models}},
year = {2009}
}
@article{Kohonen1990,
abstract = {The self-organized map, an architecture suggested for artificial neural networks, is explained by presenting simulation experiments and practical applications. The self-organizing map has the property of effectively creating spatially organized internal representations of various features of input signals and their abstractions. One result of this is that the self-organization process can discover semantic relationships in sentences. Brain maps, semantic maps, and early work on competitive learning are reviewed. The self-organizing map algorithm (an algorithm which order responses spatially) is reviewed, focusing on best matching cell selection and adaptation of the weight vectors. Suggestions for applying the self-organizing map algorithm, demonstrations of the ordering process, and an example of hierarchical clustering of data are presented. Fine tuning the map by learning vector quantization is addressed. The use of self-organized maps in practical speech recognition and a simulation experiment on semantic mapping are discussed.},
author = {Kohonen, T.},
doi = {10.1109/5.58325},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Kohonen{\_}1990{\_}The self-organizing map.pdf:pdf},
isbn = {0018-9219},
issn = {0018-9219},
journal = {Proceedings of the IEEE},
keywords = {SOM,neural networks,self organizing maps},
mendeley-tags = {SOM,neural networks,self organizing maps},
number = {9},
pages = {1464--1480},
pmid = {21447162},
title = {{The self-organizing map}},
volume = {78},
year = {1990}
}
@article{Kivinen2007,
abstract = {We develop nonparametric Bayesian models for multiscale representations of images depicting natural scene categories. Individual features or wavelet coefficients are marginally described by Dirichlet process (DP) mixtures, yielding the heavy-tailed marginal distributions characteristic of natural images. Dependencies between features are then captured with a hidden Markov tree, and Markov chain Monte Carlo methods used to learn models whose latent state space grows in complexity as more images are observed. By truncating the potentially infinite set of hidden states, we are able to exploit efficient belief propagation methods when learning these hierarchical Dirichlet process hidden Markov trees (HDP-HMTs) from data. We show that our generative models capture interesting qualitative structure in natural scenes, and more accurately categorize novel images than models which ignore spatial relationships among features.},
author = {Kivinen, Jyri J. and Sudderth, Erik B. and Jordan, Michael I.},
doi = {10.1109/ICCV.2007.4408870},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Kivinen, Sudderth, Jordan{\_}2007{\_}Learning multiscale representations of natural scenes using dirichlet processes.pdf:pdf},
isbn = {978-1-4244-1631-8},
issn = {1550-5499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
keywords = {read-sept1},
mendeley-tags = {read-sept1},
title = {{Learning multiscale representations of natural scenes using dirichlet processes}},
year = {2007}
}
@techreport{Rublee2012,
abstract = {Feature matching is at the base of many computer vi- sion problems, such as object recognition or structure from motion. Current methods rely on costly descriptors for de- tection and matching. In this paper, we propose a very fast binary descriptor based on BRIEF, called ORB, which is rotation invariant and resistant to noise. We demonstrate through experiments how ORB is at two orders of magni- tude faster than SIFT, while performing as well in many situations. The efficiency is tested on several real-world ap- plications, including object detection and patch-tracking on a smart phone.},
author = {Rublee, Ethan and Bradski, Gary},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Rublee, Bradski{\_}2012{\_}ORB an efﬁcient alternative to SIFT or SURF.pdf:pdf},
keywords = {Ethan Rublee Vincent Rabaud Kurt Konolige Gary Bra,Image Feature Extraction},
mendeley-tags = {Image Feature Extraction},
title = {{ORB: an efﬁcient alternative to SIFT or SURF}},
url = {http://www.willowgarage.com/sites/default/files/orb{\_}final.pdf},
year = {2012}
}
@article{Honig2015,
author = {Honig, Peter and Stewart, Charles and Gallager, Scott and York, Amber and Hole, Woods},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Honig et al.{\_}2015{\_}Automated Optical Classification and Segmentation of Marine Substrate.pdf:pdf},
keywords = {Oceanography,Underwater CV},
mendeley-tags = {Oceanography,Underwater CV},
title = {{Automated Optical Classification and Segmentation of Marine Substrate}},
year = {2015}
}
@article{Rosten2006,
abstract = {Where feature points are used in real-time frame-rate applications, a high-speed feature detector is necessary. Feature detectors such as SIFT (DoG), Harris and SUSAN are good methods which yield high quality features, however they are too computationally intensive for use in real-time applications of any complexity. Here we show that machine learning can be used to derive a feature detector which can fully process live PAL video using less than 7{\%} of the available processing time. By comparison neither the Harris detector (120{\%}) nor the detection stage of SIFT (300{\%}) can operate at full frame rate. Clearly a high-speed detector is of limited use if the features produced are unsuitable for downstream processing. In particular, the same scene viewed from two different positions should yield features which correspond to the same real-world 3D locations [1]. Hence the second contribution of this paper is a comparison corner detectors based on this criterion applied to 3D scenes. This comparison supports a number of claims made elsewhere concerning existing corner detectors. Further, contrary to our initial expectations, we show that despite being principally constructed for speed, our detector significantly outperforms existing feature detectors according to this criterion.},
author = {Rosten, Edward and Drummond, Tom},
doi = {10.1007/11744023_34},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Rosten, Drummond{\_}2006{\_}Machine learning for high-speed corner detection.pdf:pdf},
isbn = {3540338322},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Image Feature Extraction},
mendeley-tags = {Image Feature Extraction},
pages = {430--443},
pmid = {18684738},
title = {{Machine learning for high-speed corner detection}},
volume = {3951 LNCS},
year = {2006}
}
@techreport{Frigyik2010,
abstract = {This tutorial covers the Dirichlet distribution, Dirichlet process, P{\'{o}}lya urn (and the associated Chinese restaurant process), hierarchical Dirichlet Process, and the Indian buffet process. Apart from basic properties, we describe and contrast three methods of generating samples: stick-breaking, the P{\'{o}}lya urn, and drawing gamma random variables. For the Dirichlet process we first present an informal introduction, and then a rigorous description for those more comfortable with probability theory.},
author = {Frigyik, Bela a and Kapila, Amol and Gupta, Maya R},
booktitle = {Electrical Engineering},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Frigyik, Kapila, Gupta{\_}2010{\_}Introduction to the Dirichlet Distribution and Related Processes.pdf:pdf},
isbn = {0132066920},
issn = {01987097},
keywords = {LDA Theory},
mendeley-tags = {LDA Theory},
number = {206},
pages = {46--48,50--52},
pmid = {15988873},
title = {{Introduction to the Dirichlet Distribution and Related Processes}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Introduction+to+the+Dirichlet+Distribution+and+Related+Processes{\#}0},
volume = {27},
year = {2010}
}
@article{Mimno2011,
abstract = {Real document collections do not fit the independence assumptions asserted by most statistical topic models, but how badly do they violate them? We present a Bayesian method for measuring how well a topic model fits a corpus. Our approach is based on posterior predictive checking, a method for diagnosing Bayesian models in user-defined ways. Our method can identify where a topic model fits the data, where it falls short, and in which directions it might be improved.},
author = {Mimno, David and Blei, David},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Mimno, Blei{\_}2011{\_}Bayesian Checking for Topic Models.pdf:pdf},
isbn = {978-1-937284-11-4},
journal = {Proceedings of the Conference on Empirical Methods in Natural Language Processing},
keywords = {LDA Evaluation,LDA Theory,topic models},
mendeley-tags = {LDA Evaluation,LDA Theory},
pages = {227--237},
title = {{Bayesian Checking for Topic Models}},
url = {http://dl.acm.org/citation.cfm?id=2145432.2145459},
year = {2011}
}
@article{Schoening2012,
abstract = {Megafauna play an important role in benthic ecosystem function and are sensitive indicators of environmental change. Non-invasive monitoring of benthic communities can be accomplished by seafloor imaging. However, manual quantification of megafauna in images is labor-intensive and therefore, this organism size class is often neglected in ecosystem studies. Automated image analysis has been proposed as a possible approach to such analysis, but the heterogeneity of megafaunal communities poses a non-trivial challenge for such automated techniques. Here, the potential of a generalized object detection architecture, referred to as iSIS (intelligent Screening of underwater Image Sequences), for the quantification of a heterogenous group of megafauna taxa is investigated. The iSIS system is tuned for a particular image sequence (i.e. a transect) using a small subset of the images, in which megafauna taxa positions were previously marked by an expert. To investigate the potential of iSIS and compare its results with those obtained from human experts, a group of eight different taxa from one camera transect of seafloor images taken at the Arctic deep-sea observatory HAUSGARTEN is used. The results show that inter- and intra-observer agreements of human experts exhibit considerable variation between the species, with a similar degree of variation apparent in the automatically derived results obtained by iSIS. Whilst some taxa (e. g. Bathycrinus stalks, Kolga hyalina, small white sea anemone) were well detected by iSIS (i. e. overall Sensitivity: 87{\%}, overall Positive Predictive Value: 67{\%}), some taxa such as the small sea cucumber Elpidia heckeri remain challenging, for both human observers and iSIS.},
author = {Schoening, Timm and Bergmann, Melanie and Ontrup, J{\"{o}}rg and Taylor, James and Dannheim, Jennifer and Gutt, Julian and Purser, Autun and Nattkemper, Tim W.},
doi = {10.1371/journal.pone.0038179},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Schoening et al.{\_}2012{\_}Semi-automated image analysis for the assessment of megafaunal densities at the Artic deep-sea observatory HAUSGAR.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
keywords = {Oceanography,Underwater CV},
mendeley-tags = {Oceanography,Underwater CV},
number = {6},
pages = {1--14},
pmid = {22719868},
title = {{Semi-automated image analysis for the assessment of megafaunal densities at the Artic deep-sea observatory HAUSGARTEN}},
volume = {7},
year = {2012}
}
@article{Lee1996,
abstract = {This paper extends to two dimensions the frame criterion developed by Daubechies for one-dimensional wavelets, and it computes the frame bounds for the particular case of 2D Gabor wavelets. Completeness criteria for 2D Gabor image representations are important because of their increasing role in many computer vision applications and also in modeling biological vision, since recent neurophysiological evidence from the visual cortex of mammalian brains suggests that the filter response profiles of the main class of linearly-responding cortical neurons (called simple cells) are best modeled as a family of self-similar 2D Gabor wavelets. We therefore derive the conditions under which a set of continuous 2D Gabor wavelets will provide a complete representation of any image, and we also find self-similar wavelet parametrization which allow stable reconstruction by summation as though the wavelets formed an orthonormal basis. Approximating a “tight frame” generates redundancy which allows low-resolution neural responses to represent high-resolution images},
annote = {This paper contains the standard mathematical model of the way simple cells do edge detection using gabor wavelets. It spends a whole bunch of time deriving a family of gabor functions and their parameters such that when you apply take the gabor wavelet transform on an image, you get a complete representation.},
author = {Lee, Tai Sing},
doi = {10.1109/34.541406},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Lee{\_}1996{\_}Image representation using 2d gabor wavelets.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Coarse coding,Gabor wavelets,Image Feature Extraction,Image reconstruction,Image representation,Texton,Visual cortex},
mendeley-tags = {Image Feature Extraction,Texton},
number = {10},
pages = {959--971},
title = {{Image representation using 2d gabor wavelets}},
volume = {18},
year = {1996}
}
@article{Schoening2014,
author = {Schoening, Timm and Kuhn, Thomas and Nattkemper, Tim W.},
doi = {10.1109/CVAUI.2014.9},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Schoening, Kuhn, Nattkemper{\_}2014{\_}Seabed Classification Using a Bag-of-Prototypes Feature Representation.pdf:pdf},
isbn = {978-1-4799-6713-1},
journal = {2014 ICPR Workshop on Computer Vision for Analysis of Underwater Imagery},
keywords = {BoW,HSOM,Oceanography,SOM,Underwater CV,feature representation,neural networks,resource exploration,underwater image analysis},
mendeley-tags = {BoW,HSOM,Oceanography,SOM,Underwater CV,neural networks},
pages = {17--24},
title = {{Seabed Classification Using a Bag-of-Prototypes Feature Representation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6961264},
year = {2014}
}
@article{Edu2014,
abstract = {Topic models such as the latent Dirichlet allocation (LDA) have become a standard staple in the modeling toolbox of machine learning. They have been applied to a vast variety of data sets, contexts, and tasks to varying degrees of success. However, to date there is almost no formal theory explicating the LDA's behavior, and despite its familiarity there is very little systematic analysis of and guidance on the properties of the data that affect the inferential performance of the model. This paper seeks to address this gap, by providing a systematic analysis of factors which characterize the LDA's performance. We present theorems elucidating the posterior contraction rates of the topics as the amount of data increases, and a thorough supporting empirical study using synthetic and real data sets, including news and web-based articles and tweet messages. Based on these results we provide practical guidance on how to identify suitable data sets for topic models, and how to specify particular model parameters.},
author = {Edu, Qmei Umich},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Edu{\_}2014{\_}Understanding the Limiting Factors of Topic Modeling via Posterior Contraction Analysis.pdf:pdf},
journal = {International Conference on Machine Learning},
keywords = {LDA Evaluation,LDA Theory},
mendeley-tags = {LDA Evaluation,LDA Theory},
title = {{Understanding the Limiting Factors of Topic Modeling via Posterior Contraction Analysis}},
volume = {32},
year = {2014}
}
@article{Nowak2006,
author = {Nowak, Eric and Jurie, Frederic and Triggs, Bill},
doi = {10.1007/11744085_38},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Nowak, Jurie, Triggs{\_}2006{\_}Sampling Strategies for Bag-of-Features.pdf:pdf},
journal = {Eccv},
keywords = {BoW,Image Feature Extraction},
mendeley-tags = {BoW,Image Feature Extraction},
pages = {490--503},
title = {{Sampling Strategies for Bag-of-Features}},
volume = {3954},
year = {2006}
}
@article{Bay2006,
abstract = {In this paper, we present a novel scale- and rotation-invariant interest point detector and descriptor, coined SURF (Speeded Up Robust Features). It approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster. This is achieved by relying on integral images for image convolutions; by building on the strengths of the leading existing detectors and descriptors (in casu, using a Hessian matrix-based measure for the detector, and a distribution-based descriptor); and by simplifying these methods to the essential. This leads to a combination of novel detection, description, and matching steps. The paper presents experimental results on a standard evaluation set, as well as on imagery obtained in the context of a real-life object recognition application. Both show SURF's strong performance.},
author = {Bay, Herbert and Tuytelaars, Tinne and {Van Gool}, Luc},
doi = {10.1007/11744023_32},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Bay, Tuytelaars, Van Gool{\_}2006{\_}SURF Speeded up robust features.pdf:pdf},
isbn = {3540338322},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Image Feature Extraction},
mendeley-tags = {Image Feature Extraction},
pages = {404--417},
pmid = {16081019},
title = {{SURF: Speeded up robust features}},
volume = {3951 LNCS},
year = {2006}
}
@article{Perronnin2007,
abstract = {Within the field of pattern classification, the Fisher kernel is a powerful framework which combines the strengths of generative and discriminative approaches. The idea is to characterize a signal with a gradient vector derived from a generative probability model and to subsequently feed this representation to a discriminative classifier. We propose to apply this framework to image categorization where the input signals are images and where the underlying generative model is a visual vocabulary: a Gaussian mixture model which approximates the distribution of low-level features in images. We show that Fisher kernels can actually be understood as an extension of the popular bag-of-visterms. Our approach demonstrates excellent performance on two challenging databases: an in-house database of 19 object/scene categories and the recently released VOC 2006 database. It is also very practical: it has low computational needs both at training and test time and vocabularies trained on one set of categories can be applied to another set without any significant loss in performance.},
author = {Perronnin, Florent and Dance, Christopher},
doi = {10.1109/CVPR.2007.383266},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Perronnin, Dance{\_}2007{\_}Fisher kernels on visual vocabularies for image categorization.pdf:pdf},
isbn = {1424411807},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
title = {{Fisher kernels on visual vocabularies for image categorization}},
year = {2007}
}
@article{Reed2012,
abstract = {The aim of this tutorial is to introduce the reader to Latent Dirichlet Allocation (LDA) for topic modeling. This tutorial is not all-inclusive and should be accompanied/cross- referenced with Blei et al. (2003). The unique aspect of this tutorial is that I provide a full pseudo-code implementation of variational expectation-maximization LDA and an R code implementation at http://highenergy.physics.uiowa.edu/{\~{}}creed/{\#}code. The R code is arguably the simplest variational expectation-maximization LDA implementation I've come across. Unfortunately, the simple implementation makes it very slow and unrealistic for actual application, but it's designed to serve as an educational tool},
author = {Reed, Colorado},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Reed{\_}2012{\_}Latent Dirichlet Allocation Towards a Deeper Understanding.pdf:pdf},
keywords = {LDA Implementation,LDA Theory,Tutorial},
mendeley-tags = {LDA Implementation,LDA Theory,Tutorial},
number = {January},
pages = {1--13},
title = {{Latent Dirichlet Allocation : Towards a Deeper Understanding}},
year = {2012}
}
@incollection{Paris2015,
abstract = {In this paper, we address the general problem of image/object categorization with a novel approach referred to as Bag-of-Scenes (BoS). Our approach is efficient for both low semantic applications, such as texture classification and higher semantic tasks such as natural scenes recognition. It is based on the widely used combination of (i) Sparse coding (Sc), (ii) Max-pooling and (iii) Spatial Pyramid Matching (SPM) techniques applied to histograms of multi-scale Local Binary/Ternary Patterns (LBP/LTP) as local features. This approach can be considered as a two-layer hierarchical architecture. The first layer encodes quickly the local spatial patch structure via histograms of LBP/LTP, while the second layer encodes the relationships between pre-analyzed LBP/LTP-scenes/objects. In order to provide comparative results, we also introduce an alternate 2-layer architecture. For this latter, the first layer is encoding directly the multi-scale Differential Vectors (DV) local patches instead of histograms of LBP/LTP. Our method outperforms SIFT-based approaches using Sc techniques and can be trained efficiently with a simple linear SVM. Our BoS method achieves 87.46{\%}, and 90.35{\%} of accuracy for Scene-15, UIUC-Sport datasets respectively.},
author = {Paris, S{\'{e}}bastien and Halkias, Xanadu and Glotin, Herv{\'{e}}},
booktitle = {Pattern Recognition Applications and Methods},
doi = {10.1007/978-3-319-12610-4_12},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Paris, Halkias, Glotin{\_}2015{\_}Beyond SIFT for Image Categorization by Bag-of-Scenes Analysis.pdf:pdf},
keywords = {BoW,Image Feature Extraction},
mendeley-tags = {BoW,Image Feature Extraction},
pages = {191--207},
title = {{Beyond SIFT for Image Categorization by Bag-of-Scenes Analysis}},
url = {http://link.springer.com/chapter/10.1007{\%}2F978-3-319-12610-4{\_}12 http://link.springer.com/10.1007/978-3-319-12610-4{\_}12},
volume = {204},
year = {2015}
}
@article{Dolan2008,
abstract = {Video data and high-resolution multibeam bathymetry were acquired using a Remotely Operated Vehicle (ROV) on the flank of a carbonate mound (???850 m depth) in the Porcupine Seabight, SW Ireland. The ROV-mounted multibeam system revealed details of bathymetry that were not resolved by ship-borne multibeam survey, but appear to be important in structuring the distribution of the cold-water corals Lophelia pertusa and Madrepora oculata. Quantitative measures of slope, orientation, roughness and curvature were calculated from the ROV multibeam bathymetry data across a range of spatial scales. These parameters were analysed for their ecological relevance to the distribution of the corals and used in an Ecological Niche Factor Analysis (ENFA) to identify the most suitable areas for coral colonisation within the extent of our ROV multibeam data. The suitability map covers an area nine times the size of the area imaged directly by video. Cross-validation of the results with video data indicates that the predictions are reliable. This combined survey and modelling approach offers a comprehensive method for ground-truthing discrete seabed features such as mounds. It provides spatial context to high-resolution deep-water video observations and highlights the importance of bathymetric variables in influencing coral distribution. ?? 2008 Elsevier Ltd. All rights reserved.},
author = {Dolan, Margaret F J and Grehan, Anthony J. and Guinan, Janine C. and Brown, Colin},
doi = {10.1016/j.dsr.2008.06.010},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Dolan et al.{\_}2008{\_}Modelling the local distribution of cold-water corals in relation to bathymetric variables Adding spatial context to d.pdf:pdf},
isbn = {0967-0637},
issn = {09670637},
journal = {Deep-Sea Research Part I: Oceanographic Research Papers},
keywords = {Cold-water coral,ENFA,Habitat suitability modelling,Multibeam bathymetry,Underwater video,Unmanned underwater vehicle},
number = {11},
pages = {1564--1579},
title = {{Modelling the local distribution of cold-water corals in relation to bathymetric variables: Adding spatial context to deep-sea video data}},
volume = {55},
year = {2008}
}
@article{Lowe2004,
author = {Lowe, David G},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Lowe{\_}2004{\_}Distinctive Image Features from Scale-Invariant Keypoints.pdf:pdf},
journal = {International Journal of Computer Vision},
keywords = {Image Feature Extraction,SIFT},
mendeley-tags = {Image Feature Extraction,SIFT},
pages = {1--28},
title = {{Distinctive Image Features from Scale-Invariant Keypoints}},
year = {2004}
}
@misc{Alahi2012,
author = {Alahi, Alexandre and Ortiz, Raphael and Vandergheynst, Pierre},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Alahi, Ortiz, Vandergheynst{\_}2012{\_}FREAK Fast Retina Keypoint.pdf:pdf},
isbn = {978-1-4673-1226-4},
keywords = {Image Feature Extraction},
mendeley-tags = {Image Feature Extraction},
title = {{FREAK: Fast Retina Keypoint}},
url = {http://infoscience.epfl.ch/record/175537},
year = {2012}
}
@article{Leutenegger2011,
abstract = {Effective and efficient generation of keypoints from an image is a well-studied problem in the literature and forms the basis of numerous Computer Vision applications. Established leaders in the field are the SIFT and SURF algorithms which exhibit great performance under a variety of image transformations, with SURF in particular considered as the most computationally efficient amongst the high-performance methods to date. In this paper we propose BRISK1, a novel method for keypoint detection, description and matching. A comprehensive evaluation on benchmark datasets reveals BRISK's adaptive, high quality performance as in state-of-the-art algorithms, albeit at a dramatically lower computational cost (an order of magnitude faster than SURF in cases). The key to speed lies in the application of a novel scale-space FAST-based detector in combination with the assembly of a bit-string descriptor from intensity comparisons retrieved by dedicated sampling of each keypoint neighborhood.},
author = {Leutenegger, Stefan and Chli, Margarita and Siegwart, Roland Y.},
doi = {10.1109/ICCV.2011.6126542},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Leutenegger, Chli, Siegwart{\_}2011{\_}BRISK Binary Robust invariant scalable keypoints.pdf:pdf},
isbn = {9781457711015},
issn = {1550-5499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
keywords = {Image Feature Extraction},
mendeley-tags = {Image Feature Extraction},
pages = {2548--2555},
title = {{BRISK: Binary Robust invariant scalable keypoints}},
year = {2011}
}
@inproceedings{Boiman2008,
abstract = {State-of-the-art image classification methods require an intensive learning/training stage (using SVM, Boosting, etc.) In contrast, non-parametric Nearest-Neighbor (NN) based image classifiers require no training time and have other favorable properties. However, the large performance gap between these two families of approaches rendered NN- based image classifiers useless. We claim that the effectiveness of non-parametric NN- based image classification has been considerably under- valued. We argue that two practices commonly used in im- age classification methods, have led to the inferior perfor- mance of NN-based image classifiers: (i) Quantization of local image descriptors (used to generate “bags-of-words”, codebooks). (ii) Computation of ‘Image-to-Image' dis- tance, instead of ‘Image-to-Class' distance. We propose a trivial NN-based classifier – NBNN, (Naive-Bayes Nearest-Neighbor), which employs NN- distances in the space of the local image descriptors (and not in the space of images). NBNN computes direct ‘Image- to-Class' distances without descriptor quantization. We fur- ther show that under the Naive-Bayes assumption, the theo- retically optimal image classifier can be accurately approx- imated by NBNN. Although NBNN is extremely simple, efficient, and re- quires no learning/training phase, its performance ranks among the top leading learning-based image classifiers. Empirical comparisons are shown on several challenging databases (Caltech-101,Caltech-256 and Graz-01)},
author = {Boiman, Oren and Shechtman, Eli and Irani, Michal},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2008.4587598},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Boiman, Shechtman, Irani{\_}2008{\_}In Defense of Nearest-Neighbor Based Image Classification.pdf:pdf},
isbn = {9781424422432},
keywords = {BoW},
mendeley-tags = {BoW},
pages = {1--8},
title = {{In Defense of Nearest-Neighbor Based Image Classification}},
year = {2008}
}
@article{Thomsen2012,
abstract = {The NEPTUNE Canada cabled observatory network enables non-destructive, controlled experiments and time-series observations with mobile robots on gas hydrates and benthic community structure on a small plateau of about 1 km2 at a water depth of 870 m in Barkley Canyon, about 100 km offshore Vancouver Island, British Columbia. A mobile Internet operated vehicle was used as an instrument platform to monitor and study up to 2000 m2 of sediment surface in real-time. In 2010 the first mission of the robot was to investigate the importance of oscillatory deep ocean currents on methane release at continental margins. Previously, other experimental studies have indicated that methane release from gas hydrate outcrops is diffusion-controlled and should be much higher than seepage from buried hydrate in semipermeable sediments. Our results show that periods of enhanced bottom currents associated with diurnal shelf waves, internal semidiurnal tides, and also wind-generated near-inertial motions can modulate methane seepage. Flow dependent destruction of gas hydrates within the hydrate stability field is possible from enhanced bottom currents when hydrates are not covered by either seafloor biota or sediments. The calculated seepage varied between 40{\&}{\#}8211;400 {\&}{\#}956;mol CH4 m{\&}{\#}8722;2 s{\&}{\#}8722;1. This is 1{\&}{\#}8211;3 orders of magnitude higher than dissolution rates of buried hydrates through permeable sediments and well within the experimentally derived range for exposed gas hydrates under different hydrodynamic boundary conditions. We conclude that submarine canyons which display high hydrodynamic activity can become key areas of enhanced seepage as a result of emerging weather patterns due to climate change.},
author = {Thomsen, Laurenz and Barnes, Christopher and Best, Mairi and Chapman, Ross and Pirenne, Benot and Thomson, Richard and Vogt, Joachim},
doi = {10.1029/2012GL052462},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Thomsen et al.{\_}2012{\_}Ocean circulation promotes methane release from gas hydrate outcrops at the NEPTUNE Canada Barkley Canyon node.pdf:pdf},
isbn = {0094-8276},
issn = {00948276},
journal = {Geophysical Research Letters},
keywords = {canyons,gas hydrates,inertial currents,seepage},
number = {16},
pages = {2--7},
title = {{Ocean circulation promotes methane release from gas hydrate outcrops at the NEPTUNE Canada Barkley Canyon node}},
volume = {39},
year = {2012}
}
@article{Oliva2001,
abstract = {In this paper, we propose a computational model of the recognition of real world scenes that bypasses the segmentation and the processing of individual objects or regions. The procedure is based on a very low dimen- sional representation of the scene, that we term the Spatial Envelope. We propose a set of perceptual dimensions (naturalness, openness, roughness, expansion, ruggedness) that represent the dominant spatial structure of a scene. Then, we show that these dimensions may be reliably estimated using spectral and coarsely localized information. The model generates a multidimensional space in which scenes sharing membership in semantic categories (e.g., streets, highways, coasts) are projected closed together. The performance of the spatial envelope model shows that specific information about object shape or identity is not a requirement for scene categorization and that modeling a holistic representation of the scene informs about its probable semantic category.},
annote = {This paper tries to compute a single FFT based descriptor to get the 'gist' of a scene in a single step. I have a feeling that if you run this against a modern dataset it would fail miserably.

Have to read through this in a little more detail.},
author = {Oliva, Aude and Torralba, Antonio},
doi = {10.1023/A:1011139631724},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Oliva, Torralba{\_}2001{\_}Modeling the shape of the scene A holistic representation of the spatial envelope.pdf:pdf},
isbn = {0920-5691},
issn = {09205691},
journal = {International Journal of Computer Vision},
keywords = {Energy spectrum,GIST,Image Feature Extraction,Natural images,Principal components,Scene recognition,Spatial layout},
mendeley-tags = {GIST,Image Feature Extraction},
number = {3},
pages = {145--175},
title = {{Modeling the shape of the scene: A holistic representation of the spatial envelope}},
volume = {42},
year = {2001}
}
@article{Buntine2009,
abstract = {Topic models are a discrete analogue to principle component analysis and independent component analysis that model {\{}$\backslash$it topic{\}} at the word level within a document. They have many variants such as NMF, PLSI and LDA, and are used in many fields such as genetics, text and the web, image analysis and recommender systems. However, only recently have reasonable methods for estimating the likelihood of unseen documents, for instance to perform testing or model comparison, become available. This paper explores a number of recent methods, and improves their theory, performance, and testing.},
author = {Buntine, Wray},
doi = {10.1007/978-3-642-05224-8_6},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Buntine{\_}2009{\_}Estimating likelihoods for topic models.pdf:pdf},
isbn = {3642052231},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {LDA Evaluation,LDA Theory},
mendeley-tags = {LDA Evaluation,LDA Theory},
pages = {51--64},
title = {{Estimating likelihoods for topic models}},
volume = {5828 LNAI},
year = {2009}
}
@article{Canclini2013,
author = {Canclini, A. and Cesana, M. and Redondi, A. and Tagliasacchi, M. and Ascenso, J. and Cilla, R.},
doi = {10.1109/ICDSP.2013.6622757},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Canclini et al.{\_}2013{\_}Evaluation of low-complexity visual feature detectors and descriptors.pdf:pdf},
isbn = {9781467358057},
journal = {2013 18th International Conference on Digital Signal Processing, DSP 2013},
keywords = {Binary descriptors,Image retrieval,Local feature descriptors,Local feature detectors},
pages = {0--6},
title = {{Evaluation of low-complexity visual feature detectors and descriptors}},
year = {2013}
}
@article{Hovland2003,
abstract = {Hundreds of large coral reefs up to 45 m high exist (A) on the continental shelf and (B) on morainic threshold ridges in fjords of northern, mid-, and southwest Norway. They occur in water depths between 400 and 40 m, and contain a large variety of megafauna. The most common frame-building coral is Lophelia pertusa (L.). The oldest reef found to date is 8600 calendar years old, determined by radiocarbon dating of buried Lophelia skeleton. Even though many of these reefs have been known to science for over 200 years, there is as yet no viable and unifying hypothesis to explain their existence in deep, cool waters, other than perhaps the 'hydraulic theory', presented by the author in 1990. It states that primary producers (mainly bacteria) are locally formed and concentrated at reef locations due to seepage of light hydrocarbons (mainly methane) on the continental shelf, and nutrient-rich groundwater in the fjords. The hydraulic theory is supported by the following topographic, reflection seismic, and geochemical indicators; for the continental shelf reefs (A): seaward-dipping sedimentary permeable strata, enhanced acoustic seismic reflectors, adjacent pockmark craters, locally elevated light hydrocarbon sediment content, locally elevated seawater methane content; for the fjord reefs (B): threshold morainic substratum rather than adjacent hardrock substratum, sub-surface dipping sedimentary layers, H 2S smell of near-surface sediment samples. These indicators, or reef-associated observations, have been documented by German, British, and Norwegian researchers over the last 10-15 years. However, all the listed indicators do not necessarily occur at each of the respective shelf and fjord reef locations simultaneously. ?? 2003 Elsevier Science B.V. All rights reserved.},
author = {Hovland, M. and Risk, M.},
doi = {10.1016/S0025-3227(03)00096-3},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Hovland, Risk{\_}2003{\_}Do Norwegian deep-water coral reefs rely on seeping fluids.pdf:pdf},
isbn = {0025-3227},
issn = {00253227},
journal = {Marine Geology},
keywords = {Ahermatypic corals,Dipping reflectors,Geochemical investigations,Hydraulic theory,Hydrocarbon seepage,Oceanography,Off-shore Norway,Pockmarks},
mendeley-tags = {Oceanography},
number = {1-2},
pages = {83--96},
pmid = {1183},
title = {{Do Norwegian deep-water coral reefs rely on seeping fluids?}},
volume = {198},
year = {2003}
}
@article{Davies2008,
abstract = {Ecological-niche factor analysis (ENFA) was applied to the reef framework-forming cold-water coral Lophelia pertusa. The environmental tolerances of this species were assessed using readily available oceanographic data, including physical, chemical, and biological variables. L. pertusa was found at mean depths of 468 and 480 m on the regional and global scales and occupied a niche that included higher than average current speed and productivity, supporting the theory that their limited food supply is locally enhanced by currents. Most records occurred in areas with a salinity of 35, mean temperatures of 6.2-6.7 ??C and dissolved oxygen levels of 6.0-6.2 ml l-1. The majority of records were found in areas that were saturated with aragonite but had low concentration of nutrients (silicate, phosphate, and nitrate). Suitable habitat for L. pertusa was predicted using ENFA on a global and a regional scale that incorporated the north-east Atlantic Ocean. Regional prediction was reliable due to numerous presence points throughout the area, whereas global prediction was less reliable due to the paucity of presence data outside of the north-east Atlantic. However, the species niche was supported at each spatial scale. Predicted maps at the global scale reinforced the general consensus that the North Atlantic Ocean is a key region in the worldwide distribution of L. pertusa. Predictive modelling is an approach that can be applied to cold-water coral species to locate areas of suitable habitat for further study. It may also prove a useful tool to assist spatial planning of offshore marine protected areas. However, issues with eco-geographical datasets, including their coarse resolution and limited geographical coverage, currently restrict the scope of this approach. ?? 2008 Elsevier Ltd. All rights reserved.},
author = {Davies, Andrew J. and Wisshak, Max and Orr, James C. and {Murray Roberts}, J.},
doi = {10.1016/j.dsr.2008.04.010},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Davies et al.{\_}2008{\_}Predicting suitable habitat for the cold-water coral Lophelia pertusa (Scleractinia).pdf:pdf},
isbn = {0967-0637},
issn = {09670637},
journal = {Deep-Sea Research Part I: Oceanographic Research Papers},
keywords = {Deep sea,ENFA,Ecological engineer,Ecological tolerance,Global,Habitat suitability,North-east Atlantic,Oceanography,Predictive modelling,Reef framework-forming},
mendeley-tags = {Oceanography},
number = {8},
pages = {1048--1062},
title = {{Predicting suitable habitat for the cold-water coral Lophelia pertusa (Scleractinia)}},
volume = {55},
year = {2008}
}
@article{Jurie2005,
author = {Jurie, F and Triggs, B},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Jurie, Triggs{\_}2005{\_}Creating Efficient Codebook for Visual Recognition.pdf:pdf},
journal = {Proc. ICCV},
keywords = {BoW,Texton},
mendeley-tags = {BoW,Texton},
title = {{Creating Efficient Codebook for Visual Recognition}},
year = {2005}
}
@article{Ontrup2002,
abstract = {We introduce a new type of Self-Organizing Map (SOM) to navigate in the Semantic Space of large text collections. We propose a "hyperbolic SOM" (HSOM) based on a regular tesselation of the hyperbolic plane, which is a non-euclidean space characterized by constant negative gaussian curvature. The exponentially increasing size of a neighborhood around a point in hyperbolic space provides more freedom to map the complex information space arising from language into spatial relations.},
author = {Ontrup, J{\"{o}}rg and Ritter, H},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Ontrup, Ritter{\_}2002{\_}Hyperbolic Self-Organizing Maps for Semantic Navigation.pdf:pdf},
isbn = {0262042088},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems 14},
keywords = {HSOM,SOM,self organizing maps},
mendeley-tags = {HSOM,SOM,self organizing maps},
pages = {1417--1424},
title = {{Hyperbolic Self-Organizing Maps for Semantic Navigation}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.2.3062{\&}rep=rep1{\&}type=pdf},
year = {2002}
}
@misc{Kendall2001,
author = {Kendall, Be},
booktitle = {eLS},
isbn = {0738204536},
title = {{Nonlinear dynamics and chaos}},
url = {http://onlinelibrary.wiley.com/doi/10.1038/npg.els.0003314/full},
year = {2001}
}
@misc{Cho2010,
abstract = {This study investigated the efficacy of chlorogenic acid on altering body fat in high-fat diet (37{\%} calories from fat) induced-obese mice compared to caffeic acid. Caffeic acid or chlorogenic acid was supplemented with high-fat diet at 0.02{\%} (wt/wt) dose. Both caffeic acid and chlorogenic acid significantly lowered body weight, visceral fat mass and plasma leptin and insulin levels compared to the high-fat control group. They also lowered triglyceride (in plasma, liver and heart) and cholesterol (in plasma, adipose tissue and heart) concentrations. Triglyceride content in adipose tissue was significantly lowered, whereas the plasma adiponectin level was elevated by chlorogenic acid supplementation compared to the high-fat control group. Body weight was significantly correlated with plasma leptin (r=0.894, p{\textless}0.01) and insulin (r=0.496, p{\textless}0.01) levels, respectively. Caffeic acid and chlorogenic acid significantly inhibited fatty acid synthase, 3-hydroxy-3-methylglutaryl CoA reductase and acyl-CoA:cholesterol acyltransferase activities, while they increased fatty acid beta-oxidation activity and peroxisome proliferator-activated receptors alpha expression in the liver compared to the high-fat group. These results suggest that caffeic acid and chlorogenic acid improve body weight, lipid metabolism and obesity-related hormones levels in high-fat fed mice. Chlorogenic acid seemed to be more potent for body weight reduction and regulation of lipid metabolism than caffeic acid.},
author = {Cho, Ae-Sim and Jeon, Seon-Min and Kim, Myung-Joo and Yeo, Jiyoung and Seo, Kwon-Il and Choi, Myung-Sook and Lee, Mi-Kyung},
booktitle = {Molecular Biology and Evolution},
doi = {10.1016/j.fct.2010.01.003},
issn = {18736351},
number = {3},
pages = {1--422},
pmid = {20064576},
title = {{The Real Book of Jazz Volume II.PDF}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20064576},
volume = {48},
year = {2010}
}
@article{Li2013,
author = {Li, Mingyang and Mourikis, Anastasios I.},
doi = {10.1109/ICRA.2013.6631398},
isbn = {978-1-4673-5643-5},
journal = {2013 IEEE International Conference on Robotics and Automation},
month = {may},
pages = {5709--5716},
publisher = {Ieee},
title = {{3-D motion estimation and online temporal calibration for camera-IMU systems}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6631398},
year = {2013}
}
@article{Engineering2004,
author = {Engineering, Computer},
title = {{STOCHASTIC PROCESSES}},
year = {2004}
}
@article{Elinas,
author = {Elinas, Pantelis and Little, James J},
title = {{Stereo vision SLAM : Near real-time learning of 3D point-landmark and 2D occupancy-grid maps using particle filters .}}
}
@article{Thrun2005,
author = {Thrun, Sebastian and Burgard, W and Fox, D},
pages = {492},
title = {{Probabilistic robotics}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Probabilistic+Robotics{\#}0},
year = {2005}
}
@article{Ritz2014,
author = {Ritz, Robin and D'Andrea, Raffaello},
doi = {10.1109/ICRA.2014.6907630},
isbn = {978-1-4799-3685-4},
journal = {2014 IEEE International Conference on Robotics and Automation (ICRA)},
month = {may},
pages = {5245--5251},
publisher = {Ieee},
title = {{An on-board learning scheme for open-loop quadrocopter maneuvers using inertial sensors and control inputs from an external pilot}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6907630},
year = {2014}
}
@article{Grzonka2012,
author = {Grzonka, Slawomir and Grisetti, Giorgio and Burgard, Wolfram},
doi = {10.1109/TRO.2011.2162999},
issn = {1552-3098},
journal = {IEEE Transactions on Robotics},
month = {feb},
number = {1},
pages = {90--100},
title = {{A Fully Autonomous Indoor Quadrotor}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6004839},
volume = {28},
year = {2012}
}
@article{Hehn,
author = {Hehn, Markus and Andrea, Raffaello D},
keywords = {autonomous vehicle navigation,control,flying robots,guidance and,making,mission planning and decision,motion control,trajectory generation},
title = {{Quadrocopter Trajectory Generation and Control}}
}
@article{Zhu2005,
author = {Zhu, Song-Chun and Guo, Cheng-en and Wang, Yizhou and Xu, Zijian},
doi = {10.1023/B:VISI.0000046592.70770.61},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Zhu et al.{\_}2005{\_}What are Textons.pdf:pdf},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
keywords = {BoW,Image Feature Extraction,Texton,lightons,motons,textons,textures,transformed component analysis},
mendeley-tags = {BoW,Image Feature Extraction,Texton},
month = {apr},
number = {1/2},
pages = {121--143},
title = {{What are Textons?}},
url = {http://link.springer.com/10.1023/B:VISI.0000046592.70770.61},
volume = {62},
year = {2005}
}
@article{Zhanga,
author = {Zhang, Ruo and Tsai, Ping-sing and Cryer, James Edwin and Shah, Mubarak},
keywords = {analysis of algorithms,and cda-9222798,by nsf grants cda-9122006,lambertian model,shape from shading,survey of,this work was supported},
pages = {1--41},
title = {{Shape from Shading : A Survey 1 Introduction}}
}
@article{Zeng2011,
author = {Zeng, Zhi and Zhang, Shuwu},
doi = {10.1109/ICASSP.2011.5946426},
isbn = {978-1-4577-0538-0},
journal = {2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
month = {may},
pages = {405--408},
publisher = {Ieee},
title = {{A hierarchical generative model for Generic Audio Document Categorization}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5946426},
year = {2011}
}
@article{Wichern2010,
author = {Wichern, Gordon and Xue, Jiachen},
journal = {IEEE Transactions On Audio Speech And Language Processing},
number = {3},
pages = {688--707},
title = {{Segmentation, indexing, and retrieval for environmental and natural sounds}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5410056},
volume = {18},
year = {2010}
}
@article{York2008,
author = {York, New},
number = {1},
pages = {1--6},
title = {{How to take the Dual of a Linear Program}},
year = {2008}
}
@book{Wilf1994,
author = {Wilf, Herbert S},
title = {generatingfunctionology},
year = {1994}
}
@article{Wong,
author = {Wong, Simon},
title = {{Notes on Linear Systems (Basics)}}
}
@article{Welch2006,
author = {Welch, Greg and Bishop, Gary},
pages = {1--16},
title = {{An Introduction to the Kalman Filter}},
year = {2006}
}
@book{Axler,
author = {Axler, S and Ribet, K A},
isbn = {9780387719382},
title = {{Universitext}}
}
@article{Weickert2005,
author = {Weickert, Joachim and Es, Andr and Orr, Christoph Schn},
keywords = {confidence measures,differential techniques,optic flow,partial differential equations,performance evaluation,structure tensor,variational methods},
number = {3},
pages = {211--231},
title = {{Lucas / Kanade Meets Horn / Schunck : Combining Local and Global Optic Flow Methods}},
volume = {61},
year = {2005}
}
@article{Wayne2005,
author = {Wayne, Kevin},
title = {{Chapter 1 Introduction : Some Representative Problems 1 . 1 A First Problem : Stable Matching}},
year = {2005}
}
@article{Carlo2002,
author = {Carlo, Monte},
title = {{Markov Chain Monte Carlo and Gibbs Sampling}},
year = {2002}
}
@article{Walks,
author = {Walks, Random},
pages = {445--459},
title = {{Random Processes}}
}
@book{Function,
author = {Function, Probability},
isbn = {9780495385080},
title = {{No Title}}
}
@article{Wabbit,
author = {Wabbit, Vowpal},
title = {{Goals of the VW project}}
}
@article{Vetta,
author = {Vetta, Adrian},
title = {{The Simplex Algorithm : Technicalities 1}}
}
@article{Vettaa,
author = {Vetta, Adrian},
title = {{The Simplex Algorithm : Technicalities 1}}
}
@article{York2008a,
author = {York, New},
pages = {4--6},
title = {{COMP 360 - Winter 2008 Assignment 3 Solutions}},
year = {2008}
}
@article{Vanier1999,
abstract = {One of the most difficult and time-consuming aspects of building compartmental models of single neurons is assigning values to free parameters to make models match experimental data. Automated parameter-search methods potentially represent a more rapid and less labor-intensive alternative to choosing parameters manually. Here we compare the performance of four different parameter-search methods on several single-neuron models. The methods compared are conjugate-gradient descent, genetic algorithms, simulated annealing, and stochastic search. Each method has been tested on five different neuronal models ranging from simple models with between 3 and 15 parameters to a realistic pyramidal cell model with 23 parameters. The results demonstrate that genetic algorithms and simulated annealing are generally the most effective methods. Simulated annealing was overwhelmingly the most effective method for simple models with small numbers of parameters, but the genetic algorithm method was equally effective for more complex models with larger numbers of parameters. The discussion considers possible explanations for these results and makes several specific recommendations for the use of parameter searches on neuronal models.},
author = {Vanier, M C and Bower, J M},
issn = {0929-5313},
journal = {Journal of computational neuroscience},
keywords = {Action Potentials,Action Potentials: physiology,Algorithms,Calcium Channels,Calcium Channels: physiology,Cell Compartmentation,Cell Compartmentation: physiology,Computer Simulation,Dendrites,Dendrites: physiology,Models, Neurological,Olfactory Pathways,Olfactory Pathways: physiology,Potassium Channels,Potassium Channels: physiology,Pyramidal Cells,Pyramidal Cells: physiology,Sodium Channels,Sodium Channels: physiology},
number = {2},
pages = {149--71},
pmid = {10515252},
title = {{A comparative survey of automated parameter-search methods for compartmental neural models.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10515252},
volume = {7},
year = {1999}
}
@article{Vergin1999,
author = {Vergin, Rivarol and Shaughnessy, Douglas O and Member, Senior and Farhat, Azarshid},
number = {5},
pages = {525--532},
title = {{Generalized Mel Frequency Cepstral Coefficients Continuous-Speech Recognition}},
volume = {7},
year = {1999}
}
@article{Ooe,
author = {{\`{O}}{\'{o}}{\"{e}}, {\'{E}}},
title = {{No Title}}
}
@book{Wedel2011,
address = {London},
author = {Wedel, Andreas and Cremers, Daniel},
doi = {10.1007/978-0-85729-965-9},
isbn = {978-0-85729-964-2},
publisher = {Springer London},
title = {{Stereo Scene Flow for 3D Motion Analysis}},
url = {http://link.springer.com/10.1007/978-0-85729-965-9},
year = {2011}
}
@article{C,
author = {C, Opencv Cheat Sheet},
pages = {2--3},
title = {{Matrix Manipulations : Copying ,}}
}
@article{Level,
author = {Level, User and Internets, Micro},
pages = {3--10},
title = {1 1.1.}
}
@article{Line,
author = {Line, The Correct and Property, Markov},
pages = {61--69},
title = {{Markov Processes}}
}
@article{,
pages = {1--14},
title = {{Inner product spaces, part 1 There is an interaction between linear algebra and geometry, some of which you have probably seen. In}}
}
@misc{,
title = {{Unknown - Unknown - Graph Theory Basics.pdf}}
}
@misc{,
title = {{Unknown - Unknown - Graph Colouring.pdf}}
}
@misc{,
title = {{Unknown - Unknown - Graph Connectivity.pdf}}
}
@article{,
pages = {1--17},
title = {{Factoring polynomials We will discuss polynomials over an arbitrary field}},
volume = {1}
}
@misc{,
title = {{Unknown - Unknown - Descartes1641a.pdf.pdf}}
}
@misc{,
title = {{Unknown - Unknown - autotalent-0.2{\_}refcard.pdf.pdf}}
}
@article{,
number = {1},
pages = {658},
title = {{Assignment 4 : Approximation Algorithms}}
}
@article{,
title = {{No Title}}
}
@article{Science,
author = {Science, Computer and Design, Mechanism},
pages = {9--20},
title = {{Chapter 1 Approximation and Mechanism Design}}
}
@article{,
number = {1},
pages = {526},
title = {{Assignment 3 : NP-Completeness}}
}
@misc{,
title = {{Unknown - Unknown - 10-whitenoise (1).pdf.pdf}}
}
@book{Music2013,
author = {Music, Electronic and Graz, Performing Arts},
isbn = {9783902949004},
title = {{Linux Audio Conference 2013}},
year = {2013}
}
@article{,
number = {February},
title = {{Technology Career Fair}},
year = {2012}
}
@article{,
pages = {1--11},
title = {{A dash on moments : moments , moment-generating functions , and method-of-moments estimators}},
year = {2010}
}
@article{,
pages = {1--2},
title = {2 {\textgreater} . 1},
year = {2012}
}
@article{,
pages = {503--539},
title = {{19 Deviations}},
year = {2010}
}
@article{,
pages = {2012},
title = {{Midterm Exams}},
year = {2012}
}
@article{,
pages = {2012},
title = {{Assignment {\#} 4 Probability Theory ( MATH323 )}},
year = {2012}
}
@article{Flow2009,
author = {Flow, Optic},
number = {June},
pages = {1--26},
title = {{2D / 3D Motion vs . Optic / Scene Flow 2D Vector Fields}},
year = {2009}
}
@article{,
pages = {1--9},
title = {{structures called fields, which we now define. DEFINITION 1 . A}},
year = {2008}
}
@article{,
pages = {2007},
title = {{Principles of Distributed Computing Exercise 10}},
year = {2007}
}
@article{,
pages = {2008},
title = {{H QUebeCHH}},
year = {2008}
}
@article{,
pages = {1--7},
title = {{Mathematics MATH 236, Winter 2007 Linear Algebra On determinants Suppose that}},
year = {2007}
}
@article{Guide2004,
author = {Guide, Opengl Programming and Guide, About This and Objects, Drawing Geometric and Offset, Polygon and Variables, Appendix B State},
title = {{OpenGL Programming Guide The Official Guide to Learning OpenGL , Version 1 . 1}},
year = {2004}
}
@article{,
pages = {500},
title = {{T HE R EGINALD F ESSENDEN}},
year = {1900}
}
@misc{,
title = {{Tufte - 1983 - The visual display of quantitative information.pdf}}
}
@article{Trevisan2011,
author = {Trevisan, Luca},
pages = {1--6},
title = {{Lecture 6 The Dual of Linear Program}},
year = {2011}
}
@article{Topic2004,
author = {Topic, Randomized Algorithms and Scribe, Chernoff Bounds and Lecturer, Mugizi Rwebangira and Date, Shuchi Chawla},
number = {M},
pages = {1--6},
title = {{Markov ' s Inequality Chebyshev Bounds Chernoff Bounds}},
volume = {859},
year = {2004}
}
@article{Url2007,
author = {Url, Stable},
number = {236},
pages = {433--460},
title = {{No Title}},
volume = {59},
year = {2007}
}
@article{Text,
author = {Text, S I},
pages = {1--7},
title = {{Supporting Information}}
}
@book{Design2006,
author = {Design, Operating Systems and Edition, Third and Hall, Prentice and Date, Pub},
isbn = {9780131429383},
number = {Minix 3},
title = {{This document was created by an unregistered ChmMagic, please go to http://www.bisenter.com to register it. Thanks.}},
year = {2006}
}
@article{Toman,
author = {Toman, David},
pages = {1--21},
title = {{Standard ML Mini-tutorial ( in particular SML / NJ )}}
}
@inproceedings{Sturm2010,
author = {Sturm, BL and Morvidone, Marcela and Daudet, Laurent},
booktitle = {18th European Signal Processing Conference (EUSIPCO-2010)},
number = {1},
pages = {0--4},
title = {{Musical instrument identification using multiscale mel-frequency cepstral coefficients}},
url = {http://vbn.aau.dk/files/48186471/melfreqovercomplete.pdf},
year = {2010}
}
@article{Suri,
author = {Suri, Subhash},
title = {{Network Flows 1.}}
}
@article{Sun,
author = {Sun, Min},
title = {{Today ' s outline}}
}
@article{Structures,
author = {Structures, Discrete and Due, I I and Bounds, Chernoff},
pages = {4--5},
title = {{Assignment 4 : Probability}}
}
@article{Surfaces,
author = {Surfaces, Lambertian and Radiance, Scene and Equation, Image Irradiance and Equation, Fundamental and Shape, O F and Shading, From},
number = {1},
pages = {2--5},
title = {{Shape From Shading}}
}
@article{Structuresa,
author = {Structures, Discrete and Due, I I and Marriages, Stable and Boy, Boy and Girl, Girl and Roommates, Stable and Matchings, Bipartite},
pages = {4--5},
title = {{Assignment 1 : Matchings}}
}
@article{Strachey2000,
author = {Strachey, Christopher},
keywords = {ad hoc polymorphism,binding,cpl,foundations of computing,functions as data,l-values,mechanisms,meter passing,para-,parametric polymorphism,programming languages,r-values,semantics,type completeness,variable binding},
pages = {11--49},
title = {{Fundamental Concepts in Programming Languages}},
year = {2000}
}
@misc{,
title = {{Spivak - 1965 - Calculus on manifolds.pdf}}
}
@article{Steyvers,
author = {Steyvers, Mark and Griffiths, Tom},
title = {{Probabilistic Topic Models}}
}
@article{Referring2012,
author = {Referring, On and Series, New},
number = {235},
pages = {320--344},
title = {{On Referring}},
volume = {59},
year = {2012}
}
@article{Strawson,
author = {Strawson, Peter},
pages = {1--21},
title = {{Freedom and Resentment by}}
}
@article{Flohr1995,
abstract = {A hypothesis on the physiological conditions of consciousness is presented. It is assumed that the occurrence of states of consciousness causally depends on the formation of complex representational structures. Cortical neural networks that exhibit a high representational activity develop higher-order, self-referential representations as a result of self-organizing processes. The occurrence of such states is identical with the appearance of states of consciousness. The underlying physiological processes can be identified. It is assumed that neural assemblies instantiate mental representations; hence consciousness depends on the rate at which large active assemblies are generated. The formation of assemblies involves the activation of the N-methyl-D-aspartate receptor channel complex which controls different forms of synaptic plasticity including rapid changes of the connection strengths. The various causes of unconsciousness (e.g., anaesthetics or brain stem lesions) have a common denominator: they directly or indirectly inhibit the formation of assemblies.},
author = {Flohr, H},
issn = {0166-4328},
journal = {Behavioural brain research},
keywords = {Animals,Brain,Brain: physiology,Consciousness,Consciousness: physiology,Humans,Mental Processes,Mental Processes: physiology,Sensation,Sensation: physiology},
month = {nov},
number = {1-2},
pages = {157--61},
pmid = {8747183},
title = {{Sensations and brain processes.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/8747183},
volume = {71},
year = {1995}
}
@article{Spingarn2000,
author = {Spingarn, Jonathan and Tech, Georgia},
title = {{The K{\"{o}}nig-Egerv{\'{a}}ry Theorem}},
year = {2000}
}
@article{Sivic2003,
author = {Sivic, Josef and Zisserman, Andrew},
doi = {10.1109/ICCV.2003.1238663},
isbn = {0-7695-1950-4},
journal = {Proceedings Ninth IEEE International Conference on Computer Vision},
number = {Iccv},
pages = {1470--1477 vol.2},
publisher = {Ieee},
title = {{Video Google: a text retrieval approach to object matching in videos}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1238663},
year = {2003}
}
@article{,
title = {what role will it play in human}
}
@article{Flohr1995a,
abstract = {A hypothesis on the physiological conditions of consciousness is presented. It is assumed that the occurrence of states of consciousness causally depends on the formation of complex representational structures. Cortical neural networks that exhibit a high representational activity develop higher-order, self-referential representations as a result of self-organizing processes. The occurrence of such states is identical with the appearance of states of consciousness. The underlying physiological processes can be identified. It is assumed that neural assemblies instantiate mental representations; hence consciousness depends on the rate at which large active assemblies are generated. The formation of assemblies involves the activation of the N-methyl-D-aspartate receptor channel complex which controls different forms of synaptic plasticity including rapid changes of the connection strengths. The various causes of unconsciousness (e.g., anaesthetics or brain stem lesions) have a common denominator: they directly or indirectly inhibit the formation of assemblies.},
author = {Flohr, H},
issn = {0166-4328},
journal = {Behavioural brain research},
keywords = {Animals,Brain,Brain: physiology,Consciousness,Consciousness: physiology,Humans,Mental Processes,Mental Processes: physiology,Sensation,Sensation: physiology},
month = {nov},
number = {1-2},
pages = {157--61},
pmid = {8747183},
title = {{Sensations and brain processes.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/8747183},
volume = {71},
year = {1995}
}
@book{Sipser,
author = {Sipser, Michael},
isbn = {0534950973},
title = {{INTRODUCTION TO THE THEORY OF COMPUTATION , SECOND EDITION Massachusetts Institute of Technology}}
}
@book{,
isbn = {0534950973},
title = {{No Title}}
}
@article{Singh1990,
author = {Singh, a.},
doi = {10.1109/ICCV.1990.139516},
isbn = {0-8186-2057-9},
journal = {[1990] Proceedings Third International Conference on Computer Vision},
pages = {168--177},
publisher = {IEEE Comput. Soc. Press},
title = {{An estimation-theoretic framework for image-flow computation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=139516},
year = {1990}
}
@article{Simard,
author = {Simard, Louis},
title = {{Prepared By}}
}
@article{Silver2011,
author = {Silver, David and Morales, Deryck and Rekleitis, Ioannis and Lisien, Brad},
title = {{Arc Carving : Obtaining Accurate , Low Latency Maps from Ultrasonic Range Sensors Why Ultrasonic Sensors ?}},
year = {2011}
}
@book{,
isbn = {9781439865521},
title = {{No Title}}
}
@book{Sharpe,
author = {Sharpe, Katherine and Crane, Elizabeth},
isbn = {9780982597767},
title = {{Sex Class Action The Other One Percent Big Baby Versus the Nanny State Captain Midnight Returns The Theory Generation NEW}}
}
@article{,
number = {July 1928},
pages = {379--423},
title = {{A Mathematical Theory of Communication}},
volume = {27},
year = {1948}
}
@article{Searle2010,
author = {Searle, John R.},
doi = {10.1017/S0140525X00005756},
issn = {0140-525X},
journal = {Behavioral and Brain Sciences},
keywords = {arti,at computer simulations of,attach to recent efforts,brain,cognitive capacities,human,i find it,in answering this question,intentionality,jcial intelligence,mind,significance should we,what psychological and philosophical},
month = {feb},
number = {03},
pages = {417},
title = {{Minds, brains, and programs}},
url = {http://www.journals.cambridge.org/abstract{\_}S0140525X00005756},
volume = {3},
year = {2010}
}
@article{Sandholm2002,
address = {New York, New York, USA},
author = {Sandholm, Tuomas and Suri, Subhash and Gilpin, Andrew and Levine, David},
doi = {10.1145/544757.544760},
isbn = {1581134800},
journal = {Proceedings of the first international joint conference on Autonomous agents and multiagent systems part 1 - AAMAS '02},
pages = {69},
publisher = {ACM Press},
title = {{Winner determination in combinatorial auction generalizations}},
url = {http://portal.acm.org/citation.cfm?doid=544741.544760},
year = {2002}
}
@article{Samuel1967,
author = {Samuel, A L},
number = {November},
title = {{Some Studies in Machine Learning Using the Game of Checkers . II-Recent Progress}},
year = {1967}
}
@article{Schaeffer2007,
abstract = {The game of checkers has roughly 500 billion billion possible positions (5 x 10(20)). The task of solving the game, determining the final result in a game with no mistakes made by either player, is daunting. Since 1989, almost continuously, dozens of computers have been working on solving checkers, applying state-of-the-art artificial intelligence techniques to the proving process. This paper announces that checkers is now solved: Perfect play by both sides leads to a draw. This is the most challenging popular game to be solved to date, roughly one million times as complex as Connect Four. Artificial intelligence technology has been used to generate strong heuristic-based game-playing programs, such as Deep Blue for chess. Solving a game takes this to the next level by replacing the heuristics with perfection.},
author = {Schaeffer, Jonathan and Burch, Neil and Bj{\"{o}}rnsson, Yngvi and Kishimoto, Akihiro and M{\"{u}}ller, Martin and Lake, Robert and Lu, Paul and Sutphen, Steve},
doi = {10.1126/science.1144079},
issn = {1095-9203},
journal = {Science (New York, N.Y.)},
month = {sep},
number = {5844},
pages = {1518--22},
pmid = {17641166},
title = {{Checkers is solved.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17641166},
volume = {317},
year = {2007}
}
@article{Saltzer1984,
author = {Saltzer, J. H. and Reed, D. P. and Clark, D. D.},
doi = {10.1145/357401.357402},
issn = {07342071},
journal = {ACM Transactions on Computer Systems},
month = {nov},
number = {4},
pages = {277--288},
title = {{End-to-end arguments in system design}},
url = {http://portal.acm.org/citation.cfm?doid=357401.357402},
volume = {2},
year = {1984}
}
@article{Russell2012,
author = {Russell, Bertrand},
number = {263},
pages = {385--389},
title = {{Mind Association}},
volume = {66},
year = {2012}
}
@book{Hirsch,
author = {Hirsch, Editor-in-chief Michael},
isbn = {0136042597},
title = {{No Title}}
}
@article{Russell2012a,
author = {Russell, Bertrand},
number = {56},
pages = {479--493},
title = {{On Denoting}},
volume = {14},
year = {2012}
}
@article{Society2012,
author = {Society, The Aristotelian},
pages = {108--128},
title = {{Knowledge by Acquaintance and Knowledge by Description Author ( s ): Bertrand Russell Reviewed work ( s ): Source : Proceedings of the Aristotelian Society , New Series , Vol . 11 ( 1910 - 1911 ), pp . 108-128 Published by : Wiley-Blackwell on behalf of T}},
volume = {11},
year = {2012}
}
@article{,
title = {{A " We}}
}
@article{Roy1996a,
author = {Roy, Nicholas and Dudek, Gregory and Freedman, Paul},
journal = {Robotics and Automation, 1996. {\ldots}},
number = {April},
pages = {1224--1228},
title = {{Surface sensing and classification for efficient mobile robot navigation}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=506874},
year = {1996}
}
@article{Rose2012,
author = {Rose, Sherri},
title = {{Big Data , Causal Modeling , and Estimation General Research Areas}},
year = {2012}
}
@article{Runge1810,
author = {Runge, Phillip Otto},
title = {{Color}},
year = {1810}
}
@article{Read2011,
author = {Read, Jesse and Pfahringer, Bernhard and Holmes, Geoff and Frank, Eibe},
doi = {10.1007/s10994-011-5256-5},
issn = {0885-6125},
journal = {Machine Learning},
month = {jun},
number = {3},
pages = {333--359},
title = {{Classifier chains for multi-label classification}},
url = {http://link.springer.com/10.1007/s10994-011-5256-5},
volume = {85},
year = {2011}
}
@article{Ramachandran1988,
abstract = {The human visual system can rapidly and accurately derive the three-dimensional orientation of surfaces by using variations in image intensity alone. This ability to perceive shape from shading is one of the most important yet poorly understood aspects of human vision. Here we present several findings which may help reveal computational mechanisms underlying this ability. First, we find that perception of shape from shading is a global operation which assumes that there is only one light source illuminating the entire visual image. This implies that if two identical objects are viewed simultaneously and illuminated from different angles, then we would be able to perceive three-dimensional shape accurately in only one of them at a time. Second, three-dimensional shapes that are defined exclusively by shading can provide tokens for the perception of apparent motion, suggesting that the motion mechanism is remarkably versatile in the kinds of inputs it can use. Lastly, the occluding edges which delineate an object from its background can also powerfully influence the perception of three-dimensional shape from shading.},
author = {Ramachandran, V S},
doi = {10.1038/331163a0},
issn = {0028-0836},
journal = {Nature},
keywords = {Depth Perception,Depth Perception: physiology,Form Perception,Form Perception: physiology,Humans,Light,Motion Perception,Motion Perception: physiology},
month = {jan},
number = {6152},
pages = {163--6},
pmid = {3340162},
title = {{Perception of shape from shading.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/3340162},
volume = {331},
year = {1988}
}
@book{,
isbn = {0072465638},
title = {{It ' s your choice ! New Modular Organization ! v}}
}
@article{Putnam1974,
author = {Putnam, Hilary and Saul, A J},
number = {1966},
title = {{The Nature of Mental States}},
volume = {63},
year = {1974}
}
@article{R,
author = {R, Wei-lun Chao},
title = {{Gabor wavelet transform and its application}}
}
@article{Qsat,
author = {Qsat, Monotone},
number = {1},
pages = {1--2},
title = {{Assignment 5 : PSPACE and Branch {\&} Bound}}
}
@article{Papers1975,
address = {Cambridge},
author = {Papers, Philosophical},
doi = {10.1017/CBO9780511625251},
editor = {Putnam, Hilary},
isbn = {9780511625251},
publisher = {Cambridge University Press},
title = {{Mind, Language and Reality}},
url = {http://ebooks.cambridge.org/ref/id/CBO9780511625251},
volume = {2},
year = {1975}
}
@article{Matou2007,
author = {Matou, Ji},
doi = {10.1007/978-3-540-30717-4_6},
isbn = {9783540306979},
number = {1},
pages = {81--104},
title = {{Duality of Linear Programming}},
year = {2007}
}
@article{Papers1975a,
address = {Cambridge},
author = {Papers, Philosophical},
doi = {10.1017/CBO9780511625251},
editor = {Putnam, Hilary},
isbn = {9780511625251},
publisher = {Cambridge University Press},
title = {{Mind, Language and Reality}},
url = {http://ebooks.cambridge.org/ref/id/CBO9780511625251},
volume = {2},
year = {1975}
}
@article{Programming,
author = {Programming, Pbasic},
title = {{PBASIC Programming}}
}
@article{Prof2010,
author = {Prof, Lecturer and Borodin, Allan},
number = {4},
pages = {1--5},
title = {{CSC2420 - Fall 2010 - Lecture 5}},
year = {2010}
}
@article{Precup2012,
author = {Precup, Doina and These, Nserc Usra and Jack, Ann and Tba, Deadline},
pages = {2--3},
title = {{S U M M E R U N D E R G R A D U AT E R E S E A R C H}},
year = {2012}
}
@article{State-space2013,
author = {State-space, Pddl},
title = {{Lecture 9 : First-order Logic and Planning Recall : Propositional Logic First-Order Logic ( FOL ) Syntax of FOL : Basic elements}},
year = {2013}
}
@article{,
title = {{Lecture 7 : Logic and Planning What is planning ?}},
year = {2013}
}
@article{,
title = {{Lecture 6 : Game Playing Types of games Human or Computer : Who is Better ?}},
year = {2013}
}
@article{Carlo2013,
author = {Carlo, Monte and Search, Tree and Go, Scrabble Computer and Silver, David},
number = {Part 2},
title = {{Recall : Game search Recall : Minimax Monte-Ca}},
year = {2013}
}
@article{,
title = {{Lecture 5 : Genetic algorithms . Constraint Satisfaction Recall from last time : Optimization problems}},
year = {2013}
}
@article{Search2013,
author = {Search, Heuristic},
title = {{Lecture 3 : Informed ( Heuristic ) Search Heuristic Search A ∗ search Proof of optimality of A ∗ Recall from last time : General search • Problems are described through states , operators and costs form a search tree Uninformed vs . informed search • Unin}},
year = {2013}
}
@article{,
title = {{Lecture 4 : Search for Optimization Problems Optimization problems Canonical example : Traveling Salesman Problem ( TSP ) Real-life examples of optimization problems}},
year = {2013}
}
@article{Search2013a,
author = {Search, Uninformed},
title = {{Other Real-Lif}},
year = {2013}
}
@article{Email2013,
author = {Email, Doina Precup and Class, T B A},
title = {{Introduction to AI ( COMP-424 )}},
year = {2013}
}
@article{Monday2013,
author = {Monday, Posted},
pages = {1--2},
title = {{COMP-424 - Assignment 1}},
year = {2013}
}
@article{Prasad,
author = {Prasad, V Shiv Naga and Domke, Justin},
keywords = {gabor filters,high dimensional,information visualization},
number = {c},
title = {{Gabor Filter Visualization}}
}
@misc{,
title = {{Prakash - 2007 - Comp 330 - Dec. 2007 Final.pdf}}
}
@article{Prados,
author = {Prados, Emmanuel and Faugeras, Olivier},
pages = {1--17},
title = {{Shape From Shading}}
}
@article{Port,
author = {Port, Serial},
title = {{BASIC Stamp 2}},
volume = {2}
}
@article{Pientka2012,
author = {Pientka, Brigitte},
pages = {1--15},
title = {{Fun with functions : higher-order functions}},
year = {2012}
}
@article{Pientka2012a,
author = {Pientka, Brigitte},
pages = {1--15},
title = {{Fun with functions : higher-order functions}},
year = {2012}
}
@article{Pientka2012b,
author = {Pientka, Brigitte},
pages = {1--6},
title = {{Towards Mini-ML}},
year = {2012}
}
@article{Pientka2014,
author = {Pientka, Brigitte},
doi = {10.1111/mono.12118},
issn = {1540-5834},
journal = {Monographs of the Society for Research in Child Development},
month = {sep},
number = {3},
pages = {138--46},
pmid = {25100094},
title = {{References.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25146868},
volume = {79},
year = {2014}
}
@article{Phan2007,
author = {Phan, Xuan-hieu},
pages = {1--10},
title = {{GibbsLDA ++ A C / C ++ Implementation of Latent Dirichlet Allocation ( LDA ) using Gibbs Sampling for Parameter Estimation and Inference}},
year = {2007}
}
@article{Pientka2012c,
author = {Pientka, Prof B},
number = {March},
pages = {1--4},
title = {{COMP 302 : Programming Languages and Paradigms Assignment 4 ( 100 points total )}},
volume = {4},
year = {2012}
}
@article{Pendavingh,
author = {Pendavingh, Rudi},
pages = {1--18},
title = {{Matroids}}
}
@article{Pientka2012d,
author = {Pientka, Brigitte},
pages = {1--5},
title = {{A brief introduction to theoretical concepts in programming languages : formal syntax and evaluation}},
year = {2012}
}
@article{Parfit1971,
author = {Parfit, Derek},
journal = {Philosophical Review},
number = {1},
pages = {3--27},
title = {{Personal Identity}},
volume = {80},
year = {1971}
}
@article{Report2009,
author = {Report, Technical},
doi = {10.1214/154957804100000000},
keywords = {and phrases,causal effects,causes of effects,confounding,counterfactuals,graph-,ical methods,mediation,policy evaluation,potential-outcome,structural equation models},
number = {August},
title = {{Causal Inference in Statistics : An}},
volume = {0},
year = {2009}
}
@article{Outline,
author = {Outline, Course},
pages = {1--2},
title = {{McGill University}}
}
@article{Notes,
author = {Notes, Rao-blackwellized Particle Filter and Mori, Greg},
pages = {1--3},
title = {{Rao-Blackwellized Particle Filter Simultaneous Localization and Mapping ( SLAM ) Visual Tracking - Continuous Variables}}
}
@article{Oxley2007,
author = {Oxley, James},
pages = {1--45},
title = {{What is a matroid?}},
year = {2007}
}
@article{Pang2008,
author = {Pang, Bo and Lee, Lillian},
doi = {10.1561/1500000011},
issn = {1554-0669},
journal = {Foundations and Trends{\textregistered} in Information Retrieval},
number = {1–2},
pages = {1--135},
title = {{Opinion Mining and Sentiment Analysis}},
url = {http://www.nowpublishers.com/product.aspx?product=INR{\&}doi=1500000011},
volume = {2},
year = {2008}
}
@article{Murali2009,
author = {Murali, T M},
title = {{Network Flow}},
year = {2009}
}
@inproceedings{Muhammad2010,
author = {Muhammad, Ghulam and Alotaibi, Yousef a. and Alsulaiman, Mansour and Huda, Mohammad Nurul},
booktitle = {2010 Fifth International Conference on Digital Telecommunications},
doi = {10.1109/ICDT.2010.10},
isbn = {978-1-4244-7271-0},
month = {jun},
pages = {11--16},
publisher = {Ieee},
title = {{Environment Recognition Using Selected MPEG-7 Audio Features and Mel-Frequency Cepstral Coefficients}},
year = {2010}
}
@article{Movellan,
author = {Movellan, Javier R},
title = {{Tutorial on Gabor Filters}}
}
@article{Nagel1974,
author = {Nagel, Thomas},
journal = {The Philosophical Review},
number = {4},
pages = {435--450},
title = {{What It It Like to Be a Bat?}},
volume = {83},
year = {1974}
}
@article{Mitrovic2007,
author = {Mitrovic, D},
journal = {Journal of Digital Information Management},
title = {{Analysis of the data quality of audio descriptions of environmental sounds}},
url = {http://www.freepatentsonline.com/article/Journal-Digital-Information-Management/186470812.html},
year = {2007}
}
@article{Mill2008,
author = {Mill, A Return T O},
pages = {1--13},
title = {{Kripke : “ Naming and Necessity ”}},
year = {2008}
}
@article{Milani,
author = {Milaniˇ, Martin},
keywords = {algorithm,exact weighted independent set,modular decomposition,np -completeness,pseudo-polynomial},
pages = {1--24},
title = {{On the complexity of the exact weighted independent set problem}}
}
@article{Mcdermott,
author = {Mcdermott, Erik},
number = {3},
pages = {4--7},
title = {{A DISCRIMINATIVE FILTER BANK MODEL FOR SPEECH RECOGNITION}}
}
@article{Mockapetris1988,
author = {Mockapetris, Paul V and Information, U S C and Rey, Marina and Dunlap, Kevin J},
pages = {1--11},
title = {{Development of the Domain Name System *}},
year = {1988}
}
@article{Mirzaei2008,
author = {Mirzaei, Nariman},
title = {{Cloud Computing}},
year = {2008}
}
@article{Mary,
author = {Mary, Queen},
title = {{The Vamp Audio Analysis Plugin API : A Programmer ' s Guide}}
}
@article{Mccallum2006b,
author = {Mccallum, Andrew},
title = {{Pachinko Allocation : DAG-Structured Mixture Models of Topic Correlations}},
year = {2006}
}
@article{Mar1959,
author = {Mar, No Jan},
number = {1},
title = {{Review : [ Untitled ] Reviewed Work ( s ): Verbal behavior by B . F . Skinner Noam Chomsky}},
volume = {35},
year = {1959}
}
@article{Marr1980,
abstract = {A theory of edge detection is presented. The analysis proceeds in two parts. (1) Intensity changes, which occur in a natural image over a wide range of scales, are detected separately at different scales. An appropriate filter for this purpose at a given scale is found to be the second derivative of a Gaussian, and it is shown that, provided some simple conditions are satisfied, these primary filters need not be orientation-dependent. Thus, intensity changes at a given scale are best detected by finding the zero values of delta 2G(x,y)*I(x,y) for image I, where G(x,y) is a two-dimensional Gaussian distribution and delta 2 is the Laplacian. The intensity changes thus discovered in each of the channels are then represented by oriented primitives called zero-crossing segments, and evidence is given that this representation is complete. (2) Intensity changes in images arise from surface discontinuities or from reflectance or illumination boundaries, and these all have the property that they are spatially. Because of this, the zero-crossing segments from the different channels are not independent, and rules are deduced for combining them into a description of the image. This description is called the raw primal sketch. The theory explains several basic psychophysical findings, and the operation of forming oriented zero-crossing segments from the output of centre-surround delta 2G filters acting on the image forms the basis for a physiological model of simple cells (see Marr {\&} Ullman 1979).},
author = {Marr, D and Hildreth, E},
issn = {0080-4649},
journal = {Proceedings of the Royal Society of London. Series B, Containing papers of a Biological character. Royal Society (Great Britain)},
keywords = {Animals,Form Perception,Form Perception: physiology,Humans,Mathematics,Vision, Ocular,Vision, Ocular: physiology},
month = {feb},
number = {1167},
pages = {187--217},
pmid = {6102765},
title = {{Theory of edge detection.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/6102765},
volume = {207},
year = {1980}
}
@article{Malcolm2006,
author = {Malcolm, Norman},
number = {1},
pages = {45--72},
title = {{The Conceivability of Mechanism Norman Malcolm}},
volume = {77},
year = {2006}
}
@article{Protocols,
author = {Protocols, Internet Multicast},
pages = {107--113},
title = {{13.5.1. Overview}}
}
@misc{,
title = {{Mallat, Zhang - 1993 - Matching pursuits with time-frequency dictionaries.pdf}}
}
@article{,
pages = {1--6},
title = {{The matrix of a linear transformation In the case where}}
}
@inproceedings{Maher2012,
author = {Maher, Robert C and Studniarz, Joseph},
booktitle = {Audio Engineering Society 46th International Conference},
pages = {14--17},
title = {{Automatic Search and Classification of Sound Sources in Long-Term Surveillance Recordings}},
year = {2012}
}
@article{,
title = {{No Title}}
}
@article{Ma2006,
author = {Ma, Ling and Milner, Ben and Smith, Dan},
doi = {10.1145/1149290.1149292},
issn = {15504875},
journal = {ACM Transactions on Speech and Language Processing},
month = {jul},
number = {2},
pages = {1--22},
title = {{Acoustic environment classification}},
url = {http://portal.acm.org/citation.cfm?doid=1149290.1149292},
volume = {3},
year = {2006}
}
@article{Experiments,
author = {Experiments, Gini},
title = {{GINI Experiments 10}}
}
@article{Lowe2004a,
author = {Lowe, David G.},
doi = {10.1023/B:VISI.0000029664.99615.94},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
month = {nov},
number = {2},
pages = {91--110},
title = {{Distinctive Image Features from Scale-Invariant Keypoints}},
url = {http://link.springer.com/10.1023/B:VISI.0000029664.99615.94},
volume = {60},
year = {2004}
}
@article{,
pages = {1--17},
title = {{Spanning, linear dependence, dimension In the crudest possible measure of these things, the real line}}
}
@article{Leighton2006,
author = {Leighton, Tom and Rubinfeld, Ronitt},
pages = {1--12},
title = {{Graph Theory II Matchings}},
year = {2006}
}
@article{Lehmann,
author = {Lehmann, Daniel and Rudolf, M},
number = {Chapter 1},
pages = {1--41},
title = {{Chapter 12 The Winner Determination Problem}}
}
@article{,
pages = {1--10},
title = {{Linear transformations: the basics A linear transformation is a homomorphism from one vector space to another (over the same field). That is, a linear transformation is a function from one}}
}
@article{Lewis,
author = {Lewis, David},
title = {{Mad Pain and Martian Pain}}
}
@article{Locke,
author = {Locke, Quotations},
pages = {143--146},
title = {{Chapter 8 Handouts}}
}
@article{,
title = {{No Title}}
}
@misc{,
title = {{Langer - Unknown - Lecture 2 - Image Formation.pdf}}
}
@misc{,
title = {{Lee, Moray - 1994 - Trust, self-confidence, and operators' adaptation to automation.pdf}}
}
@article{Lazebnik2006,
author = {Lazebnik, S. and Schmid, C. and Ponce, J.},
doi = {10.1109/CVPR.2006.68},
isbn = {0-7695-2597-0},
journal = {2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Volume 2 (CVPR'06)},
pages = {2169--2178},
publisher = {Ieee},
title = {{Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1641019},
volume = {2},
year = {2006}
}
@misc{,
title = {{Langer - Unknown - Lecture 1 - Intro to Image Formation.pdf}}
}
@article{,
pages = {1--5},
title = {0 70 ,},
year = {2013}
}
@article{Langer2013,
author = {Langer, Michael},
pages = {1--5},
title = {. id: grade: /20},
year = {2013}
}
@article{,
pages = {1--8},
title = {{White noise Autocorrelation Example : Spike triggered averages ( revisited )}},
year = {2013}
}
@article{,
pages = {1--7},
title = {{Questions 1.}},
year = {2013}
}
@article{,
pages = {3--6},
title = {are measured in angle from center of},
year = {2013}
}
@article{,
number = {lecture 12},
pages = {1--6},
title = {{Binocular disparity History : random dot stereograms and the correspondence problem}},
year = {2013}
}
@misc{,
title = {{Langer - 2013 - Lecture 8 - Fourier Transforms, COMP 598.pdf}}
}
@misc{,
title = {{Langer - 2013 - Lecture 5 - Convolution, COMP 598.pdf}}
}
@misc{,
title = {{Langer - 2013 - Lecture 9 - Convolution Theorem, COMP 598.pdf}}
}
@misc{,
title = {{Langer - 2013 - Lecture 7 - Complex Cells (COMP 598).pdf}}
}
@misc{,
title = {{Langer - 2013 - Lecture 3 - Color, COMP 598.pdf}}
}
@misc{,
title = {{Langer - 2013 - Lecture 22 - Auditory Pathway {\&} Source Localization, Critical Bands, COMP 598.pdf}}
}
@misc{,
title = {{Langer - 2013 - Lecture 22 - Bat Echolocation, COMP 598.pdf}}
}
@misc{,
title = {{Langer - 2013 - Lecture 4 - The Retina, COMP 598.pdf}}
}
@article{,
title = {{No Title}}
}
@misc{,
title = {{Langer - 2013 - Lecture 20 - Sound 2, COMP 598.pdf}}
}
@misc{,
title = {{Langer - 2013 - Lecture 19 - Sound 1, COMP 598.pdf}}
}
@article{,
title = {{http://www.cim.mcgill.ca/{\~{}}langer/646/MATLAB/likelihood.m}}
}
@misc{,
title = {{Langer - 2013 - Lecture 16 - Noise {\&} Psychophysics, COMP 598.pdf}}
}
@misc{,
title = {{Langer - 2013 - Lecture 18 - Bayes {\&} Priors, COMP 598.pdf}}
}
@misc{,
title = {{Langer - 2013 - Lecture 14 - Observer Motion, COMP 598.pdf}}
}
@article{,
title = {{No Title}}
}
@article{,
title = {{No Title}}
}
@article{,
title = {{No Title}}
}
@article{,
number = {c},
pages = {1--6},
title = {{Questions 1.}},
year = {2013}
}
@article{,
pages = {1--7},
title = {{Sines and cosines : the basis of the Fourier transform Complex numbers ( review )}},
year = {2013}
}
@article{,
title = {{Matlab script plot2Dbands.m}}
}
@article{,
pages = {2--5},
title = {{Questions 1.}},
year = {2013}
}
@article{On-center2013,
author = {On-center, D O G},
pages = {1--12},
title = {{Questions 1.}},
year = {2013}
}
@article{Exercise2013,
author = {Exercise, Recall},
pages = {1--8},
title = {{Questions 1.}},
year = {2013}
}
@article{Apple2013,
author = {Apple, The},
pages = {1--11},
title = {{COMP 598/646, Winter 2013 Exercises 1 - Image formation}},
year = {2013}
}
@article{Theorem2013,
author = {Theorem, Convolution and Fourier, Its},
pages = {1--6},
title = {{Re-defining functions on the integers mod N Circular convolution}},
year = {2013}
}
@article{Labbe2011,
author = {Labb{\'{e}}, M and Michaud, F},
journal = {International Conference on Intelligent Robots and Systems},
title = {{Memory management for real-time appearance-based loop closure detection}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6094602},
year = {2011}
}
@article{Laganiere,
author = {Lagani{\`{e}}re, Robert},
number = {5},
title = {{OpenCV 2 Computer Vision Application Programming Cookbook About the Author}}
}
@article{Michael2013,
author = {Michael, Prof},
title = {{Winter 2013 Assignment 3 Solutions}},
year = {2013}
}
@article{Lainiotis1971,
author = {Lainiotis, Demetrios G},
number = {2},
pages = {160--170},
title = {{Optimal Adaptive Estimation : Structure and}},
year = {1971}
}
@article{Kurose2012,
author = {Kurose, Jim and Ross, Keith},
number = {March},
title = {{Chapter 3 Transport Layer}},
year = {2012}
}
@article{Kurose2012a,
author = {Kurose, Jim and Ross, Keith},
number = {March},
title = {{Chapter 4 Network Layer}},
year = {2012}
}
@article{Kurose2012b,
author = {Kurose, Jim and Ross, Keith},
number = {March},
title = {{Chapter 3 Transport Layer}},
year = {2012}
}
@article{Kurose2004,
author = {Kurose, James F and Ross, Keith W},
title = {{Computer Networking A Top-Down Approach Featuring the Internet}},
year = {2004}
}
@article{Kurose2012c,
author = {Kurose, Jim and Ross, Keith},
number = {March},
title = {{Chapter 2 Application Layer}},
year = {2012}
}
@article{Krioukov2012,
abstract = {Prediction and control of the dynamics of complex networks is a central problem in network science. Structural and dynamical similarities of different real networks suggest that some universal laws might accurately describe the dynamics of these networks, albeit the nature and common origin of such laws remain elusive. Here we show that the causal network representing the large-scale structure of spacetime in our accelerating universe is a power-law graph with strong clustering, similar to many complex networks such as the Internet, social, or biological networks. We prove that this structural similarity is a consequence of the asymptotic equivalence between the large-scale growth dynamics of complex networks and causal networks. This equivalence suggests that unexpectedly similar laws govern the dynamics of complex networks and spacetime in the universe, with implications to network science and cosmology.},
archivePrefix = {arXiv},
arxivId = {1203.2109},
author = {Krioukov, Dmitri and Kitsak, Maksim and Sinkovits, Robert S. and Rideout, David and Meyer, David and Boguna, Marian},
doi = {10.1038/srep00793},
eprint = {1203.2109},
month = {mar},
pages = {1--26},
title = {{Network Cosmology}},
url = {http://arxiv.org/abs/1203.2109},
year = {2012}
}
@article{Korf1988,
author = {Korf, Richard E},
title = {{Real-Time Heuristic Search : New Results * Minimin with Alpha Pruni}},
year = {1988}
}
@article{Koppula,
author = {Koppula, Hema S and Saxena, Ashutosh},
title = {{Anticipating Human Activities using Object Affordances for Reactive Robotic Response}}
}
@article{,
doi = {10.2139/ssrn.629283.},
title = {{W3C Workshop on Privacy and Data Usage Control Using Network Science To Understand and Apply Privacy Usage Controls? 1 Erin Kenneally}},
year = {2010}
}
@article{Kim2009,
author = {{Kim, Samuel; Narayanan, Shrikanth; Sundaram}, Shiva},
journal = {IEEE Workshop on Applications of Signal Processing to Audio and Acoustics},
pages = {2--5},
title = {{Acoustic Topic Model for Audio Information Retrieval}},
year = {2009}
}
@misc{,
title = {{Kleinberg - Unknown - Algorithm Design and Analysis.pdf}}
}
@article{Clark1998,
author = {Clark, Andy and Chalmers, David},
number = {January},
pages = {7--19},
title = {{The extended mind 1}},
year = {1998}
}
@article{Kim2010,
author = {Kim, Samuel and Georgiou, PG},
journal = {IEEE Workshop on Applications of Signal Processing to Audio and Acoustics},
title = {{Supervised acoustic topic model for unstructured audio information retrieval}},
url = {http://worshipersam.net/www/pages/pdf{\_}papers/apsipa2010-supervisedLDA.pdf},
year = {2010}
}
@article{Kaess2008,
author = {Kaess, Michael and Member, Student and Ranganathan, Ananth},
pages = {1--14},
title = {{iSAM : Incremental Smoothing and Mapping}},
year = {2008}
}
@article{Kalmbach2011,
author = {Kalmbach, Arnold and Prof, Supervisor and Dudek, Gregory},
pages = {2011},
title = {{A QUA R OBOT H ARDWARE D IAGNOSTICS}},
year = {2011}
}
@article{Kalmbach2009,
author = {Kalmbach, Resume Arnold M},
number = {508},
pages = {2009--2010},
title = {{No Title}},
year = {2009}
}
@article{Jamakovic2009,
abstract = {Network motifs are small building blocks of complex networks, such as gene regulatory networks. The frequent appearance of a motif may be an indication of some network-specific utility for that motif, such as speeding up the response times of gene circuits. However, the precise nature of the connection between motifs and the global structure and function of networks remains unclear. Here we show that the global structure of some real networks is statistically determined by the distributions of local motifs of size at most 3, once we augment motifs to include node degree information. That is, remarkably, the global properties of these networks are fixed by the probability of the presence of links between node triples, once this probability accounts for the degree of the individual nodes. We consider a social web of trust, protein interactions, scientific collaborations, air transportation, the Internet, and a power grid. In all cases except the power grid, random networks that maintain the degree-enriched connectivity profiles for node triples in the original network reproduce all its local and global properties. This finding provides an alternative statistical explanation for motif significance. It also impacts research on network topology modeling and generation. Such models and generators are guaranteed to reproduce essential local and global network properties as soon as they reproduce their 3-node connectivity statistics.},
archivePrefix = {arXiv},
arxivId = {0908.1143},
author = {Jamakovic, Almerima and Mahadevan, Priya and Vahdat, Amin and Boguna, Marian and Krioukov, Dmitri},
eprint = {0908.1143},
month = {aug},
pages = {1--15},
title = {{How small are building blocks of complex networks}},
url = {http://arxiv.org/abs/0908.1143},
year = {2009}
}
@article{Flynn2000,
author = {Flynn, P J},
number = {3},
title = {{Data Clustering : A Review}},
volume = {31},
year = {2000}
}
@article{Bear2009,
author = {Bear, H. S.},
doi = {10.1109/9780470546765.ch40},
isbn = {9780470546765},
number = {x},
pages = {245--253},
title = {{Double Integrals}},
year = {2009}
}
@article{Information2012,
author = {Information, General and Maheswaran, Muthucumaru and Building, Mcconnell Engineering and Liu, Shitai and Wadhwani, Sandeep and Description, Brief Course},
number = {September},
pages = {1--5},
title = {{COMP 535 -- Computer Networks}},
year = {2012}
}
@article{Jain2008,
author = {Jain, Anil K.},
doi = {10.1109/CVPR.2008.4587389},
isbn = {978-1-4244-2242-5},
journal = {2008 IEEE Conference on Computer Vision and Pattern Recognition},
month = {jun},
pages = {1--8},
publisher = {Ieee},
title = {{Rank-based distance metric learning: An application to image retrieval}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4587389},
year = {2008}
}
@article{Ii,
author = {Ii, Discrete Structures and Ii, Solution and Ii, Proof and Hall, State},
number = {c},
pages = {1--4},
title = {{Practice Midterm Exam : Solutions}}
}
@article{Jackson1982,
author = {Jackson, Frank},
number = {127},
pages = {127--136},
title = {{Epiphenomenal Qualia Frank Jackson}},
volume = {32},
year = {1982}
}
@article{Hofmann2001,
author = {Hofmann, Thomas},
journal = {Machine Learning},
keywords = {dimension reduction,em algorithm,information retrieval,language modeling,latent class models,mixture models,natural language processing,unsupervised learning},
pages = {177--196},
title = {{Unsupervised learning by probabilistic latent semantic analysis}},
url = {http://www.springerlink.com/index/L5656365840672G8.pdf},
year = {2001}
}
@article{Herrera1998,
author = {Herrera, Perfecto and Serra, Xavier},
keywords = {audio material,describe and organize more,labeling techniques and thesauri,order to help to,techniques in,that can,the key,thoroughly the content of,work synergistically with automatic},
title = {{Audio Descriptors and Descriptor Schemes in the Context of MPEG-7}},
year = {1998}
}
@article{Horn1979,
abstract = {It appears that the development of machine vision may benefit from a detailed understanding of the imaging process. The reflectance map, showing scene radiance as a function of surface gradient, has proved to be helpful in this endeavor. The reflectance map depends both on the nature of the surface layers of the objects being imaged and the distribution of light sources. Recently, a unified approach to the specification of surface reflectance in terms of both incident and reflected beam geometry has been proposed. The reflecting properties of a surface are specified in terms of the bidirectional reflectance-distribution function (BRDF). Here we derive the reflectance map in terms of the BRDF and the distribution of source radiance. A number of special cases of practical importance are developed in detail. The significance of this approach to the understanding of image formation is briefly indicated.},
author = {Horn, B K and Sjoberg, R W},
issn = {0003-6935},
journal = {Applied optics},
keywords = {Reflectance Map, Image Brightness, Scene Radiance,},
month = {jun},
number = {11},
pages = {1770--9},
pmid = {20212547},
title = {{Calculating the reflectance map.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20212547},
volume = {18},
year = {1979}
}
@article{Humphrey2011,
author = {Humphrey, E. J. and Glennon, a. P. and Bello, J. P.},
doi = {10.1109/ICMLA.2011.105},
isbn = {978-1-4577-2134-2},
journal = {2011 10th International Conference on Machine Learning and Applications and Workshops},
month = {dec},
pages = {142--147},
publisher = {Ieee},
title = {{Non-Linear Semantic Embedding for Organizing Large Instrument Sample Libraries}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6147663},
year = {2011}
}
@article{Horn1981,
author = {Horn, Berthold K.P. and Schunck, Brian G.},
doi = {10.1016/0004-3702(81)90024-2},
issn = {00043702},
journal = {Artificial Intelligence},
month = {aug},
number = {1-3},
pages = {185--203},
title = {{Determining optical flow}},
url = {http://linkinghub.elsevier.com/retrieve/pii/0004370281900242},
volume = {17},
year = {1981}
}
@article{Heeger,
author = {Heeger, David},
title = {{Linear Systems theory Handout}}
}
@article{Harper2009,
author = {Harper, Robert},
title = {{Programming in Standard ML}},
year = {2009}
}
@article{Harvey-lewis2011,
author = {Harvey-lewis, Colin},
pages = {9--11},
title = {{Psych 211 : Introduction to Behavioural Neuroscience}},
year = {2011}
}
@article{Heeger2000,
author = {Heeger, David},
pages = {1--18},
title = {{Signals , Linear Systems , and Convolution}},
year = {2000}
}
@article{Heinrich,
author = {Heinrich, Gregor},
title = {{Parameter estimation for text analysis}}
}
@article{Hansen2004,
author = {Hansen, Michael R},
title = {{Introduction to SML Finite Trees}},
year = {2004}
}
@article{Hallett2012,
author = {Hallett, Michael},
title = {{Philosophy of Language}},
year = {2012}
}
@book{Genosko2012,
author = {Genosko, Gary},
doi = {10.1017/CCO9780511753657.008},
isbn = {9780511753657},
pages = {151--169},
title = {{Deleuze and Guattari}},
year = {2012}
}
@article{Division2009,
author = {Division, U C Berkeley and Paper, Working and Gruber, Susan and Laan, Mark J Van Der},
title = {{University of California , Berkeley Targeted Maximum Likelihood Estimation : A Gentle Introduction Targeted Maximum Likelihood Estimation : A Gentle Introduction}},
year = {2009}
}
@article{Griffiths:2004,
abstract = {A first step in identifying the content of a document is determining which topics that document addresses. We describe a generative model for documents, introduced by Blei, Ng, and Jordan [Blei, D. M., Ng, A. Y. {\&} Jordan, M. I. (2003) J. Machine Learn. Res. 3, 993-1022], in which each document is generated by choosing a distribution over topics and then choosing each word in the document from a topic selected according to this distribution. We then present a Markov chain Monte Carlo algorithm for inference in this model. We use this algorithm to analyze abstracts from PNAS by using Bayesian model selection to establish the number of topics. We show that the extracted topics capture meaningful structure in the data, consistent with the class designations provided by the authors of the articles, and outline further applications of this analysis, including identifying ``hot topics'' by examining temporal dynamics and tagging abstracts to illustrate semantic content.},
author = {Griffiths, Thomas L and Steyvers, Mark},
doi = {10.1073/pnas.0307752101},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Gibbs,LDA,dirichlet,lda,mcmc,topic,topics},
mendeley-tags = {dirichlet,lda,mcmc,topics},
number = {Suppl 1},
pages = {5228--5235},
title = {{Finding scientific topics}},
url = {http://www.pnas.org/content/101/suppl.1/5228.abstract},
volume = {101},
year = {2004}
}
@article{Perona,
author = {Perona, P.},
doi = {10.1109/CVPR.2005.16},
isbn = {0-7695-2372-2},
journal = {2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)},
pages = {524--531},
publisher = {Ieee},
title = {{A Bayesian Hierarchical Model for Learning Natural Scene Categories}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1467486},
volume = {2}
}
@article{Dragan2013,
author = {Dragan, Anca D. and Lee, Kenton C.T. and Srinivasa, Siddhartha S.},
doi = {10.1109/HRI.2013.6483603},
isbn = {978-1-4673-3101-2},
journal = {2013 8th ACM/IEEE International Conference on Human-Robot Interaction (HRI)},
keywords = {1,a predictable and a,above,action interpretation,center,day-to-day,expected handwriting vs,fig,formalism,handwriting,human-robot collaboration,legible,legible trajectory of a,manipulation,motion planning,predictable,robot,s hand,tory optimization,trajec-},
month = {mar},
pages = {301--308},
publisher = {Ieee},
title = {{Legibility and predictability of robot motion}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6483603},
volume = {1},
year = {2013}
}
@book{Cowling2003b,
author = {Cowling, Michael and Sitte, Renate},
booktitle = {Pattern Recognition Letters},
editor = {{Wysocki, Tadeusz A. and Darnell, Michael and Honary}, Bahram},
number = {15},
pages = {31--46},
publisher = {Springer US},
title = {{Advanced Signal Processing for Communication Systems: Recognition of environmental sounds using speech recognition techniques}},
url = {http://dx.doi.org/10.1007/0-306-47791-2{\_}3},
volume = {24},
year = {2003}
}
@article{Cowling2003a,
author = {Cowling, Michael and Sitte, Renate},
doi = {10.1016/S0167-8655(03)00147-8},
editor = {{ysocki, Tadeusz A. and Darnell, Michael and Honary}, Bahram},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {acoustic signal processing,audio signal processing,environmental sound recognition,joint time-frequency feature extraction,non-speech sound recognition},
month = {nov},
number = {15},
pages = {2895--2907},
title = {{Comparison of techniques for environmental sound recognition}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865503001478},
volume = {24},
year = {2003}
}
@inproceedings{Chu2006,
author = {Chu, Selina and Narayanan, Shrikanth and Kuo, C.-C. Jay and Matariu, Maja J.},
booktitle = {IEEE Conference on Multimedia and Expo},
pages = {885--888},
title = {{Where am I? Scene recognition for mobile robots using audio features}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4036742},
year = {2006}
}
@article{Briggs2013,
abstract = {Bird sound data collected with unattended microphones for automatic surveys, or mobile devices for citizen science, typically contain multiple simultaneously vocalizing birds of different species. However, few works have considered the multi-label structure in birdsong. We propose to use an ensemble of classifier chains combined with a histogram-of-segments representation for multi-label classification of birdsong. The proposed method is compared with binary relevance and three multi-instance multi-label learning (MIML) algorithms from prior work (which focus more on structure in the sound, and less on structure in the label sets). Experiments are conducted on two real-world birdsong datasets, and show that the proposed method usually outperforms binary relevance (using the same features and base-classifier), and is better in some cases and worse in others compared to the MIML algorithms.},
archivePrefix = {arXiv},
arxivId = {1304.5862},
author = {Briggs, Forrest and Fern, Xiaoli Z. and Irvine, Jed},
eprint = {1304.5862},
month = {apr},
pages = {6},
title = {{Multi-Label Classifier Chains for Bird Sound}},
url = {http://arxiv.org/abs/1304.5862},
volume = {28},
year = {2013}
}
@book{Boyd2004,
address = {Cambridge},
author = {Boyd, Stephen and Vandenberghe, Lieven},
doi = {10.1017/CBO9780511804441},
isbn = {9780511804441},
publisher = {Cambridge University Press},
title = {{Convex Optimization}},
url = {http://ebooks.cambridge.org/ref/id/CBO9780511804441},
year = {2004}
}
@article{Boucheron2008,
author = {Boucheron, Laura E. and {De Leon}, Phillip L.},
doi = {10.1109/ICSES.2008.4673475},
isbn = {978-83-88309-47-2},
journal = {2008 International Conference on Signals and Electronic Systems},
pages = {485--488},
publisher = {Ieee},
title = {{On the inversion of Mel-frequency cepstral coefficients for speech enhancement applications}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4673475},
year = {2008}
}
@article{Biederman1987,
author = {Biederman, I},
issn = {0033-295X},
journal = {Psychological review},
keywords = {Attention,Concept Formation,Discrimination Learning,Form Perception,Humans,Orientation,Pattern Recognition, Visual},
month = {apr},
number = {2},
pages = {115--47},
pmid = {3575582},
title = {{Recognition-by-components: a theory of human image understanding.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/3575582},
volume = {94},
year = {1987}
}
@article{Berlow2009,
abstract = {Darwin's classic image of an "entangled bank" of interdependencies among species has long suggested that it is difficult to predict how the loss of one species affects the abundance of others. We show that for dynamical models of realistically structured ecological networks in which pair-wise consumer-resource interactions allometrically scale to the (3/4) power--as suggested by metabolic theory--the effect of losing one species on another can be predicted well by simple functions of variables easily observed in nature. By systematically removing individual species from 600 networks ranging from 10-30 species, we analyzed how the strength of 254,032 possible pair-wise species interactions depended on 90 stochastically varied species, link, and network attributes. We found that the interaction strength between a pair of species is predicted well by simple functions of the two species' biomasses and the body mass of the species removed. On average, prediction accuracy increases with network size, suggesting that greater web complexity simplifies predicting interaction strengths. Applied to field data, our model successfully predicts interactions dominated by trophic effects and illuminates the sign and magnitude of important nontrophic interactions.},
author = {Berlow, Eric L and Dunne, Jennifer a and Martinez, Neo D and Stark, Philip B and Williams, Richard J and Brose, Ulrich},
doi = {10.1073/pnas.0806823106},
issn = {1091-6490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Animals,Biomass,Body Size,Ecology,Extinction, Biological,Feeding Behavior,Food Chain,Models, Theoretical,Population Dynamics},
month = {jan},
number = {1},
pages = {187--91},
pmid = {19114659},
title = {{Simple prediction of interaction strengths in complex food webs.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2629248{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {106},
year = {2009}
}
@article{Beltr2012,
author = {Beltr, Jessica},
pages = {334--343},
title = {{Environmental Sound Recognition by Measuring Significant Changes in the Spectral Entropy}},
year = {2012}
}
@article{Blei2003,
author = {Blei, D and Ng, A and {and M. Jordan}},
file = {:home/arnold/Documents/MendeleyDesktop/Library/Blei, Ng, and M. Jordan{\_}2003{\_}Latent Dirichlet Allocation.pdf:pdf},
issn = {1532-4435},
journal = {Jmlr},
pages = {993--1022},
title = {{Latent Dirichlet Allocation}},
volume = {3},
year = {2003}
}
@article{Anderson2010,
author = {Anderson, Examiner W J},
title = {{MATH 323 Probability}},
year = {2010}
}
@article{Angeli2008,
author = {Angeli, Adrien and Doncieux, Stephane and Meyer, Jean-Arcady and Filliat, David},
doi = {10.1109/ROBOT.2008.4543475},
isbn = {978-1-4244-1646-2},
journal = {2008 IEEE International Conference on Robotics and Automation},
month = {may},
pages = {1842--1847},
publisher = {Ieee},
title = {{Real-time visual loop-closure detection}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4543475},
year = {2008}
}
@inproceedings{Yao2009,
 author = {Yao, Limin and Mimno, David and McCallum, Andrew},
 title = {Efficient Methods for Topic Model Inference on Streaming Document Collections},
 booktitle = {Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
 series = {KDD '09},
 year = {2009},
 isbn = {978-1-60558-495-9},
 location = {Paris, France},
 pages = {937--946},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1557019.1557121},
 doi = {10.1145/1557019.1557121},
 acmid = {1557121},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {inference, topic modeling},
} 