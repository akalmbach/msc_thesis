%!TEX root = thesis.tex

\chapter{Spatial Awareness for Robot Cameras: Predicting images from topic context} \label{ch:spatial-prediction}

In this chapter we consider learning a word distribution function which is suitable for topic modelling as well as a function to map such a word distribution back to the original domain. Such a feature representation, combined with a spatio-temporal topic model opens a broad range of applications where we would like to develop a high-level understanding of the data, but only want to view it in the origina data domain. Specifially, we consider modelling the views available to a Pan, Tilt, Zoom (PTZ) camera. Our word distribution function is a novel convolutional autoencoder (CAE), which takes an image as input, produces a PMF as its encoding, and then attempts to reconstruct the original image from this encoding. By learning the image reconstruction function, we enable a user of the PTZ camera to inspect the topic distributions $\Phi$ directly as images. These topic images provide an intuitive decomposition of the parts of the high-level features of the scene, and a novel interface to explore views by combing them in different proportions.

Further, we employ a similar approach to Ch.~\ref{sec:plankton-seasonal} to recover the MAP word distributions given the learned topic prior for a neighborhood. Together with the reconstruction function, this enables us to directly view the data captured by the topic model and the data that is lost by constraining ourselves to a sparse, low-dimensional representation. Finally, we develop an energy-based topic mapping model that exploits the spatial smoothness of the learned topic priors to make topic prior predictions for unobserved views. Combining all of these pieces together, we develop a system to predict the images a PTZ camera will encounter when it configures itself for novel views.

% Due to their relative ease of training, we consider using a convolutional autoencoder (CAE) model for this task. A CAE is a CNN that is trained to reproduce its inputs while also encoding them in a hidden representation, usually of much smaller size than the inputs. Typically CAEs feature a sequence of convolutions and downsampling operations, with a smallest hidden dimension of dimensionality in the hundreds. Although many variants exist, the simplest version, uses the loss $\mathcal{L}_{ae} = (x - d(e(x))^2$ where $e(x)$ is the encoding network and $d(z)$ the decoding network. After training, the representation $e(x)$ may be used as a feature vector for other tasks, or else included pre-trained as the first few layers of model for a supervised learning task that is difficult to train from scratch.

\section{Learning an invertible word distribution function} \label{sec:learning-features}

\subsection{Background}

Our work on learning an invertible feature function draws from the extensive literature on Deep Convolutional Autoencoders. The simplest version of an autoencoder is a parameterized model, trained to reproduce it's inputs with minimal error. Most often, autoencoders feature a `bottleneck' intermediate representation with many fewer dimensions that the input space; this helps ensure that the model learns a useful representation of the data rather than a trivial solution. The bottleneck layer divides the autoencoder into an encoding function $e$ and a decoding function $d$, while the activations for a particular input at the bottleneck layer are used as it's encoding. We will write $I_x$ for the image at point $x$, $h_x = e(I_x)$ for its hidden encoding, and $\hat{I}_x = d(h_x)$ for the `autoencoded' representation of the image through the network. The standard account of autoencoder training considers the random variables $\mathbf{I}$ and $\mathbf{\hat{I}}$ and seeks to minimize $L_{ae} = -\log p(\mathbf{I} | \mathbf{\hat{I}})$ given a modelling choice for $p(\mathbf{I} | \mathbf{\hat{I}})$. When $I$ is considered an unconstrained real-valued tensor, it is common to assume $\mathbf{I}|\mathbf{\hat{I}} \sim \mathcal{N}(\mathbf{\hat{I}}, \Sigma)$, which leads to the standard MSE objective $\mathcal{L}_{ae} \propto \sum_x \| I_x - \hat{I_x}\|_2^2$ as an unbiased estimator for $L_{ae}$ \citep{vincent2010stacked}.

A key strength of autoencoder models is that the only training data they require is the images themselves, and therefore large enough datasets to support these large models are relatively easy to produce. Because of this, as larger, more elaborate, supervised CNN architectures have improved and the availablility of high-performance GPUs has increased, it has become tempting to use the similarly large networks for to get more exact autoencoders without increasing bottleneck sizes. However, it is often the case that a very powerful decoder will learn to largely ignore the inputs and instead memorize a few indistinct outputs to use for every input \citep{vincent2010stacked,Chen2016}\footnote{This problem has been discussed most prominently under the name `posterior collapse' within the Variational Autoencoder literature, referring to models that learn $e$ as an approximate posterior $q(h | I) \approx p(h)$.}. In the applications discussed in this chapter, avoiding such collapsed encodings is crucial -- for the topic model to learn the spatial arrangement of the scene the learned feature representation should encode as much about the differences between images as possible rather than deferring to the decoder network. Many recent models partially solve this problem by using recurrent networks rather than single-shot encodings to progressively encode and refine their representations, for example \citep{OordPixelCNN,Chen2016,Gregor2016}. These approaches have achieved some of the most convincing results to date, especially in the domain of high-resolution image encodings. These methods are not, in principal, exclusive with ours, however they add significant complexity to the possible set of architectures, so we choose to address this issue by other means.

Beyond the common collapsed encoder issue, the most notable novel requirement for our feature encoding compared to the traditional situation is that features must be discrete-valued, whereas most autoencoders produce real-vector valued encodings. Discrete-valued autoencoder representations have been previously considered in the context of image compression by \citep{AgustssonSoftToHardVQ,vqvae2017}. Both of these approaches rely on a codebook (fixed beforehand by \citep{AgustssonSoftToHardVQ} and learnt by \citep{vqvae2017}) of closest centers to discretize the encoding space. In contrast, we design an autoencoder model where the latent space is taken to be the parameter of a categorical or multinomial random variable. This has the advantage of fitting directly with the histogram of words document model assumed by LDA, while simultaneously considerably simplifying the required modifications to standard autoencoder architectures.

\subsection{Categorical Information Boost Autoencoder -- CIB-AE}

In order to learn a representation compatible with topic modelling, we develop a modified CAE, the Categorical Information Boost Autoencoder (CIB-AE), where the encoding can be seen as the parameter of a categorical or multinomial distribution so that it gives an image's word distribution directly. Similar to DNNs used for classification tasks, we simply apply the softmax function to the activations of a standard CNN architecture to produce such a distribution. In doing so, we discretize the feature space by considering each dimension of the representation as a discrete `word', and preserve detail in our model by modelling the full feature distribution directly.

Through the Dirichet priors on topics and topic-word distributions, LDA descendants such as the spatio-temporal topic model make the implicit assumption that documents have sparse word distributions. Although we do not specify exactly how sparse such a sparse mixture of sparse multinomials is, we have found in practice that simply renormalizing the features of conventional autoencoders produces word-distributions that are too uniform to be modelled productively by LDA. Therefore, CIB-AE employs an additional regularization term in its loss,
\begin{equation}
    L_{CIB} = -\log p(\mathbf{I} | \mathbf{\hat{I}}) - \lambda MI\left(\mathbf{I}, W\right)
\end{equation}
where $W \sim e(I)$ and $MI$ stands for the mutual information, a measure of the degree if dependence between two random variables. Choosing a large value for $\lambda$ will result in representations that favor high mutual information between images and encoding distributions, \emph{ie.} those where training images are as distinguishable as possible in encoding space. On the other hand, choosing a small $\lambda$ will result in encodings which may be very similar, but result in a low MSE. In other words this regularization term ensures that encoder collapse does not occur as it does with many other autoencoders. We note an interesting connection to the literature on VAEs, which uses an approximate prior $q(W|\mathbf{I}) \approx p(W)$. Rewriting $MI\left(\mathbf{I}, W\right)$ as $D_{KL}[(I,W) \| IW ]$, our objective and the VAE objective take extremely similar forms: In particular, $L_{CIB} = -\log p(\mathbf{I} | \mathbf{\hat{I}}) - \lambda D_{KL}[(\mathbf{I},W) \| IW ]$ while $L_{VAE} = -\log p(\mathbf{I} | \mathbf{\hat{I}}) + \lambda D_{KL}[q(W|\mathbf{I}) \| p(W) ]$.

To illustrate how our loss works, consider another way to rewrite the mutual information: $ MI\left(\mathbf{I}, W\right) = H[W] - H[W | \mathbf{I}] $ where $H[W | \mathbf{I}]$ signifies the conditional entropy. By subtracting the mutual information from the loss, the $H[W | \mathbf{I}]$ term ensures that individual encodings must have low-entropy, and are therefore sparse, while the $-H[W]$ term ensures that the overall distribution of encodings is high-entropy, and therefore as uniform as possible. The combination of these two terms means that the encodings must use different dimensions of the feature space from one another.

Reorganizing the mutual information in this also way reveals how to estimate it from a minibatch of size $M$:
\begin{equation}
\begin{split}
    -MI\left(\mathbf{I}, W\right) = H[W | \mathbf{I}] - H[W] = \\
    \mathbb{E}_{w \sim e(\mathbf{I})}\left[ -\log p(w) \right] - \mathbb{E}_\mathbf{I}\left[ \mathbb{E}_{w \sim e(\mathbf{I})}\left[ -\log p(w) \right] \right] \approx \\
    -\frac{1}{M} \sum_{m=1}^M \sum_{d=0}^V e(I_m)_d (\log e(I_m)_d) +
            \sum_{d=0}^V \overline{e}_d \log \overline{e}_d
    \end{split}
\end{equation}
where $\overline{e}$ signifies the average encoding for the $M$ images in the batch. Clearly, estimating an expectation over the space of all possible training images with a minibatch of few dozen or at most a few hundred examples is crude. Nevertheless, the terms of this approximation naturally fit our goal of producing maximally sparse, maximally distinct encodings for as much of the training data as possible. To summarize our approximation of the CIB loss
\begin{equation}
L_{CIB} \approx \mathcal{L}_{CIB} = \frac{1}{M} \left( \sum_{i=1}^M \frac{(I_m - \hat{I}_m)^2}{|I|}
                                    - \lambda \sum_{d=0}^V e(I_m)_d (\log e(I_m)_d) \right)+
                                     \lambda \sum_{d=0}^V \overline{e}_d \log \overline{e}_d
\end{equation}
where $|I|$ denotes the (fixed) number of pixel for an input image. We find in practice that when the image space is normalized to the range $[-a,a]$, $\lambda = \frac{(2a)^2}{\log V}$ is a good choice, where $V$ is the dimension of the encodings \footnote{Note $\log V$ is the maximum possible entropy for a categorical distribution of dimension $V$, so this choice represents letting the MSE and MI terms have `equal' weight when both are in their worst cases}.

Our specific choice of architecutre is inspired by the feature layers of VGG \citep{Simonyan14c}. In particular, following the approach of Segnet \citep{BadrinarayananK15} we choose a VGG-like encoder, with a symmetric decoder of transposed convolutional layers. This style of encoder uses a sequence of convolution `blocks', which each ultimately either downsample (Max Pool) or upsample (Nearest Neighbor) the activations by a factor of two. We add an additional dropout and an additional softmax layer to the encodings to constrain the real-valued activations of a Segnet style encoding to a simplex. Our feature representation downsamples the image spatially by a factor of 32, while producing a 512 channel feature activation for each spatial region. All experiments are conducted with $128 \times 128 \times 3$ input images, resulting in $4 \times 4 \times 512 = 8192$ dimensional encodings distributions. The full details of our architecture are found in App.~\ref{ch:cibae-arch}.

\section{Spatial topic prediction model}

Beyond simply finding any low-dimensional representation of the image space a robotic camera may encounter, the spatio-temporal topic model is able to learn such a representation that is also spatially coherent. We situate the topic model's neighborhoods in the space of views, meaning that the resulting learned topic priors will form a map of the visually distinct regions in pan-tilt-zoom space.

Because our model is designed to make the topic priors locally smooth, we are able to develop a simple spatial prediction scheme for topic priors. Our spatial prediction model is fast to fit and run online, taking camera pose, learned topic prior pairs as training data, and predicting the topic prior for a test pose. We take inspiration from a nearest-neighbor model: in the simplest case our model is to predict the topic prior of the nearest training point for a test point. However, we envision using this system with only a few dozen training points, and would prefer predictions to degrade towards the global average topic distribution than a poor estimate of the topic prior when the test point is not near any training point.

To achieve this property, we make our predictions by a weighted sum over the topic distributions we have observed so far, where far training points have low weight and near training points have high weight. Rather than performing such a prediction with a direct mixture and renormalizing, we choose to implement the weighted sum in terms of the log-probability of each training topic, and then use a softmax to return to the natural probability space. Note that if the mixture of log-probabilities belongs to a valid probability distribution (\emph{eg.} all the weight is put on a single topic prior), the softmax function returns exactly that distribution.


We define the weights in terms of a Boltzmann distribution (\emph{ie.} the softmax of negative energy), using the distance to each observed topic prior as the energy. Let $D_X(x)$ denote the distance vector from point $x$ to each point $x_i \in X$. This choice of weight function has the appealing property that if any entry of $D_X(x)$ is much smaller than the rest, all of the weight will be put on that training point. Further, with this weight function by controlling the scale of the distances we can control the sparsity of the weight vector; when the average distance is low the weights will be fairly uniform, while when the average distance is high, the weights will be sparser. We choose to use the plain $\mathit{L}_2^2$ distance between pan-tilt-zoom points as our distance function. However, in order to appropriately scale the distances and balance dimensions which may not truly incur equal loss of relevance for higher distances, we first scale the input points by the vector $w$ \footnote{Our distance could also be called a Mahalanobis distance (normally formualted in terms of the covariance matrix $\Sigma = ww^T$)}. To summarize our prediction model given $N$ training points $X$ each paired with an observed topic prior in $\Theta_X$:
\begin{equation}
\begin{split}
    D_X(x) = 
    \begin{bmatrix}
    \|w^Tx - w^T x_1 \|_2^2, & \|w^Tx - w^T x_2 \|_2^2, \ldots, \|w^Tx - w^T x_N \|_2^2
    \end{bmatrix}\\
    W(x) = \sigma \left( -D_{X}(x) \right)\\
    \theta(x) = \sigma \left( W(x) \log \left(\Theta_X \right) \right)
\end{split}
\end{equation}

Finally, we note that setting $w$ by hand is difficult as the best choice may vary from environment to environment, and is unnecessary, as both fitting and prediction are extremely efficient with this model. Instead, we choose to set $w$ by 2-fold cross validation. We exhaustively cover the 3d grid of values for each dimension of $w$, fitting and predicting for half the data for every parameter candidate. Then each parameter candidate is evaluated according to the mean Jenson-Shannon Divergence ($D_{JS}$) of its topic-prior predictions compared to the held-out topic-prior predictions, where $D_{JS}$ is defined as:
\begin{equation}
D_{JS}\left[ p, q \right] = \frac{1}{2} \left(D_{KL}[p, m] + D_{KL}[q, m]\right)
\end{equation}
where $m = \frac{p+q}{2}$. While $D_{KL}$ would most directly measure the information lost by approxmating the true topic distribution with the prediction, $D_{JS}$ is a similar alternative, with the attractive properties of being symmetric, positive and bounded above that $D_{KL}$
 does not possess. We note that both this objective and our model are differentiable in $w$ given a fixed training set, however we found that given the small size of our training set, an exhaustive search solution was often preferable to gradient-based methods which in principle could give a more precise solution, but in practice result in overfitting.

\section{Experiments}

\subsection{Simulated Pan-Tilt-Zoom Camera Dataset}
\begin{figure}
    \centering
    \includegraphics[width=0.85\textwidth]{figures/ptz/sun360_9x.png}
    \caption{Example 360 degree panoramas from the SUN360/street dataset.}
    \label{fig:sun360_ex}
\end{figure}

We demonstrate our approach with a simulated outdoor pan-tilt-zoom camera, using data from the `street' category of the SUN360 dataset \citep{SUN360}. Specifically SUN360/street comprises a dataset of 161 360 degree panoramas projected into rectangular $9104 \times 4552$ pixel images (Fig.~\ref{fig:sun360_ex}). These panoramas are taken in various cities and towns across the world, from street corners and from car-mounted cameras. We parameterize views in terms of tilt angle $T$, pan angle $P$, zoom factor $Z$. Given a base field-of-view (fov) $A_0$ and a target camera resolution, we compute the projection to the desired perspective $R = R_TR_P$ where
\begin{equation}
\begin{split}
R_T =& 
\begin{bmatrix}
1 & 0 & 0\\
0 & \cos(T) & -\sin(T)\\
0 & \sin(T) & \cos(T)
\end{bmatrix}\\
R_P =&  
\begin{bmatrix}
\cos(P)        & \sin(P) - \sin(T)            & \sin(P)\cos(T)\\
\sin(P)\sin(T) & \cos(P)\cos(T)^2(1 -\cos(P)) & \cos(T)\sin(T)(1-\cos(P))\\
\sin(P)\cos(T) & \sin(T)\cos(T)(1-\cos(P))    & \cos(P)+\sin(T)^2(1-\cos(P))
\end{bmatrix}
\end{split}
\end{equation}
Using the fov $A = A_0/Z$ and output image resolution, we represent the output pixel locations as pan-tilt rotations from the center $P$, $T$. Then, projection proceeds as rotation by $R$ and a lookup in the full panorama image for each pixel in the output image. In all our experiments we consider a base fov of 76 deg and zoom factors $Z \in (1, 12]$, similar to a standard commodity camera. Further, to avoid wrapping angles, we consider only $T \in [-60, 60]$ and $P \in [-120, 120]$ deg (SUN360 is designed so that the horizon is horizontal and vertically centered in all panoramas).

\subsection{Place-specific autoencoder}

A key advantage of the topic-model based approach is that it can be trained online, whereas this is not practical for large deep-networks. This means that rather than learning a single model which aims to produce a low-dimensional embedding of all possible images, our robotic camera system can learn a new, place specific model, and is thus required to represent a vastly simpler space of images.

Our approach delegates learning the globally-salient image embedding to the CIB-AE. In particular, we trained our feature model using epochs of 5000 random poses from 80 random SUN360/street panoramas. We found that choosing a new set of random training poses for each epoch was crucial, however we kept the validation set fixed throughout training. The perspective projection described in the previous section is time consuming for high-resolution images, especially for those that do not fit in GPU memory. Therefore we trained for 175 epochs on random rectangular crops from the warped panorama images, and then 25 more epochs on projections to fine-tune the results. We achieved our results using the adaptive learning rate optimizer Adam~\citep{KingmaAdam}, however found that a learning rate decay by a factor of 0.9 every 10 epochs was also necessary. Example input images and their autoencoded counterparts can be seen in Fig.~\ref{fig:cibae_encodings}. We found the mean entropy of image encodings from 1000 random poses from the test set to be 2.335 nats \footnote{While bits is the unit for information measured by entropy using $log_2$, nats is the unit when the entropy is calculated using $ln$}, or approximately equivalent to a uniform PMF over just 11 dimensions. On the other hand, the entropy of the mean encoding distribution from these 1000 examples was 8.769 nats, whereas the maximum possible entropy where all feature dimensions were used uniformly would have had an entropy of 9.011 nats.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/ptz/mc3ae_encodings}
    \caption{Random example input images (left) and CIB-AE reconstructions (right) for 4 worlds (rows) from the SUN360/street test set.}
    \label{fig:cibae_encodings}
\end{figure}

We train our view-predictor by alternating between observing new encoding distributions and refining a spatio-temporal topic model for a fixed period of 1 second. At the end of each refinement phase the spatial topic map is refit using normalized pan, tilt, and zoom-factor spatial coordinates and the corresponding MAP topic priors. Then, the partially trained spatial topic-prior map can be used to select the next training view (discussed in Sec.~\ref{sec:active-learning}) or it can be chosen uniformly at random.

When the image corresponding to the new view is extracted its word distribution is computed based on our trained CIB-AE. Because our Gibbs sampling inference method samples from the posterior for the topic assignment of a single word given all the others, it is necessary to discretize the encoding distribution rather than work with it directly. Note that despite an encoding dimension of 8192, it is possible to use a relatively small number of discrete observations without losing detail because individual images have low entropy encodings. In our experiments we define a constant document size of 2000 words; initial experimentation showed some degradation of performance below this level and little improvement above, however clearly the document size defines a tradeoff between the required number of sampling iterations and the amount of detail modelled.

We choose a spatio-temporal topic model with $K = 25$ topics and neighborhoods of size 10 deg $\times$ 20 deg $\times$ 6 (tilt $\times$ pan $\times$ zoom). This neighborhood size choice corresponds to the intuition that environments are semantically `smoother' in tilt than pan, and much smoother while zooming than moving the camera. We chose concentration parameter values $\alpha = 0.2,~\beta = 0.2$. We performed extensive cross validation to select these hyerparameter values values and certain other choices were found to perform much better for particular environments, however we did not find a combination of hyperparemeter values that was definitively much more performant than the others on average.

Since we aim to demonstrate a real-time system that can learn about its surroundings relatively quickly, we stop learning after 50 observations. As a first inquiry into what the model has learned, we directly decode the MAP topic distributions $\hat{\Phi}$ by passing them through the CIB-AE decoder (See Figs.~\ref{fig:pano-7-topics},\ref{fig:pano-15-topics},\ref{fig:pano-24-topics}). Most other applications of visual topic models employ non-invertible feature functions and choose to inspect topics by looking at the most representative training images. To our knowledge, this is the first application of a visual topic model where the topics can be fully decoded and inspected directly as images.

In addition, prediction with our spatial topic prior model is extremely efficient, so we are able to predict the topic priors for 10,000 random $P,T,Z$ views. In Figs.~\ref{fig:pano-7-map}, ~\ref{fig:pano-15-map}, and ~\ref{fig:pano-24-map} we present the resulting topic prior maps (one scatter plot per topic, in the same arrangement as the topic images), with the point $x_i$ omitted for topic $k$ if $\theta(x_i)_k < 0.01$, blue indicating $\theta(x)_k = 0.01$ and red $\theta(x)_k = 1$. Together, the decoded topics and view-topic maps provide an evocative graphical representation of how low-level textures combine into a higher level scene structure. After training, we collected 50 more views along with their inferred MAP topic priors (without performing any more refinement of $\Phi$). Among these observed views for each of the 35 test panoramas, we found $D_{JS}[\theta(x), \hat{\theta}]$ to be 0.2953 nats, while $D_{JS}[\mathrm{Unif}(K), \hat{\theta}]$ was on average 0.4122 nats\footnote{Recall $0 \leq D_{JS} \leq \ln(2) \approx 0.6931$}. This indicates that the topic mapping procedure is imperfect as is expected because of its weak geometric assumptions, nevertheless that it is better than a trivial strategy.

\begin{figure}{}{}
    \begin{minipage}{0.55\textwidth}
        \subfloat[][]{
            \includegraphics[width=\columnwidth]{figures/ptz/pano_7_topics.png}
            \label{fig:pano-7-topics}
        }
    \end{minipage} \hfill
    \begin{minipage}{0.44\textwidth}
        \subfloat[][]{{}
            \includegraphics[width=\columnwidth]{figures/ptz/pano_7_map.png}
            \label{fig:pano-7-map}
        } \\
        \subfloat[][]{
            \includegraphics[width=\columnwidth]{figures/ptz/pano_7_pano.jpg}
            \label{fig:pano-7-pano}
        }
    \end{minipage}
    
    \caption{Example spatial prediction model after 50 training observations. \protect\subref{fig:pano-7-pano}} Fish-eye view of the entire panorama \protect\subref{fig:pano-7-topics}, Direct decoding of each topic-distribution ($\Phi_k$), And \protect\subref{fig:pano-7-map} predicted mixture of each topic.
    \label{fig:pano-7}
\end{figure}

\begin{figure}
    \begin{minipage}{0.55\textwidth}
        \subfloat[][]{
            \includegraphics[width=\columnwidth]{figures/ptz/pano_15_topics.png}
            \label{fig:pano-15-topics}
        }
    \end{minipage} \hfill
    \begin{minipage}{0.44\textwidth}
        \subfloat[][]{
            \includegraphics[width=\columnwidth]{figures/ptz/pano_15_map.png}
            \label{fig:pano-15-map}
        } \\
        \subfloat[][]{
            \includegraphics[width=\columnwidth]{figures/ptz/pano_15_pano.jpg}
            \label{fig:pano-15-pano}
        }
    \end{minipage}
    
    \caption{Example spatial prediction model after 50 training observations. \protect\subref{fig:pano-15-pano}} Fish-eye view of the entire panorama \protect\subref{fig:pano-15-topics}, Direct decoding of each topic-distribution ($\Phi_k$), And \protect\subref{fig:pano-15-map} predicted mixture of each topic.
    \label{fig:pano-15}
\end{figure}

\begin{figure}
    \begin{minipage}{0.55\textwidth}
        \subfloat[][]{
            \includegraphics[width=\columnwidth]{figures/ptz/pano_24_topics.png}
            \label{fig:pano-24-topics}
        }
    \end{minipage} \hfill
    \begin{minipage}{0.44\textwidth}
        \subfloat[][]{
            \includegraphics[width=\columnwidth]{figures/ptz/pano_24_map.png}
            \label{fig:pano-24-map}
        } \\
        \subfloat[][]{
            \includegraphics[width=\columnwidth]{figures/ptz/pano_24_pano.jpg}
            \label{fig:pano-24-pano}
        }
    \end{minipage}
    
    \caption{Example spatial prediction model after 50 training observations. \protect\subref{fig:pano-24-pano}} Fish-eye view of the entire panorama \protect\subref{fig:pano-24-topics}, Direct decoding of each topic-distribution ($\Phi_k$), And \protect\subref{fig:pano-24-map} predicted mixture of each topic.
    \label{fig:pano-24}
\end{figure}

In addition to decoding the topic distributions themselves, we can predict the topic prior for a view and the corresponding encoding distribution given the topic distributions, and decode these predicted encodings. We evaluate our system based on the root mean squared error pixel value between the predictions. We call views that where we have run topic inference and topic map training but not topic refinement `observed'. At observed views our model predicts exactly the topic prior observed in that location, and so is able to represent the information encoded in the topic model without external loss.

As performance baselines, we trained two additional autoencoders with similarly constrained latent spaces offline, on the SUN360/street training set used to train the CIB-AE. First, we trained a plain CAE, without a categorical distribution latent space or additional loss terms beyond the MSE. We copy the architecture of the CIB-AE up to the softmax layer, and then add 3 additional fully connected layers of sizes 4096, 2048, and 1024, with ReLU and BatchNorm nonlinearities and with a final latent representation of 25 dimensions to complete the encoder. As with CIB-AE, the decoder is symmetric with the encoder. We initialilzed the weights of the encoder by copying them from the pretrained CIB-AE where possible, and trained the CAE using the same procedure as the CIB-AE.

In addition, we trained a the previously discussed VQ-VAE (VQ-VAE, \citep{vqvae2017}) with a codebook of size $25 \times 8192$, identical to the size of the topic model's $\Phi$ parameter. VQ-VAE is a state of the art autoencoder model designed to address many of the same issues as the topic model. However it cannot be trained online like the topic model, and so it must fit a globally salient representation into the fully compressed $25$-dimensional encoding sapce. We found that VQ-VAEs are very sensitive to architecture choices. For a fair comparison, it would be preferrable to use a VQ-VAE with spatial resolution $4 \times 4$ and feature size $512$ for each spatial block, however we could not escape posterior collapse using this representation. In fact, the only spatial resolution we found that attained reasonable results was the one the original authors demonstrated, therefore our comparisons are against a VQ-VAE with spatial resolution $32 \times 32$ and a feature size of just $8$ for each spatial block.

Fig.~\ref{fig:vs_vqvae} shows the root mean squared error for observed topic encodings and predicted topic priors compared to the real images at the corresponding views.

\begin{figure}
    \begin{center}
    \subfloat[]{
        \includegraphics[width=0.89\columnwidth]{figures/ptz/train_stacked_intext}
        \label{fig:encoding-obs-strip}
    }\\
    \subfloat[]{
        \includegraphics[width=0.89\columnwidth]{figures/ptz/test_stacked_intext}
        \label{fig:encoding-pred-strip}
    }
    \end{center}
    \caption{Observed \protect\subref{fig:encoding-obs-strip} and predicted encodings \protect\subref{fig:encoding-pred-strip} for random views from SUN360/street. From left to right the columns represent true image, CIB-AE autoencoding, topic prediction autoencoding, VQ-VAE latent prediction autoencoding, and plain CAE autoencoding. Many more examples are found in App.~\ref{ch:app-predictions}).
    }
    \label{fig:encoding-strip}
\end{figure}

\begin{figure}
    \begin{center}
    \subfloat[]{
        \includegraphics[width=0.48\columnwidth]{figures/ptz/vs_vqvae_obs.png}
        \label{fig:vs_vqvae_obs}
    }%
    \subfloat[]{
        \includegraphics[width=0.48\columnwidth]{figures/ptz/vs_vqvae_pred.png}
        \label{fig:vs_vqvae_pred}
    }
    \end{center}
    \caption{ Comparison of topics autoencoder with vqvae.
    \protect\subref{fig:vs_vqvae_obs} Reconstructions of observed images
    \protect\subref{fig:vs_vqvae_pred} Predicted images after training with 50 uniform random pan-tilt-zoom poses.
    (MSEs are computed with respect to pixel values in the range [0,255])
    }
    \label{fig:vs_vqvae}
\end{figure}

\subsection{Visual topic-mixture search}

\section{Discussion}
\subsection{Active learning}\label{sec:active-learning}
Active learning \citep{Jayaraman2017}


