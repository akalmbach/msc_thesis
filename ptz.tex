%!TEX root = thesis.tex

\chapter{Spatial Awareness for Robot Cameras: Predicting images from topic context} \label{ch:spatial-prediction}

In this chapter we consider learning a word distribution function which is suitable for topic modelling as well as a function to map such a word distribution back to the original domain. Such a feature representation combined with a spatio-temporal topic model opens a broad range of applications where we would like to develop a high-level understanding of data in its original domain rather than in a word-like representation. In this chapter, we consider modelling the views available to a robotic Pan, Tilt, Zoom (PTZ) camera. Our word distribution function is a novel convolutional autoencoder (CAE), which takes an image as input, produces a distribution of words as its encoding, and then attempts to reconstruct the original image from this encoding. By fitting a topic model within a domain where we can decode word distributions to images, we enable a user of the PTZ camera to inspect the topic distributions $\Phi$ directly as images. These topic images provide an intuitive decomposition of the high-level parts of the scene and a novel interface to explore views by combing them in different proportions.

Further, we employ a similar approach to Ch.~\ref{sec:plankton-seasonal} to recover the word distribution for a neighborhood given its estimated topic prior. Together with the reconstruction function, this enables us to directly view the data captured by the topic model and evaluate the data that is lost by constraining ourselves to a sparse, low-dimensional representation. Finally, we develop an energy-based topic-prior mapping model that exploits the spatial smoothness of the learned topic priors to make predictions for unobserved views. Combining all of these pieces together, we develop a system to predict the images the PTZ camera would encounter if it were to configure itself for a novel view. Fig.~\ref{fig:spatial-system} shows a diagram of the components of this system.

% \todo[inline]{I think you need a cartoon here illustration showing graphically how you want this to work or after the paragraph below}
\begin{figure}
    \centering
    \subfloat[][]{
        \includegraphics[width=0.88\textwidth]{figures/ptz/system_train}
        \label{fig:spatial-system-train}
    }\\
    \subfloat[][]{
        \includegraphics[width=0.52\textwidth]{figures/ptz/system_test}
        \label{fig:spatial-system-test}
    }\\
    \subfloat[][]{
        \includegraphics[width=0.42\textwidth]{figures/ptz/system_topics}
        \label{fig:spatial-system-topics}
    }
    \caption{Illustration of our proposed spatial image prediction system.
             \protect\subref{fig:spatial-system-train} We train an autoencoder to produce word distributions as its latent representation, which we model in turn with a spatio-temporal topic model trained for a specific scene. We also train a separate model to predict the topic prior based on the camera pose (its pan, tilt, zoom configuration) and previously observed topic assignments.
             \protect\subref{fig:spatial-system-test} At test time, we predict a topic prior for a new camera pose. The topic model then allows us to interpret this as a word distribution, which we can decode as an image using our autoencoder.
             \protect\subref{fig:spatial-system-topics} The topics themselves are also word distributions. Viewing these directly by passing them through our deocder allows us to visualize the high-level components of a scene.}
    \label{fig:spatial-system}
\end{figure}

\section{Learning an invertible word distribution function} \label{sec:learning-features}
\subsection{Background}
Our work on learning an invertible feature function draws from the extensive literature on Deep Convolutional Autoencoders. The simplest version of an autoencoder is a parameterized model, trained to reproduce its inputs with minimal error. Most often, autoencoders adopt a `bottleneck' intermediate representation with many fewer dimensions than the input space; this helps ensure that the model learns a useful representation of the data rather than a trivial one. The bottleneck layer divides an autoencoder into an encoding function $e$ and a decoding function $d$, while the activations for a particular input at the bottleneck layer are its encoding. We will write $I_x$ for the image at point $x$, $h_x = e(I_x)$ for its hidden encoding, and $\hat{I}_x = d(h_x)$ for the `autoencoded' representation of the image through the network. The standard account of autoencoder training considers the random variables $\mathbf{I}$ and $\mathbf{\hat{I}}$ and seeks to minimize $L_{ae} = -\log P(\mathbf{I} | \mathbf{\hat{I}})$ given a modelling choice for $P(\mathbf{I} | \mathbf{\hat{I}})$. When $I$ is considered an unconstrained real-valued tensor, it is common to assume $\mathbf{I}|\mathbf{\hat{I}} \sim \mathcal{N}(\mathbf{\hat{I}}, \Sigma)$, which leads to the standard MSE loss $\mathcal{L}_{ae} = \sum_x \| I_x - \hat{I_x}\|_2^2$, as an unbiased estimator for $L_{ae}$ \citep{vincent2010stacked}.

A key strength of autoencoder models is that the only training data they require are the images themselves, and therefore large datasets are relatively easy to produce. Because of this, as larger, more elaborate, supervised CNN architectures have improved and the availablility of high-performance GPUs has increased, it has become tempting to use the similarly large networks for to get more exact autoencoders without increasing bottleneck sizes. It is often the case, however, that a very powerful decoder will learn to largely ignore the inputs and their encodings and instead memorize a few indistinct outputs to use for every input, eventually learning identical encodings for all inputs \citep{vincent2010stacked,Chen2016}\footnote{This problem has been discussed most prominently under the name `posterior collapse' within the Variational Autoencoder literature, referring to models that learn $e$ as an approximate posterior $q(h | I) \approx p(h)$.}. In the applications discussed in this chapter, avoiding such collapsed encodings is crucial; for the topic model to learn the spatial arrangement of the scene, the learned feature representation should encode as much about the differences between images as possible rather than deferring to the decoder network. Many recent models partially solve this problem by using recurrent networks rather than single-shot encodings to progressively encode and refine their representations, for example \citep{OordPixelCNN,Chen2016,Gregor2016}. These approaches have achieved some of the most visually convincing results to date, especially in the domain of high-resolution image encodings. These methods are complimentary to ours, however we seek to demonstrate our approach with a more traditional convolutional autoencoder architecture to simplify the training process.
% \todo[inline]{This preceding sentence is a bit awkward and hard to decode, and once decoded I am not sure if  the argument is persuasive.}

Beyond the common collapsed encoder issue, the most notable novel requirement for our feature encoding is that features must be discrete-valued, whereas most autoencoders produce real-vector valued encodings. Discrete-valued autoencoder representations have been previously considered in the context of image compression, however these approaches have relied on vector-quantization based on a fixed codebook to discretize the encoding space \citep{AgustssonSoftToHardVQ}. The Vector Quantized Variational Autoencoder (VQ-VAE), is a notable recent improvement on this approach, that simplifies implementation by learning the codebook at the same time as fitting the parameters of the feature network \citep{vqvae2017}. Its quantization still relies on the Euclidean distances between feature vectors however, despite the Euclidien distance being only marginally meaningful in very high-dimensional spaces \citep{aggarwal2001metrics}.

In contrast, we design an autoencoder model where the latent space is taken to be the parameter of a categorical or multinomial random variable. In other words, we use the dimensions of the encoding as discrete feature values directly, rather than trying to discretize a high-dimensional space with a codebook. This has the advantage of fitting directly with the histogram of words document model assumed by LDA, while simultaneously considerably simplifying the required modifications to standard autoencoder architectures to achieve discretization, as well as sidestepping the issue of choosing a distance function for quantization.
% \todo[inline]{The story is OK, in fact very good, but stylistically I don't like using the reference ID's as if they were adjectives. In other places in our text you treat the citations as parenthetical itms.}

\subsection{Categorical Information Boost Autoencoder -- CIB-AE}

% \todo[inline]{Latter part of the explanation below is very dense and takes a lot of work to follow.  It could use some extra simplification iff you have time, but  it's not critical}
In order to learn a representation compatible with topic modelling, we develop a modified CAE, the Categorical Information Boost Autoencoder (CIB-AE), where the encoding can be seen as the parameter of a $V$-dimensional multinomial distribution, providing a natural link to the topic modelling methodology we have discussed throughout this thesis. We consider encoding an image by the normalized histogram of $N$ `words' -- the learned visual primitives which we can use to reconstruct an image. Similar to DNNs used for supervised classification tasks, we simply use a softmax layer to convert activations to a PMF parameterizing a multinomial distribution. In doing so, we discretize the feature space by considering each dimension of the representation as a discrete `word', and preserve detail in our model by modelling the full word distribution for an image. Fig.~\ref{fig:cibae-words} presents decodings of single words from our trained model.

\begin{figure}
    \subfloat[][]{
        \includegraphics[width=0.3\textwidth]{figures/ptz/words_spatial}
        \label{fig:words-spatial}
    }
    \subfloat[][]{
        \includegraphics[width=0.6\textwidth]{figures/ptz/top_words}
        \label{fig:words-top}
    }
    \caption{CIB-AE decodings of single `words', i.e. one-hot encoding vectors. An image is reconstructed as a normalized word-histogram.
             \protect\subref{fig:words-spatial} Each word is duplicated at $4 \times 4$ spatial locations within a $128 \times 128$ input image.
             \protect\subref{fig:words-top} The top 128 most common words from the training set, at their central position.}
    \label{fig:cibae-words}
\end{figure}

Through its Dirichlet priors, our topic modelling approach makes the assumption that each topic-word distribution and neighborhood-topic distribution is sparse (recall Fig.~\ref{fig:beta-pdf}). Consequently, the topic model's implicit assumption is that neighborhood-word distributions are somewhat sparse as well.
%\todo[inline]{I suggest you cite the prior section where this came up and also the references you cited earlier}
We have found in practice that simply renormalizing the features of a conventional autoencoder produces word-distributions that are too uniform to be modelled productively by LDA. CIB-AE employs an additional regularization term in its loss to ensure images are encoded using a variety of sparse word distribtions:
\begin{equation}
    L_{CIB} = -\log P(\mathbf{I} | \mathbf{\hat{I}}) - \lambda MI\left(\mathbf{I}, W\right)
\end{equation}
where $W \sim e(I)$ and $MI$ stands for the mutual information, a measure of the degree of dependence between two random variables. Choosing a large value for $\lambda$ will result in representations that favor high mutual information between images and encoding distributions, i.e. those where training images are as distinguishable as possible in encoding space. In other words this regularization term ensures that encoder collapse does not occur as it does with many other autoencoders \footnote{We note an interesting connection to the literature on VAEs, which use an approximate prior $q(W|\mathbf{I}) \approx p(W)$. Rewriting $MI\left(\mathbf{I}, W\right)$ as $D_{KL}[(I,W) \| IW ]$, our objective and the VAE objective take extremely similar forms: In particular, $L_{CIB} = -\log p(\mathbf{I} | \mathbf{\hat{I}}) - \lambda D_{KL}[(\mathbf{I},W) \| IW ]$ while $L_{VAE} = -\log p(\mathbf{I} | \mathbf{\hat{I}}) + \lambda D_{KL}[q(W|\mathbf{I}) \| p(W) ]$.} \footnote{Note that in the context of a Gaussian or Bernoulli model for $P(I|\hat{I})$, denoising autoencoders \citep{vincent2010stacked}, i.e. autoencoders with dropout, can also be considered to enhance the MI between images and encoding.}.

To illustrate how our loss works, consider another way to rewrite the mutual information:
\begin{equation}
MI\left(\mathbf{I}, W\right) = H[W] - H[W | \mathbf{I}]
\end{equation}
where $H[W | \mathbf{I}]$ signifies the conditional entropy. If we subtract the mutual information from the loss, the $H[W | \mathbf{I}]$ term ensures that individual encodings must have low entropy, and are therefore sparse, while the $-H[W]$ term ensures that the overall distribution of encodings is high entropy, and therefore as uniform as possible. The combination of these two terms means that the encodings must use different dimensions of the feature space from one another.

Reorganizing the mutual information in this also way reveals how to estimate it while performing stochastic gradient descent (SGD). In SGD, the gradient of the loss function over the whole dataset is estimated by taking the gradient of the loss over a portion of the dataset, called a minibatch. Given a minibatch of $M$ training images chosen at random from the dataset, we use a simple Monte-Carlo estimate to approximate the mutual information by rewriting it in terms of expectations:
% \todo[inline]{Requires some extra explanation for the casual reader. I do not think you have even used the term minibatch before!}
\begin{equation}
\begin{split}
    -MI\left(\mathbf{I}, W\right) = H[W | \mathbf{I}] - H[W] = \\
    \mathbb{E}_{w \sim e(I)}\left[ -\log P(w) \right] - \mathbb{E}_\mathbf{I}\left[ \mathbb{E}_{w \sim e(I)}\left[ -\log P(w) \right] \right] \approx \\
    -\frac{1}{M} \sum_{m=0}^{M-1} \sum_{d=0}^V h_{m,d} (\log(h_{m,d})) +
            \sum_{d=0}^V \overline{h}_d \log(\overline{h}_d)
    \end{split}
\end{equation}
Recall that we write $h_m = e(I_m)$, and here we use $\overline{h}$ to signify the average encoding for the $M$ images in the minibatch.
Clearly, estimating an expectation over the space of all possible training images with a minibatch of few dozen or at most a few hundred examples is crude. More precisely, this estimator is likely to have extremely high variance, and thus lead to slow convergence while optimizing the loss function. Nevertheless, the terms of this approximation naturally fit our goal of producing sparse, distinct encodings for as much of the training data as possible.

To summarize our approximation of the CIB loss
% \begin{equation}
% L_{CIB} \approx \mathcal{L}_{CIB} = \frac{1}{M} \left( \sum_{i=1}^M \frac{(I_m - \hat{I}_m)^2}{|I|}
%                                     - \lambda \sum_{d=0}^V e(I_m)_d (\log e(I_m)_d) \right)+
%                                      \lambda \sum_{d=0}^V \overline{h}_d \log \overline{h}_d
% \end{equation}
\begin{equation}
\begin{split}
& L_{CIB} \approx \mathcal{L}_{CIB} =\\
&  \frac{1}{M |I|} \left( \sum_{m=0}^{M-1} I_m - \hat{I}_m \right)^2
- \frac{\lambda}{M} \left( \sum_{m=0}^{M-1} \sum_{d=0}^{V-1} h_{m,d}\log(h_{m,d}) \right)
+ \lambda \sum_{d=0}^{V-1} \bar{h}_{d}\log(\bar{h}_{d})
\end{split}
\end{equation}
where $|I|$ denotes the (constant) number of pixels in an input image. In other words our loss is the MSE plus the average entropy of all encodings, minus the entropy of the average encoding. We find in practice that when the image space is normalized to the range $[-a,a]$, $\lambda = \frac{(2a)^2}{\log V}$ is a good choice, where $V$ is the dimension of the encodings \footnote{Note $\log V$ is the maximum possible entropy for a categorical distribution of dimension $V$, so this choice represents letting the MSE and MI terms have `equal' weight when both are in their worst cases}.

Our specific choice of architecture is inspired by the feature layers of VGG \citep{Simonyan14c}. In particular, following the approach of Segnet \citep{BadrinarayananK15} we choose a VGG-like encoder, with a symmetric decoder of transposed convolutional layers. This style of encoder uses a sequence of convolution `blocks', which each ultimately either downsample (Max Pool) or upsample (Nearest Neighbor) the activations by a factor of two (See Fig.~\ref{fig:cib-ae_modules}). We add an additional dropout and an additional softmax layer to the encodings to constrain the real-valued activations of a Segnet style encoding to a simplex. Our feature representation downsamples the image spatially by a factor of 32, while producing a 512 channel feature activation for each spatial region. All experiments are conducted with $128 \times 128 \times 3$ input images, resulting in $4 \times 4 \times 512 = 8192$ dimensional encodings. The details of our architecture are found in App.~\ref{ch:cibae-arch}, however we emphasize that by adding these two extra layers and the additional loss term, our approach can potentially be transferred to many existing autoencoder architectures and other domains.
% \todo[inline]{Could really use a figure above, if you have time}

\section{Predicting topic priors from camera poses} \label{sec:spatial-prediction-spatial-model}
% \todo[inline]{Someplace you need a picture of the camera, an explanation in simple terms of the problem and a cartoon.}
In this work, our goal is not only to encode and decode observed images, but also to use the encodings of observed images to predict the encodings of images at new locations, and decode these to produce a prediction of what the camera might see if it went to that location. We have seen in Chs.~\ref{ch:topic-models-examples} and~\ref{ch:plankton} how a spatio-temporal topic model may be used to learn spatially coherent topic-prior maps which encode the high-level categories of a dataset, as well as how to interpret the maximum likelihood word distribution associated with a particular topic prior. In this section, we focus on how to predict topic priors for a new location given those produced by our topic model.

In this work we consider the location associated with an observation to be the Pan, Tilt, and Zoom coordinates of a robotic camera (i.e. a PTZ camera). Rather than fully exploiting the overlap between views, we seek a more general method to predict topic priors, and therefore only make limited assumptions beyond those that the spatio-temporal topic model makes itself. Recall that our topic model learns topic priors that are spatially smooth. This means that without observing the image associated with a location, a reasonable guess for its topic prior is that it will be similar to the topic priors for nearby locations.

The simplest reasonable prediction for a new location would be to predict the topic prior for the nearest point which has been observed by the topic model. If our system were allowed to observe images for many locations, this strategy could be very accurate, however reconfiguring the camera and taking photos many times is time consuming, and we envision training our model with only a few dozen locations.

To better accommodate test locations that may be far from training locations, we adopt an energy-based approach to topic prior prediction. We predict the topic prior for a new location using a weighted average over the topic priors we have observed so far, where far observations have low weight and near observations have high weight. Rather than performing such a prediction with a direct mixture of observed topic-priors and renormalizing, we choose to implement the weighted sum in log-probability space, and then return the result to the natural probability space with a softmax. We find that in practice this approach helps to maintain sparcity of predicted topic priors, a property assumed by the topic model. Note that if the mixture of log-probabilities belongs to a valid probability distribution (\emph{eg.} all the weight is placed on a single training location), the softmax function returns exactly that distribution.

We define the weights in terms of a Boltzmann distribution (i.e. the softmax of negative energy), using the distance to each observed topic prior as the energy. Let $D_X(x)$ denote the distance vector from point $x$ to each point in the training set $x_i \in X$. This choice of weight function has the appealing property that if any entry of $D_X(x)$ is much smaller than the rest, all of the weight will be put on that training point. Further, with this weight function, by controlling the scale of the distances we can control the sparsity of the weight vector; when the average distance is low the weights will be fairly uniform, while when the average distance is high, the weights will be sparser. We choose to use the $\mathit{L}_2$ distance between pan-tilt-zoom points as our distance function. In order to appropriately scale the distances and balance dimensions which may not truly incur equal loss of relevance for higher distances, we first scale the input points by the element-wise product with the vector $w$ \footnote{Our distance could also be called a Mahalanobis distance (normally formualted in terms of the covariance matrix $\Sigma = ww^T$)}. To summarize our prediction model, given $N$ training points $X$ each paired with an observed topic prior in $\Theta_X$:
\begin{equation} \label{eqn:spatial-prediction}
\begin{split}
    D_X(x) = 
    \begin{bmatrix}
    % \|w \odot x - w \odot  x_1 \|_2^2, & \|w \odot x - w \odot  x_2 \|_2^2, \ldots, \|w \odot x - w \odot  x_N \|_2^2
    \|w \odot (x - x_1) \|_2, & \|w \odot (x - x_2) \|_2, \ldots, \|w \odot (x - x_N) \|_2
    \end{bmatrix}^T\\
    W(x) = \sigma \left( -D_{X}(x) \right)\\
    \theta(x) = \sigma \left( \log \left(\Theta_X \right) W(x) \right)
\end{split}
\end{equation}

Finally, we note that setting $w$ by hand is difficult as the best choice may vary from environment to environment, and is unnecessary, as both fitting and prediction are extremely efficient with this model. Instead, given a validation set $X^\star, \Theta^\star$ held out of training, we consider the information lost by using our model, measured by the mean Jensen-Shannon Divergence on the validation set \citep{lin1991divergence}:
\begin{equation}
L_w = \frac{1}{|X^\star|} \sum_{x \in X} D_{JS}\left[\theta(x) \| \Theta^\star_x \right]
 =  \frac{1}{|X^\star|} \sum_{x \in X} \frac{1}{2} \left(D_{KL}\left[\theta(x) \| m(x) \right] + D_{KL}\left[\Theta^\star_x \| m(x) \right]\right)
\end{equation}
where $m(x) = \frac{\theta(x)+\Theta^\star_x}{2}$. While $D_{KL}\left[\theta(x) \| \Theta^\star_x \right]$ would most directly measure the information lost by approxmating the true topic distribution with the prediction, $D_{JS}$ is additionally symmetric, positive, and bounded above, making it a more convenient loss function. This loss and our model are differentiable, so we are able to fit $w$ using gradient descent and a 2-fold cross-validation procedure. As a final note, although a more complex distance model may better match how topic-priors are spatially distributed, because we aim to train this model with only a few dozen examples we prefer a simpler model that is less prone to overfitting.

\section{Experiments}
Training our view prediction system requires several steps. First, we train the CIB-AE on a large dataset of views extracted from panorama images. Then, we train our topic model and spatial topic prior prediction model for a specific scene. This process involves alternating between observing new images and refining the topic model for a fixed period of one second. At the end of each refinement phase the topic prior map is refit based on the counts of topic assignments for each location that has been observed so far. Then, the locations for the next training images are chosen uniformly at random (we discuss our efforts to use the partially trained model to select subsequent training points in Sec~\ref{sec:active-learning}). Finally, after a fixed number of iterations the models are frozen, and query locations are passed through the topic prior map and the topic model to produce word distributions, which are then decoded as images by the CIB-AE.

\subsection{Simulated Pan-Tilt-Zoom Camera Dataset}
\begin{figure}
    \centering
    \includegraphics[width=0.85\textwidth]{figures/ptz/sun360_9x.jpg}
    \caption{Example 360 degree panoramas from the SUN360/street dataset.}
    \label{fig:sun360_ex}
\end{figure}

We demonstrate our approach with a simulated outdoor pan-tilt-zoom camera, using data from the `street' category of the SUN360 dataset \citep{SUN360}. Specifically SUN360/street comprises a dataset of 161 360 degree panoramas projected into rectangular $9104 \times 4552$ pixel images (Fig.~\ref{fig:sun360_ex}). These panoramas are taken in various cities and towns across the world, from street corners and from car-mounted cameras. The data are pre-processed so that the horizon is horizontal and vertically centered in all panoramas. We parameterize views in terms of tilt angle $T$, pan angle $P$, zoom factor $Z$. Given a base field-of-view (fov) $A_0$ and a target camera resolution, we compute the projection to the desired perspective $R = R_TR_P$ where
\begin{equation}
\begin{split}
R_T =& 
\begin{bmatrix}
1 & 0 & 0\\
0 & \cos(T) & -\sin(T)\\
0 & \sin(T) & \cos(T)
\end{bmatrix}\\
R_P =&  
\begin{bmatrix}
\cos(P)        & \sin(P) - \sin(T)            & \sin(P)\cos(T)\\
\sin(P)\sin(T) & \cos(P)\cos(T)^2(1 -\cos(P)) & \cos(T)\sin(T)(1-\cos(P))\\
\sin(P)\cos(T) & \sin(T)\cos(T)(1-\cos(P))    & \cos(P)+\sin(T)^2(1-\cos(P))
\end{bmatrix}
\end{split}
\end{equation}
Using the fov $A = A_0/Z$ and fixed output image resolution, we represent each output pixel location as a pan-tilt rotation from the center $P$, $T$. Then, projection proceeds as rotation by $R$ and a lookup in the full panorama image for each pixel in the output image. In all our experiments we consider a base fov of 76 deg and zoom factors $Z \in [1, 12]$, similar to a standard commodity camera. Further, to avoid wrapping angles, we consider only $T \in [-60, 60]$ and $P \in [-120, 120]$ degrees.

\subsection{CIB-AE Training}
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/ptz/mc3ae_encodings}
    \caption{Random example input images (left) and CIB-AE reconstructions (right) for 4 worlds (rows) from the SUN360/street validation set.}
    \label{fig:cibae_encodings}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.95\textwidth]{figures/ptz/word_frequency}
    \caption{Word-distribution, sorted by frequency, of the average encoding over 1000 test images (heavy black line) and normalized
             word-histograms for 3 random images (colored lines). A uniform histogram over the 8192 dimensions (dashed black line) represents the ideal average encoding and the worst-case individual encoding.}
    \label{fig:cibae-words-distribution}
\end{figure}

We trained our feature model using epochs of 5000 random views from amongst 80 SUN360/street panoramas. We found that choosing a new set of random training views for each epoch was crucial, however we kept the validation set fixed throughout training. The projection described in the previous section is time consuming for high-resolution images, especially for those that do not fit in GPU memory. Therefore we trained for 175 epochs on random square crops from the warped panorama images, and then 25 more epochs on projections to fine-tune the results. We achieved our results using the adaptive learning rate optimizer Adam~\citep{KingmaAdam}, however found that a learning rate decay by a factor of 0.9 every 10 epochs was also necessary. Example input images and their autoencoded counterparts can be seen in Fig.~\ref{fig:cibae_encodings}. We found the mean entropy of image encodings from 1000 random views from the test set to be 6.170 nats \footnote{While bits is the unit for information measured by entropy using $log_2$, nats is the unit when the entropy is calculated using $ln$}, or approximately equivalent to a uniform PMF over just 470 dimensions. On the other hand, the entropy of the mean encoding distribution from these 1000 examples was 8.192 nats, whereas the maximum possible entropy where all feature dimensions were used uniformly would have had an entropy of 9.011 nats. Typical word distributions and the average word distribution are illustrated in Fig.~\ref{fig:cibae-words-distribution}.

\subsection{View Prediction Training}
After training the word distribution function, we next train a spatio-temporal topic model and topic prior map specific to a single scene. A key advtantage of our approach is that it can be trained online, whereas this is not always practical for large deep-networks. This means that while the CIB-AE must be trained to encode images from the entire dataset, our most compressed representation of the scene, the topic priors, only has to capture images from a single scene, a considerable simplification.

When the image corresponding to a new view is extracted, its word distribution is computed based on the trained CIB-AE. Because our Gibbs sampling inference method samples from the posterior for the topic assignment of a single word given all the others, it is necessary to discretize the word distribution by sampling from it rather than work with it directly. Note that despite an encoding dimension of 8192, it is possible to use a relatively small number of discrete observations without losing detail because individual images have low entropy encodings. In our experiments we define a constant encoding resolution of 3000 words per image; initial experimentation showed some degradation of performance below this level and little improvement above, however clearly the document size defines a tradeoff between the required number of sampling iterations and the amount of detail modelled.

We choose a spatio-temporal topic model with $K = 25$ topics and neighborhoods of size 10 degrees $\times$ 20 degrees $\times$ 6 (tilt $\times$ pan $\times$ zoom). This neighborhood size choice corresponds to the intuition that environments are semantically `smoother' in pan than tilt, and much smoother while zooming than moving the camera. We chose concentration parameter values $\alpha = 0.2,~\beta = 0.2$. We performed extensive cross validation to select these hyerparameter values and certain other choices were found to perform much better for particular environments, however we did not find a combination of hyperparemeter values that was definitively much more performant than the others on average.

\subsection{Encoding and Prediction Results} \label{sec:spatial-prediction-results}
Since we aim to demonstrate a real-time system that can learn about its surroundings relatively quickly, we stop learning after 50 observations. As a first inquiry into what the model has learned, we directly decode the point estimates of the topic distributions $\hat{\Phi}$ by passing them through the CIB-AE decoder (See Figs.~\ref{fig:pano-0-topics},~\ref{fig:pano-1-topics},~\ref{fig:pano-2-topics}). Most other applications of visual topic models employ non-invertible feature functions and choose to inspect topics by looking at the most representative training images. To our knowledge, this is the first application of a visual topic model where the topics can be fully decoded and inspected directly as images. Interestingly, we find that for our autoencoder network, sparser encodings produce clearer images, while more uniform encodings produce muddled, mostly gray, blurry images (See Fig.~\ref{fig:map_vs_mle_topics}). This motivates us to use a \emph{maximum a posteriori} (MAP) point-estimate for $\hat{\Phi}$ (as well as for $\hat{\Theta}$) as opposed to the MLE discussed in Eqn.~\ref{eqn:mle_thetaphi}
\begin{equation} \label{eqn:map_thetaphi}
\begin{split}
\hat{\Theta}_{g(x), k} =&
    \begin{cases}
        \frac{N^k_{g(x)} + \alpha - 1}{\sum_{j=1}^K N^j_{g(x)} + \alpha - 1} & if~N^k_{g(x)} > 0 \\
        0 & otherwise
    \end{cases} \\
\hat{\Phi}_{k, w_i} =&
    \begin{cases}
        \frac{N^{w_i}_k + \beta - 1}{\sum_{v=1}^V N^v_k + \beta - 1} & if~N^k_{g(x)} > 0 \\
        0 & otherwise
    \end{cases}
\end{split}
\end{equation}

\begin{figure}
    \centering
    \subfloat[][]{
        \includegraphics[width=0.4\textwidth]{figures/ptz/map_topics.png}
        \label{fig:map_topics}
    }
    \subfloat[][]{
        \includegraphics[width=0.4\textwidth]{figures/ptz/mle_topics.png}
        \label{fig:mle_topics}
    }
    \caption{MAP \protect\subref{fig:map_topics} vs MLE \protect\subref{fig:mle_topics} point estimates for $\Phi$ trained with identical datasets, directly decoded as images. Note that the MAP estimate features more distinctive topic images.}
    \label{fig:map_vs_mle_topics}

\end{figure}

Prediction with our spatial topic prior model is extremely efficient, so we predict the topic priors for 10,000 random $P,T,Z$ views. In Figs.~\ref{fig:pano-0-map}, ~\ref{fig:pano-1-map}, and ~\ref{fig:pano-2-map} we present the resulting topic prior maps (one scatter plot per topic, in the same arrangement as the topic images), with the point $x_i$ omitted for topic $k$ if $\theta(x_i)_k < 0.01$, blue indicating $\theta(x)_k = 0.01$ and red $\theta(x)_k = 1$. Together, the decoded topics and view-topic maps provide an evocative graphical representation of how low-level textures combine into a higher level scene structure. After training, we collected 50 more views along with their inferred MAP topic priors (without performing any more refinement of $\Phi$). Among these observed views for each of the 35 test panoramas, we found $D_{JS}[\theta(x), \hat{\theta}]$ to be on average 0.1953 nats, while $D_{JS}[\mathrm{Unif}(K), \hat{\theta}]$ was on average 0.5122 nats\footnote{Recall $0 \leq D_{JS} \leq \ln(2) \approx 0.6931$}. This indicates that the topic mapping procedure is imperfect as is expected because of its weak geometric assumptions, however it does capture a significant portion of the information about the spatial distribution of topics.

\begin{figure}{}{}
    \centerline{
    \begin{minipage}{0.6\textwidth}
        \subfloat[][]{
            \includegraphics[width=\columnwidth]{figures/ptz/pano_0_topics.png}
            \label{fig:pano-0-topics}
        }
    \end{minipage} \hfill
    \begin{minipage}{0.6\textwidth}
        \subfloat[][]{
            \includegraphics[width=\columnwidth]{figures/ptz/pano_0_map.png}
            \label{fig:pano-0-map}
        } \\
        \subfloat[][]{
            \includegraphics[width=\columnwidth]{figures/ptz/pano_0_pano.jpg}
            \label{fig:pano-0-pano}
        }
    \end{minipage}
    }
    
    \caption{Example spatial prediction model after 50 training observations. \protect\subref{fig:pano-0-pano}} Fish-eye view of the entire panorama \protect\subref{fig:pano-0-topics}, Direct decoding of each topic-distribution ($\Phi_k$), And \protect\subref{fig:pano-0-map} predicted mixture of each topic.
    \label{fig:pano-0}
\end{figure}

\begin{figure}
    \centerline{
    \begin{minipage}{0.6\textwidth}
        \subfloat[][]{
            \includegraphics[width=\columnwidth]{figures/ptz/pano_1_topics.png}
            \label{fig:pano-1-topics}
        }
    \end{minipage} \hfill
    \begin{minipage}{0.6\textwidth}
        \subfloat[][]{
            \includegraphics[width=\columnwidth]{figures/ptz/pano_1_map.png}
            \label{fig:pano-1-map}
        } \\
        \subfloat[][]{
            \includegraphics[width=\columnwidth]{figures/ptz/pano_1_pano.jpg}
            \label{fig:pano-1-pano}
        }
    \end{minipage}
    }
    
    \caption{Example spatial prediction model after 50 training observations. \protect\subref{fig:pano-1-pano}} Fish-eye view of the entire panorama \protect\subref{fig:pano-1-topics}, Direct decoding of each topic-distribution ($\Phi_k$), And \protect\subref{fig:pano-1-map} predicted mixture of each topic.
    \label{fig:pano-1}
\end{figure}

\begin{figure}
    \centerline{
    \begin{minipage}{0.6\textwidth}
        \subfloat[][]{
            \includegraphics[width=\columnwidth]{figures/ptz/pano_2_topics.png}
            \label{fig:pano-2-topics}
        }
    \end{minipage} \hfill
    \begin{minipage}{0.6\textwidth}
        \subfloat[][]{
            \includegraphics[width=\columnwidth]{figures/ptz/pano_2_map.png}
            \label{fig:pano-2-map}
        } \\
        \subfloat[][]{
            \includegraphics[width=\columnwidth]{figures/ptz/pano_2_pano.jpg}
            \label{fig:pano-2-pano}
        }
    \end{minipage}
    }
    
    \caption{Example spatial prediction model after 50 training observations. \protect\subref{fig:pano-2-pano}} Fish-eye view of the entire panorama \protect\subref{fig:pano-2-topics}, Direct decoding of each topic-distribution ($\Phi_k$), And \protect\subref{fig:pano-2-map} predicted mixture of each topic.
    \label{fig:pano-2}
\end{figure}

In addition to decoding the topic distributions themselves, we can predict the topic prior for a view and the corresponding encoding distribution given the topic distributions, and decode these predicted encodings. We evaluate our system based on the root mean squared error (RMSE) pixel value between the predicted images and the true image at that view. We call views where we have run topic inference and topic map training but not topic refinement `observed'. At observed views our model predicts exactly the topic prior observed in that location, and so is able to represent the information encoded in the topic model without external loss.

As performance baselines, we trained two additional autoencoders with similarly constrained latent spaces on the full SUN360/street training set used to train the CIB-AE. First, we trained a plain CAE, without a categorical distribution latent space or additional loss terms beyond the MSE. We copy the architecture of the CIB-AE up to the softmax layer, and then add 3 additional fully connected layers of sizes 4096, 2048, and 1024, with ReLU and BatchNorm nonlinearities and with a final latent representation of 25 dimensions to complete the encoder. As with CIB-AE, the decoder is symmetric with the encoder. We initialilzed the weights of the encoder by copying them from the pretrained CIB-AE where possible, and trained the CAE using the same procedure as the CIB-AE.

In addition, we trained the previously mentioned vector-quantized variational autoencoder (VQ-VAE) of \citep{vqvae2017} with a codebook of size $25 \times 8192$, identical to the size of the topic model's $\Phi$ parameter. The VQ-VAE is a state of the art discrete-latent autoencoder model designed to address many of the same issues as the topic model, however it cannot be trained online like the topic model, and so it must fit a globally salient representation into the fully compressed $25$-dimensional encoding sapce. We found that VQ-VAEs are very sensitive to architecture choices. For a fair comparison, it would be preferrable to use a VQ-VAE with spatial resolution $4 \times 4$ and feature size $512$ for each spatial block, however we could not escape encoder collapse using this representation. In fact, the only spatial resolution we found that attained reasonable results was the one the original authors demonstrated, therefore our comparisons are against a VQ-VAE with spatial resolution $32 \times 32$ and a feature size of just $8$ for each spatial block.

For the baseline approaches, we apply a similar energy-based prediction model as for the topic-priors (Eqn.~\ref{eqn:spatial-prediction}), however as the encodings are not probability distributions, we perform the weighted sum of encodings directly rather than in log-probability space. In addition, the cross-validation metric for $w$ is the MSE of the predicted encodings rather than $D_{JS}$. Note that the baselines are not trained with spatial information, so we would not expect this method to achieve the same performance as with the spatially smooth topic model.

Fig.~\ref{fig:encoding-obs-strip} shows examples of the autoencoding performance of the various systems through their 25 dimensional representations. The leftmost column is the observed image and the second column is the CIB-AE autoencoding of that image. The subsequent columns are the decoded neighborhood-word distribution, the VQ-VAE autoencoding, and the CAE autoencoding. The CAE is not able to capture any detail in this regime, the gray images seen are characteristic of the encoder collapse problem mentioned previously. The VQ-VAE, unsurprisingly because of its high-spatial resolution and low feature dimensionality, captures the spatial structure of the image relatively well, but loses much of the color and distorts the texture. Finally, our approach is competetive with the VQ-VAE, featuring images that are more spatially distorted, but interestingly, that enhance the color contrast of its input images. Fig.~\ref{fig:encoding-pred-strip} is similar, however it shows unobserved views, predicted through the topic prediction model. For many more examples, see App.~\ref{ch:app-predictions}.

\begin{figure}
    \begin{center}
    \subfloat[]{
        \includegraphics[width=0.89\columnwidth]{figures/ptz/train_stacked_intext.png}
        \label{fig:encoding-obs-strip}
    }\\
    \subfloat[]{
        \includegraphics[width=0.89\columnwidth]{figures/ptz/test_stacked_intext.png}
        \label{fig:encoding-pred-strip}
    }
    \end{center}
    \caption{Observed \protect\subref{fig:encoding-obs-strip} and predicted encodings \protect\subref{fig:encoding-pred-strip} for random views from SUN360/street. From left to right the columns represent true image, CIB-AE autoencoding, topic prediction autoencoding, VQ-VAE latent prediction autoencoding, and plain CAE autoencoding. Many more examples are found in App.~\ref{ch:app-predictions}).
    }
    \label{fig:encoding-strip}
\end{figure}

Fig.~\ref{fig:vs_vqvae} shows the RMSE for observed and predicted topic priors decoded as images compared to the real images at the corresponding views. The low extent of the CAE whisker reveals that the conservative collapsed strategy of consistently predicting uniform grayish images is not necessarily bad in terms of pure pixel error, drawing out an important flaw of the pixel error as a performance measure at this level of compression. Nevertheless, to the extent that the RMSE distributions measure performance in our view prediction task, the data confirm our qualitative assesment that our method performs approximately as well as the VQ-VAE on average, and the extent of its typical errors is lower. Further, the more colorful, highher contrast images produced by our method are reflected by its higher variance than the more conservative VQ-VAE approach which tends to encode roughly the right shape, but obscure much of the color and texture.

\begin{figure}
    \begin{center}
    \subfloat[]{
        \includegraphics[width=0.48\columnwidth]{figures/ptz/vs_vqvae_obs.png}
        \label{fig:vs_vqvae_obs}
    }%
    \subfloat[]{
        \includegraphics[width=0.48\columnwidth]{figures/ptz/vs_vqvae_pred.png}
        \label{fig:vs_vqvae_pred}
    }
    \end{center}
    \caption{Performance comparison of our topic prior map with a VQ-VAE and a standard CAE.
    \protect\subref{fig:vs_vqvae_obs} Reconstruction performance for observed images (held out of training),
    \protect\subref{fig:vs_vqvae_pred} View prediction performance after training. The dashed line shows the median reconstruction error of the CIB-AE on this dataset. The box in this plot represents the interquartile range, while the whiskers represent the range of typical data (1.5 times IQR). RMSEs are computed for each image with respect to pixel values in the range [0,255].
    }
    \label{fig:vs_vqvae}
\end{figure}

\subsection{Topic Search Interface}
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/ptz/interaction.png}
    \caption{Screenshot of visual topic-mixture search interaction: Counter-clockwise from Top-left: Direct decodings of 25 topics; numbers in each decoded topic indicate the proportion of the target mixture given to that topic. Image of the target-mixture. Sample from the view with the closest topic distribution given by our spatial prediction model: topic div gives the divergence between the predicted encoding and the target encoding. Spatial topic map. Full extent of the possible views. True image at the closest topic distribution. Demo video at \protect\url{http://cim.mcgill.ca/\~akalmbach/thesis/demos.html}}
    \label{fig:interaction}
\end{figure}

A second outcome of our work is a tool for exploring views in an environment through mixtures of topics. By directly decoding the topics, we are able to visually present a summary of the scene. Then, by providing an interface to specify a mixture of these topics, and performing a search within the topic-prior map, we enable a unique interaction that enables a user to visually explore an environment, as well as the information encoded by the topic model and the topic-prior map (See Fig.~\ref{fig:interaction}).

\begin{figure}
    \centering
    \subfloat[][]{
        \includegraphics[width=\textwidth]{figures/ptz/topic_transitions}
        \label{fig:interaction-topic-transitions}
    }\\
    \subfloat[][]{
        \includegraphics[width=0.63\textwidth]{figures/ptz/topic_samples}
        \label{fig:interaction-topic-samples}
    }
    \caption{\protect\subref{fig:interaction-topic-transitions} Setting all other topic probabilities to 0, we can visualize the transition between two topics.
             \protect\subref{fig:interaction-topic-samples} Top left: A target image specified as a mixture of topics. Bottom left: The real image with the closest predicted topic distribution to the target. Right: Multinomial ($N=3000$ words) samples from the predicted word distributions.}
    \label{fig:interaction-insights}
\end{figure}

This interface leads us to a few interesting qualitative assesments of the topic model. For instance, in Fig.~\ref{fig:interaction-topic-transitions} we visualize interpolating topic distributions connecting two topics. This experiment confirms that the powerful decoder network is doing more than linearly mixing the topic images as we transition between two topics. Instead, it reveals colored segments of the images gradually changing in shape and intensity. Secondly, in Fig.~\ref{fig:interaction-topic-samples}, we visualize a predicted word distribution, as well as samples from the multinomial defined by that distribution. Recall that to train our model we resorted to discretizing the feature distribution in the same way, using a fixed number of samples from the word distribution. These samples are thus more akin to the training data our topic model sees. Further, they illustrate the true diversity of view predictions implied by our model. Each individual sample features slightly different shapes compared to eachother and compared to the prediction based on the mean word distribution. Further, the samples often contain more intense colors while being less smooth than the mean.

\section{Discussion} \label{sec:spatial-prediction-discussion}
\subsection{Active learning}\label{sec:active-learning}
As a complimentary task to learning the view prediction model, we also spent significant effort on the problem of \emph{efficiently} learning the view prediction model, that is, trying to accurately predict views in novel locations with as few training observations as possible. Although our specific view prediction model is novel, there is a rich literature on active learning (well summarized by \citep{settles2012active}), including many applications of efficient spatial sampling strategies drawing from the Gaussian Process (GP) models of \citep{Seo2000,guestrin2005near}. Recent work has touched on applications very similar to ours, such as determining where to take images to build a mosaic using a GP-based approach \citep{sandeep_crv_2016}, and which cells in a `view-grid' to observe to be able to predict the rest using a deep reinforcement learning based approach \citep{Jayaraman2017}.

We experimented with using GP-based upper confidence bounds on both the cross-entropy $H\left[e(I_x) \| \theta_x\Phi \right]$ and encoding divergence $D_{KL}\left[e(I_x) \| \theta_x\Phi\right]$ as well as using the predicted topic-prior entropy $H\left[\theta(x)\right]$ from our model as acquisition functions determining the most problematics regions in the training data, and therefore the best candidate locations for further training. We applied these acquisition functions to both the view-prediction problem and training a semantic segmentation model, however found that neither outperformed a uniform sampling strategy. We did observe that these policies resulted in more observations of complex images as opposed to simple ones (such as all blue sky or gray asphalt), however spending more time learning from complex examples did not improve performance as measured by predicted pixel MSE. It is our hypothesis that this was caused by a combination of biasing the topic model to use all its topics to describe a few small parts of the image (\emph{eg.} the cross-entropy method was very attracted to leaves if they were in the scene), and the limitations of MSE as a performance measurement for images with complex textures.
b
\subsection{Getting more from geometry and training other feature function}
Of note is the level of performance our model is able to achieve with a very simplistic geometric model. We use a Euclidean distance to place a prior on the similarity between views in both the topic model and the spatial prediction model. Clearly, however, there is more information to be gained from geometry in this problem. For instance, our feature does contain spatial information, so an edge in one location and the same edge translated or rotated to another location do not necessarily have similar word distributions. This means that the natural categories in a panorama may need to be described by multiple very location specific topics with our model. Further, the spatial prediction model could benefit from more sophisticated geometry. A similar operation to our projection of rectangular panorama images to rectified views could be used to ensure that training images are used to fill in information for all overlapping views. Even the simplpe addition of upsampling the center of a wide fov training image to get information about a more zoomed in test view would significantly improve our predictions.

\subsection{End-to-End Variational LDA Autoencoder}
While the CIB-AE provides slight modifications that can be made to many standard networks to produce encodings roughly compatible with the Dirichlet prior of LDA, it is natural to ask whether the full model can be learned end-to-end, and fine-tuned for particular environments instead of training a separate model. In fact, \citep{srivastava2017autoencoding} explores implementing the LDA prior for the latent space of a Variational autoencoder, and although the Dirichlet distribution resists exact reparameterization with a simple distribution as is used in other Variational autoencoders, they use a Laplace approximation to achieve comparable results to the standard mean-field variational approximation with much more flexibility. Although the extra dependencies in the spatio-temporal topic model are not compatible with the reparameterization used, we envision pre-training an LDA inference network similar to \citet{srivastava2017autoencoding}. Nevertheless, our world-specific, online implementation has the advantage of only needing to be locally salient. Further, it is unclear precisely how loose or tight a variational approximation to the dependencies between topic priors between nearby views would be, while the Gibbs sampling implementation is known to converge within a reasonable number of samples. Therefore we emphasize that the end-to-end approach is best suited to a pre-training phase to determine a word-distribution function and initialize $\Phi$, while our approach is more well-suited to view prediction from a reasonable number of observations.