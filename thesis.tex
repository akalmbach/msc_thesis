\documentclass[12pt,Bold,TexShade]{mcgilletdclass}
\usepackage[pdftex]{graphicx}
\usepackage{subfig}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{listings}
\usepackage{color}
\usepackage[labelformat=simple]{caption}
\usepackage[dvips]{geometry}
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\usepackage{amssymb}
\usepackage{url}
\usepackage{siunitx}
\usepackage{appendix}
\usepackage{hyperref}
\usepackage{natbib}

\usepackage{todonotes}

% \renewcommand\thesubfigure{\alph{subfigure})}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Have you configured your TeX system for proper  %%
%% page alignment? See the McGillETD documentation %%
%% for two methods that can be used to control     %%
%% page alignment. One method is demonstrated      %%
%% below. See documentation and the ufalign.tex    %%
%% file for instructions on how to adjust these    %%
%% parameters.                                     %%
\addtolength{\hoffset}{0pt}                        %%
\addtolength{\voffset}{0pt}                        %%
%%                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%       Define student-specific info
\SetTitle{\huge{Unsupervised Learning of Interpretable Models for Sparse, Smooth Data}}%
\SetAuthor{Arnold Kalmbach}%
\SetDegreeType{Master of Computer Science (Thesis)}%
\SetDepartment{Department of Computer Science}%
\SetUniversity{McGill University}%
\SetUniversityAddr{Montreal, Quebec}%
\SetThesisDate{\today}%
\SetRequirements{A thesis submitted to McGill University in partial fulfillment of requirements for
the degree of Master of Computer Science}%
\SetCopyright{\copyright{Arnold Kalmbach, 2018}}%

% \makeindex[keylist]
% \makeindex[abbr]

%% Input any special commands below
%\newcommand{\Kron}[1]{\ensuremath{\delta_{K}\left(#1\right)}}
\listfiles%
\begin{document}

\maketitle%

\begin{romanPagenumber}{2}%
\SetDedicationName{DEDICATION}%
\SetDedicationText{TODO TODO TODO}%
\Dedication%

\SetAcknowledgeName{ACKNOWLEDGEMENTS}%
\SetAcknowledgeText{TODO TODO TODO}%
\Acknowledge%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%         English Abstract                        %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\SetAbstractEnName{ABSTRACT}%
\SetAbstractEnText{
% \todo{Are you sure "maps" is the right term for the abstract?  As opposed to either "mappings", or "image manifolds",m or "sensorial maps", or maybe "image mappings"?}
This thesis considers learning unsupervised representations that facilitate understanding how complex data varies over space and time -- in other words learning interpretable semantic spatio-temporal map representations. We focus on representations that are \emph{sparse}, meaning most locations are described by few types, and \emph{smooth}, meaning most locations are described by the same types as their neighbors. We investigate how a spatio-temporal topic model -- a descendent of Latent Dirichlet Allocation (LDA) and other probabilistic latent variable models -- can be used to learn such representations for problems outside the typical text domain associated with LDA. We apply spatio-temporal topic models to a broad range of domains including modelling ambient audio, phytoplankton populations, and images observed by a robotic camera. In developing this variety of applications, we introduce novel interpretations of the learned topic model; beyond simple clustering or unsupervised classification we also consider using the topic model to make predictions in feature space and in the space of raw observations. In our experiments we demonstrate that sparse and smooth map representations are more interpretable than alternative unsupervised representations. We evaluate this interpretability in terms of both alignment with natural human-centric representations as well as the ease of learning later supervised models.
}
\AbstractEn%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%         French Abstract                         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\SetAbstractFrName{ABR\'{E}G\'{E}}%
\SetAbstractFrText{Cette th\`{e}se consid\`{e}re l'apprentissage des repr\'{e}sentations non supervis\'{e}es qui facilitant la compr\'{e}hension de la variation des donn\'{e}es complexes dans l'espace et dans le temps, c'est-\'{a}-dire l'apprentissage des cartes spatio-temporelles s\'{e}mantiques interpr\'{e}tables. Nous nous concentrons sur les repr\'{e}sentations qui sont \emph{clairsem\'{e}es} \todo{sparse?}, ce qui signifie que la plupart des endroits sont d\'{e}crits par quelques types, et \emph{r\'{e}guli\`{e}res}, ce qui signifie que la plupart des endroits sont d\'{e}crits par les m\^{e}mes types que leurs voisins. Nous \'{e}tudions comment un mod\`{e}le de sujet \todo{topic model?} spatio-temporel - un descendant de `Latent Dirichlet Allocation' (LDA) et d'autres mod\`{e}les de variables latentes probabilistes - peut \^{e}tre utilis\'{e} pour apprendre de telles repr\'{e}sentations pour des probl\`{e}mes ext\'{e}rieurs du domaine de texte associ\'{e} \`{a} LDA. Nous appliquons des mod\`{e}les de sujets spatio-temporels \`{a} un large gamme de, domaines, y compris la mod\'{e}lisation de l'audio ambiant, des populations de phytoplancton et des images observ\'{e}es par une cam\'{e}ra robotis\'{e}e. En d\'{e}veloppant cette vari\'{e}t\'{e} d'applications, nous introduisons de nouvelles interpr\'{e}tations du mod\`{e}le de sujet appris; plus que du simple regroupement ou de la classification non supervis\'{e}e, nous envisageons \'{e}galement d'utiliser le mod\`{e}le de sujet pour faire des pr\'{e}dictions dans l'espace des descripteurs\todo{feature space?} et dans l'espace des observations propres. Dans nos essais, nous d\'{e}montrons que les repr\'{e}sentations de carte clairsem\'{e}es et lisses sont plus interpr\'{e}tables que repr\'{e}sentations standards non supervis\'{e}es. Nous \'{e}valuons cette interpr\'{e}tabilit\'{e} en relation avec l'alignement avec les repr\'{e}sentations naturelles centr\'{e}es sur l'homme et aussi la facilit\'{e} d'apprentissage des mod\`{e}les supervis\'{e}s bas\'{e}s sur nos repr\'{e}sentations.}%
\AbstractFr%

\TOCHeading{TABLE OF CONTENTS}%
\LOTHeading{LIST OF TABLES}%
\LOFHeading{LIST OF FIGURES}%

\tableofcontents %
\listoftables %
\listoffigures %

\begin{center}
LIST OF SYMBOLS\\
\begin{tabular}{cp{\textwidth}}
  $x$ & Location, spatial or spatio-temporal \\
  $w$ & Word, an instance of a discrete-valued feature ($\textbf{w}$, all words) \\
  $z$ & Topic assignment ($\textbf{z}$, all topic assignments) \\
  $g(x)$ & The neighborhood of location $x$ \\
  $g$ & Parameter controlling neighborhood size \\
  $\theta$, $\theta_{g(x)}$ & Topic prior for one neighborhood ($\Theta$, all topic priors)\\
  $\phi$, $\phi_k$ & Word distribution for one topic ($\Phi$, all topics) \\
  $\alpha$ & Parameter of symmetric Dirichlet prior on $\theta$ \\
  $\beta$ & Parameter of symmetric Dirichlet prior on $\phi$ \\
  $K$ & Number of topics \\
  $V$ & Vocabulary size, number of distinct values of $w$ \\
  $N_{g(x)}^k$ & Number of times topic $k$ was used in neighborhood $g(x)$ \\
  $N_k^v$ & Number of times topic $k$ was used for word $v$ \\
  $N_{\cdot,-i}^\cdot$ & Count as above, but excluding the $i$-th datapoint\\
  $\sigma$ & The softmax function where the $i$-th dimension $\sigma(x)_i = {e^{x_i}} / \sum_j e^{x_j}$ \\
  $H[X]$ & Entropy of the random variable $X$: \\
         & $\mathbb{E}[-\log P(x)]$ \\
  $D_{KL}[P,Q]$ & KL-Divergence (information gain) of PMFs $P, Q$:\\
         & $\mathbb{E}_X \left[\log P(x) - \log Q(x) \right]$ \\
  $MI[X,Y]$ & Mutual information between random variables $X, Y$:\\
         & $\mathbb{E}_X \left[\mathbb{E}_Y \left[P(x,y) \log\left(\frac{P(x,y)}{P(x)P(y)}\right) \right] \right]$
\end{tabular}
\end{center}

\end{romanPagenumber} 

\mainmatter %
\include{intro}
\include{topic_model}
\include{classification}
\include{plankton}
\include{ptz}
\include{conclusion}
\include{appendices}

\bibliographystyle{chicagoa}
\bibHeading{References}
\bibliography{thesis}

%\index[abbr]{IEEE@IEEE: Institute of Electrical and Electronics Engineers, Inc.}
%\index[abbr]{CDMA@CDMA: code-division multiple access}
%\index[abbr]{CTAN@CTAN: comprehensive \protect\TeX{} archive network}

%\printindex[keylist]{Index}{Index}{}
%\printindex[abbr]{KEY TO ABBREVIATIONS}{KEY TO ABBREVIATIONS}{}

\end{document}
